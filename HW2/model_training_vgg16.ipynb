{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUBAYDhf2Qyn"
      },
      "source": [
        "# VGG16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgKpnwrJ0JJJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "cfg = {\n",
        "    'A' : [64,     'M', 128,      'M', 256, 256,           'M', 512, 512,           'M', 512, 512,           'M'],\n",
        "    'B' : [64, 64, 'M', 128, 128, 'M', 256, 256,           'M', 512, 512,           'M', 512, 512,           'M'],\n",
        "    'D' : [64, 64, 'M', 128, 128, 'M', 256, 256, 256,      'M', 512, 512, 512,      'M', 512, 512, 512,      'M'],\n",
        "    'E' : [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M']\n",
        "}\n",
        "\n",
        "class VGG(nn.Module):\n",
        "\n",
        "    def __init__(self, features, num_class=100):\n",
        "        super().__init__()\n",
        "        self.features = features\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, num_class)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.features(x)\n",
        "        output = output.view(output.size()[0], -1)\n",
        "        output = self.classifier(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "def make_layers(cfg, batch_norm=False):\n",
        "    layers = []\n",
        "\n",
        "    input_channel = 3\n",
        "    for l in cfg:\n",
        "        if l == 'M':\n",
        "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            continue\n",
        "\n",
        "        layers += [nn.Conv2d(input_channel, l, kernel_size=3, padding=1)]\n",
        "\n",
        "        if batch_norm:\n",
        "            layers += [nn.BatchNorm2d(l)]\n",
        "\n",
        "        layers += [nn.ReLU(inplace=True)]\n",
        "        input_channel = l\n",
        "\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "def vgg16_bn():\n",
        "    return VGG(make_layers(cfg['D'], batch_norm=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5ibUmce2wMe"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1abAhoAJctT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split, Subset\n",
        "\n",
        "# Define the data transformations\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# Download the CIFAR-100 dataset\n",
        "train_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "# Split the dataset into training, validation, and test sets\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = int(0.1 * len(train_dataset))\n",
        "test_size = len(train_dataset) - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(train_dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# Create balanced DataLoader for training, validation, and test sets\n",
        "batch_size = 64\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iLoNf4WxCJW",
        "outputId": "b608767d-1fb8-4c21-fb28-ab69c155e719"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'pytorch-cifar100'...\n",
            "remote: Enumerating objects: 1043, done.\u001b[K\n",
            "remote: Counting objects: 100% (1043/1043), done.\u001b[K\n",
            "remote: Compressing objects: 100% (386/386), done.\u001b[K\n",
            "remote: Total 1043 (delta 658), reused 1008 (delta 645), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (1043/1043), 492.58 KiB | 1.94 MiB/s, done.\n",
            "Resolving deltas: 100% (658/658), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/K-Hooshanfar/pytorch-cifar100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lA9insc0Yad",
        "outputId": "488e42a4-dae0-426c-ca96-639c86dbad16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/pytorch-cifar100\n"
          ]
        }
      ],
      "source": [
        "%cd pytorch-cifar100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TJmhdS4YixS",
        "outputId": "6af5cba0-9b84-4535-bd7e-9775693a7b94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-12-01 12:46:15.279934: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-01 12:46:15.279987: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-01 12:46:15.280024: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-01 12:46:16.347360: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Files already downloaded and verified\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Files already downloaded and verified\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 1, Average loss: 0.0325, Accuracy: 0.0626, Time consumed:2.71s\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 2, Average loss: 0.0302, Accuracy: 0.0875, Time consumed:3.98s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 3, Average loss: 0.0285, Accuracy: 0.1195, Time consumed:2.70s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 4, Average loss: 0.0272, Accuracy: 0.1423, Time consumed:2.68s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 5, Average loss: 0.0265, Accuracy: 0.1627, Time consumed:2.64s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 6, Average loss: 0.0256, Accuracy: 0.1943, Time consumed:2.59s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 7, Average loss: 0.0266, Accuracy: 0.1984, Time consumed:3.61s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 8, Average loss: 0.0234, Accuracy: 0.2525, Time consumed:3.48s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 9, Average loss: 0.0221, Accuracy: 0.2708, Time consumed:2.73s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 10, Average loss: 0.0235, Accuracy: 0.2557, Time consumed:2.71s\n",
            "\n",
            "saving weights file to checkpoint/vgg16/Friday_01_December_2023_12h_46m_17s/vgg16-10-regular.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 11, Average loss: 0.0221, Accuracy: 0.2762, Time consumed:2.62s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 12, Average loss: 0.0227, Accuracy: 0.2938, Time consumed:3.10s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 13, Average loss: 0.0199, Accuracy: 0.3381, Time consumed:3.74s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 14, Average loss: 0.0209, Accuracy: 0.3130, Time consumed:2.60s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 15, Average loss: 0.0206, Accuracy: 0.3357, Time consumed:2.64s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 16, Average loss: 0.0201, Accuracy: 0.3523, Time consumed:2.67s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 17, Average loss: 0.0210, Accuracy: 0.3369, Time consumed:2.62s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 18, Average loss: 0.0184, Accuracy: 0.3840, Time consumed:3.91s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 19, Average loss: 0.0218, Accuracy: 0.3474, Time consumed:2.74s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 20, Average loss: 0.0181, Accuracy: 0.4019, Time consumed:2.57s\n",
            "\n",
            "saving weights file to checkpoint/vgg16/Friday_01_December_2023_12h_46m_17s/vgg16-20-regular.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 21, Average loss: 0.0207, Accuracy: 0.3492, Time consumed:2.58s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 22, Average loss: 0.0187, Accuracy: 0.3971, Time consumed:2.64s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 23, Average loss: 0.0183, Accuracy: 0.4186, Time consumed:3.78s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 24, Average loss: 0.0220, Accuracy: 0.3416, Time consumed:3.17s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 25, Average loss: 0.0167, Accuracy: 0.4402, Time consumed:2.72s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 26, Average loss: 0.0191, Accuracy: 0.3955, Time consumed:2.56s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 27, Average loss: 0.0210, Accuracy: 0.3474, Time consumed:2.62s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 28, Average loss: 0.0202, Accuracy: 0.3777, Time consumed:2.84s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 29, Average loss: 0.0191, Accuracy: 0.4020, Time consumed:3.89s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 30, Average loss: 0.0174, Accuracy: 0.4427, Time consumed:2.65s\n",
            "\n",
            "saving weights file to checkpoint/vgg16/Friday_01_December_2023_12h_46m_17s/vgg16-30-regular.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 31, Average loss: 0.0184, Accuracy: 0.4162, Time consumed:2.63s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 32, Average loss: 0.0172, Accuracy: 0.4468, Time consumed:2.68s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 33, Average loss: 0.0202, Accuracy: 0.3751, Time consumed:3.42s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 34, Average loss: 0.0193, Accuracy: 0.3923, Time consumed:3.42s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 35, Average loss: 0.0178, Accuracy: 0.4322, Time consumed:2.66s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 36, Average loss: 0.0170, Accuracy: 0.4497, Time consumed:2.66s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 37, Average loss: 0.0171, Accuracy: 0.4540, Time consumed:2.73s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 38, Average loss: 0.0197, Accuracy: 0.4004, Time consumed:3.50s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 39, Average loss: 0.0182, Accuracy: 0.4225, Time consumed:3.45s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 40, Average loss: 0.0220, Accuracy: 0.3721, Time consumed:2.63s\n",
            "\n",
            "saving weights file to checkpoint/vgg16/Friday_01_December_2023_12h_46m_17s/vgg16-40-regular.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 41, Average loss: 0.0163, Accuracy: 0.4692, Time consumed:2.71s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 42, Average loss: 0.0198, Accuracy: 0.4064, Time consumed:2.59s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 43, Average loss: 0.0177, Accuracy: 0.4365, Time consumed:2.64s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 44, Average loss: 0.0178, Accuracy: 0.4334, Time consumed:3.95s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 45, Average loss: 0.0175, Accuracy: 0.4471, Time consumed:2.75s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 46, Average loss: 0.0171, Accuracy: 0.4542, Time consumed:2.57s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 47, Average loss: 0.0174, Accuracy: 0.4445, Time consumed:2.54s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 48, Average loss: 0.0183, Accuracy: 0.4384, Time consumed:2.66s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 49, Average loss: 0.0201, Accuracy: 0.4011, Time consumed:3.13s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 50, Average loss: 0.0174, Accuracy: 0.4478, Time consumed:3.49s\n",
            "\n",
            "saving weights file to checkpoint/vgg16/Friday_01_December_2023_12h_46m_17s/vgg16-50-regular.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 51, Average loss: 0.0169, Accuracy: 0.4565, Time consumed:2.59s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 52, Average loss: 0.0188, Accuracy: 0.4219, Time consumed:2.64s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 53, Average loss: 0.0174, Accuracy: 0.4419, Time consumed:2.65s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 54, Average loss: 0.0184, Accuracy: 0.4297, Time consumed:2.83s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 55, Average loss: 0.0175, Accuracy: 0.4521, Time consumed:3.86s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 56, Average loss: 0.0189, Accuracy: 0.4285, Time consumed:2.57s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 57, Average loss: 0.0174, Accuracy: 0.4594, Time consumed:2.61s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 58, Average loss: 0.0186, Accuracy: 0.4314, Time consumed:2.57s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 59, Average loss: 0.0164, Accuracy: 0.4761, Time consumed:2.66s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 60, Average loss: 0.0102, Accuracy: 0.6423, Time consumed:3.56s\n",
            "\n",
            "saving weights file to checkpoint/vgg16/Friday_01_December_2023_12h_46m_17s/vgg16-60-regular.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 61, Average loss: 0.0103, Accuracy: 0.6408, Time consumed:3.36s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 62, Average loss: 0.0103, Accuracy: 0.6436, Time consumed:2.60s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 63, Average loss: 0.0102, Accuracy: 0.6470, Time consumed:2.57s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 64, Average loss: 0.0101, Accuracy: 0.6519, Time consumed:2.59s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 65, Average loss: 0.0099, Accuracy: 0.6600, Time consumed:2.69s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 66, Average loss: 0.0105, Accuracy: 0.6415, Time consumed:3.83s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 67, Average loss: 0.0100, Accuracy: 0.6532, Time consumed:2.57s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 68, Average loss: 0.0102, Accuracy: 0.6580, Time consumed:2.56s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 69, Average loss: 0.0106, Accuracy: 0.6480, Time consumed:2.60s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 70, Average loss: 0.0108, Accuracy: 0.6446, Time consumed:2.61s\n",
            "\n",
            "saving weights file to checkpoint/vgg16/Friday_01_December_2023_12h_46m_17s/vgg16-70-regular.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 71, Average loss: 0.0106, Accuracy: 0.6425, Time consumed:3.40s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 72, Average loss: 0.0111, Accuracy: 0.6332, Time consumed:3.49s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 73, Average loss: 0.0117, Accuracy: 0.6173, Time consumed:2.63s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 74, Average loss: 0.0112, Accuracy: 0.6334, Time consumed:2.68s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 75, Average loss: 0.0110, Accuracy: 0.6369, Time consumed:2.61s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 76, Average loss: 0.0115, Accuracy: 0.6245, Time consumed:2.84s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 77, Average loss: 0.0110, Accuracy: 0.6326, Time consumed:3.91s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 78, Average loss: 0.0111, Accuracy: 0.6361, Time consumed:2.58s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 79, Average loss: 0.0115, Accuracy: 0.6266, Time consumed:2.56s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 80, Average loss: 0.0116, Accuracy: 0.6293, Time consumed:2.62s\n",
            "\n",
            "saving weights file to checkpoint/vgg16/Friday_01_December_2023_12h_46m_17s/vgg16-80-regular.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 81, Average loss: 0.0112, Accuracy: 0.6335, Time consumed:2.58s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 82, Average loss: 0.0110, Accuracy: 0.6409, Time consumed:3.83s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 83, Average loss: 0.0108, Accuracy: 0.6490, Time consumed:3.46s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 84, Average loss: 0.0115, Accuracy: 0.6301, Time consumed:2.59s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 85, Average loss: 0.0112, Accuracy: 0.6378, Time consumed:2.60s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 86, Average loss: 0.0112, Accuracy: 0.6393, Time consumed:2.63s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 87, Average loss: 0.0111, Accuracy: 0.6411, Time consumed:2.64s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 88, Average loss: 0.0113, Accuracy: 0.6378, Time consumed:3.64s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 89, Average loss: 0.0112, Accuracy: 0.6365, Time consumed:3.39s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 90, Average loss: 0.0121, Accuracy: 0.6257, Time consumed:2.60s\n",
            "\n",
            "saving weights file to checkpoint/vgg16/Friday_01_December_2023_12h_46m_17s/vgg16-90-regular.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 91, Average loss: 0.0117, Accuracy: 0.6354, Time consumed:2.58s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 92, Average loss: 0.0117, Accuracy: 0.6338, Time consumed:2.57s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 93, Average loss: 0.0121, Accuracy: 0.6190, Time consumed:2.88s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 94, Average loss: 0.0115, Accuracy: 0.6276, Time consumed:3.60s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 95, Average loss: 0.0125, Accuracy: 0.6153, Time consumed:2.63s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 96, Average loss: 0.0112, Accuracy: 0.6423, Time consumed:2.69s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 97, Average loss: 0.0118, Accuracy: 0.6299, Time consumed:2.61s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 98, Average loss: 0.0122, Accuracy: 0.6265, Time consumed:2.61s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 99, Average loss: 0.0117, Accuracy: 0.6370, Time consumed:3.76s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 100, Average loss: 0.0117, Accuracy: 0.6329, Time consumed:3.41s\n",
            "\n",
            "saving weights file to checkpoint/vgg16/Friday_01_December_2023_12h_46m_17s/vgg16-100-regular.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 101, Average loss: 0.0124, Accuracy: 0.6241, Time consumed:2.58s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 102, Average loss: 0.0116, Accuracy: 0.6351, Time consumed:2.62s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 103, Average loss: 0.0113, Accuracy: 0.6407, Time consumed:2.64s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 104, Average loss: 0.0116, Accuracy: 0.6398, Time consumed:2.97s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 105, Average loss: 0.0121, Accuracy: 0.6271, Time consumed:3.71s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 106, Average loss: 0.0118, Accuracy: 0.6343, Time consumed:2.62s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 107, Average loss: 0.0121, Accuracy: 0.6322, Time consumed:2.57s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 108, Average loss: 0.0111, Accuracy: 0.6450, Time consumed:2.65s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 109, Average loss: 0.0117, Accuracy: 0.6378, Time consumed:2.63s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 110, Average loss: 0.0124, Accuracy: 0.6213, Time consumed:3.78s\n",
            "\n",
            "saving weights file to checkpoint/vgg16/Friday_01_December_2023_12h_46m_17s/vgg16-110-regular.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 111, Average loss: 0.0116, Accuracy: 0.6407, Time consumed:2.57s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 112, Average loss: 0.0112, Accuracy: 0.6503, Time consumed:2.59s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 113, Average loss: 0.0122, Accuracy: 0.6258, Time consumed:2.86s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 114, Average loss: 0.0127, Accuracy: 0.6259, Time consumed:3.45s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 115, Average loss: 0.0124, Accuracy: 0.6233, Time consumed:3.39s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 116, Average loss: 0.0118, Accuracy: 0.6461, Time consumed:2.62s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 117, Average loss: 0.0115, Accuracy: 0.6443, Time consumed:2.65s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 118, Average loss: 0.0112, Accuracy: 0.6522, Time consumed:2.63s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 119, Average loss: 0.0125, Accuracy: 0.6240, Time consumed:3.42s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 120, Average loss: 0.0098, Accuracy: 0.7135, Time consumed:3.49s\n",
            "\n",
            "saving weights file to checkpoint/vgg16/Friday_01_December_2023_12h_46m_17s/vgg16-120-regular.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 121, Average loss: 0.0100, Accuracy: 0.7136, Time consumed:2.67s\n",
            "\n",
            "saving weights file to checkpoint/vgg16/Friday_01_December_2023_12h_46m_17s/vgg16-121-best.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 122, Average loss: 0.0102, Accuracy: 0.7176, Time consumed:2.58s\n",
            "\n",
            "saving weights file to checkpoint/vgg16/Friday_01_December_2023_12h_46m_17s/vgg16-122-best.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 123, Average loss: 0.0106, Accuracy: 0.7131, Time consumed:2.57s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 124, Average loss: 0.0110, Accuracy: 0.7115, Time consumed:2.91s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 125, Average loss: 0.0109, Accuracy: 0.7095, Time consumed:3.83s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 126, Average loss: 0.0111, Accuracy: 0.7151, Time consumed:2.82s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 127, Average loss: 0.0112, Accuracy: 0.7108, Time consumed:2.63s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 128, Average loss: 0.0117, Accuracy: 0.7056, Time consumed:2.65s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 129, Average loss: 0.0115, Accuracy: 0.7093, Time consumed:2.50s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 130, Average loss: 0.0116, Accuracy: 0.7058, Time consumed:3.16s\n",
            "\n",
            "saving weights file to checkpoint/vgg16/Friday_01_December_2023_12h_46m_17s/vgg16-130-regular.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 131, Average loss: 0.0118, Accuracy: 0.7050, Time consumed:3.39s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 132, Average loss: 0.0119, Accuracy: 0.7054, Time consumed:2.58s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 133, Average loss: 0.0118, Accuracy: 0.7051, Time consumed:2.62s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 134, Average loss: 0.0122, Accuracy: 0.7038, Time consumed:2.58s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 135, Average loss: 0.0121, Accuracy: 0.7072, Time consumed:2.61s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 136, Average loss: 0.0121, Accuracy: 0.7086, Time consumed:3.28s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 137, Average loss: 0.0123, Accuracy: 0.7075, Time consumed:3.42s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 138, Average loss: 0.0122, Accuracy: 0.7057, Time consumed:2.66s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 139, Average loss: 0.0123, Accuracy: 0.7049, Time consumed:2.56s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 140, Average loss: 0.0127, Accuracy: 0.7044, Time consumed:2.53s\n",
            "\n",
            "saving weights file to checkpoint/vgg16/Friday_01_December_2023_12h_46m_17s/vgg16-140-regular.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 141, Average loss: 0.0123, Accuracy: 0.7043, Time consumed:2.73s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 142, Average loss: 0.0122, Accuracy: 0.7037, Time consumed:3.87s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 143, Average loss: 0.0124, Accuracy: 0.7019, Time consumed:2.71s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 144, Average loss: 0.0127, Accuracy: 0.6972, Time consumed:2.59s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 145, Average loss: 0.0128, Accuracy: 0.7000, Time consumed:2.63s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 146, Average loss: 0.0132, Accuracy: 0.6965, Time consumed:2.60s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 147, Average loss: 0.0125, Accuracy: 0.6998, Time consumed:3.52s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 148, Average loss: 0.0122, Accuracy: 0.7051, Time consumed:3.43s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 149, Average loss: 0.0130, Accuracy: 0.6993, Time consumed:2.59s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 150, Average loss: 0.0128, Accuracy: 0.6974, Time consumed:2.56s\n",
            "\n",
            "saving weights file to checkpoint/vgg16/Friday_01_December_2023_12h_46m_17s/vgg16-150-regular.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 151, Average loss: 0.0127, Accuracy: 0.6962, Time consumed:2.65s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 152, Average loss: 0.0126, Accuracy: 0.6980, Time consumed:2.93s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 153, Average loss: 0.0125, Accuracy: 0.6996, Time consumed:3.63s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 154, Average loss: 0.0128, Accuracy: 0.7007, Time consumed:2.60s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 155, Average loss: 0.0127, Accuracy: 0.6978, Time consumed:2.59s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 156, Average loss: 0.0131, Accuracy: 0.6906, Time consumed:2.58s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 157, Average loss: 0.0128, Accuracy: 0.6956, Time consumed:2.63s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 158, Average loss: 0.0125, Accuracy: 0.7027, Time consumed:2.58s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 159, Average loss: 0.0125, Accuracy: 0.6986, Time consumed:3.91s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 160, Average loss: 0.0120, Accuracy: 0.7139, Time consumed:2.61s\n",
            "\n",
            "saving weights file to checkpoint/vgg16/Friday_01_December_2023_12h_46m_17s/vgg16-160-regular.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 161, Average loss: 0.0119, Accuracy: 0.7150, Time consumed:2.60s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 162, Average loss: 0.0121, Accuracy: 0.7176, Time consumed:2.54s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 163, Average loss: 0.0122, Accuracy: 0.7152, Time consumed:2.61s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 164, Average loss: 0.0121, Accuracy: 0.7172, Time consumed:3.74s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 165, Average loss: 0.0122, Accuracy: 0.7187, Time consumed:3.49s\n",
            "\n",
            "saving weights file to checkpoint/vgg16/Friday_01_December_2023_12h_46m_17s/vgg16-165-best.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 166, Average loss: 0.0121, Accuracy: 0.7200, Time consumed:2.54s\n",
            "\n",
            "saving weights file to checkpoint/vgg16/Friday_01_December_2023_12h_46m_17s/vgg16-166-best.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 167, Average loss: 0.0123, Accuracy: 0.7206, Time consumed:2.55s\n",
            "\n",
            "saving weights file to checkpoint/vgg16/Friday_01_December_2023_12h_46m_17s/vgg16-167-best.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 168, Average loss: 0.0122, Accuracy: 0.7192, Time consumed:2.53s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 169, Average loss: 0.0123, Accuracy: 0.7197, Time consumed:2.54s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 170, Average loss: 0.0123, Accuracy: 0.7192, Time consumed:3.88s\n",
            "\n",
            "saving weights file to checkpoint/vgg16/Friday_01_December_2023_12h_46m_17s/vgg16-170-regular.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 171, Average loss: 0.0123, Accuracy: 0.7201, Time consumed:3.32s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 172, Average loss: 0.0123, Accuracy: 0.7186, Time consumed:2.56s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 173, Average loss: 0.0125, Accuracy: 0.7181, Time consumed:2.63s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 174, Average loss: 0.0124, Accuracy: 0.7190, Time consumed:2.59s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 175, Average loss: 0.0124, Accuracy: 0.7173, Time consumed:2.59s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 176, Average loss: 0.0125, Accuracy: 0.7178, Time consumed:3.74s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 177, Average loss: 0.0125, Accuracy: 0.7201, Time consumed:2.66s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 178, Average loss: 0.0125, Accuracy: 0.7190, Time consumed:2.57s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 179, Average loss: 0.0126, Accuracy: 0.7218, Time consumed:2.69s\n",
            "\n",
            "saving weights file to checkpoint/vgg16/Friday_01_December_2023_12h_46m_17s/vgg16-179-best.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 180, Average loss: 0.0125, Accuracy: 0.7210, Time consumed:2.66s\n",
            "\n",
            "saving weights file to checkpoint/vgg16/Friday_01_December_2023_12h_46m_17s/vgg16-180-regular.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 181, Average loss: 0.0126, Accuracy: 0.7204, Time consumed:3.44s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 182, Average loss: 0.0125, Accuracy: 0.7202, Time consumed:3.35s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 183, Average loss: 0.0125, Accuracy: 0.7195, Time consumed:2.57s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 184, Average loss: 0.0126, Accuracy: 0.7202, Time consumed:2.56s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 185, Average loss: 0.0125, Accuracy: 0.7198, Time consumed:2.60s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 186, Average loss: 0.0128, Accuracy: 0.7200, Time consumed:2.75s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 187, Average loss: 0.0127, Accuracy: 0.7237, Time consumed:4.18s\n",
            "\n",
            "saving weights file to checkpoint/vgg16/Friday_01_December_2023_12h_46m_17s/vgg16-187-best.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 188, Average loss: 0.0126, Accuracy: 0.7248, Time consumed:2.55s\n",
            "\n",
            "saving weights file to checkpoint/vgg16/Friday_01_December_2023_12h_46m_17s/vgg16-188-best.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 189, Average loss: 0.0126, Accuracy: 0.7255, Time consumed:2.65s\n",
            "\n",
            "saving weights file to checkpoint/vgg16/Friday_01_December_2023_12h_46m_17s/vgg16-189-best.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 190, Average loss: 0.0126, Accuracy: 0.7245, Time consumed:2.60s\n",
            "\n",
            "saving weights file to checkpoint/vgg16/Friday_01_December_2023_12h_46m_17s/vgg16-190-regular.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 191, Average loss: 0.0125, Accuracy: 0.7227, Time consumed:2.55s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 192, Average loss: 0.0126, Accuracy: 0.7236, Time consumed:3.89s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 193, Average loss: 0.0128, Accuracy: 0.7239, Time consumed:3.23s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 194, Average loss: 0.0127, Accuracy: 0.7243, Time consumed:2.60s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 195, Average loss: 0.0127, Accuracy: 0.7236, Time consumed:2.66s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 196, Average loss: 0.0128, Accuracy: 0.7226, Time consumed:2.61s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 197, Average loss: 0.0127, Accuracy: 0.7231, Time consumed:2.89s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 198, Average loss: 0.0128, Accuracy: 0.7237, Time consumed:3.63s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 199, Average loss: 0.0128, Accuracy: 0.7217, Time consumed:2.59s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 200, Average loss: 0.0127, Accuracy: 0.7229, Time consumed:2.60s\n",
            "\n",
            "saving weights file to checkpoint/vgg16/Friday_01_December_2023_12h_46m_17s/vgg16-200-regular.pth\n",
            "Figure(1000x500)\n"
          ]
        }
      ],
      "source": [
        "!python train.py -net vgg16 -gpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIihPq3b2ywe"
      },
      "source": [
        "## Save weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVREAAwi0dvY",
        "outputId": "71d2373b-fd08-4d3c-8775-96f681818981"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Hh2zUIXp1Gpn",
        "outputId": "7d910186-1ea4-4600-eeb2-7af48e593e77"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/gdrive/My Drive/vgg16-200-regular.pth'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import shutil\n",
        "\n",
        "source_path = '/content/pytorch-cifar100/checkpoint/vgg16/Friday_01_December_2023_12h_46m_17s/vgg16-200-regular.pth'\n",
        "destination_path = '/content/gdrive/My Drive/vgg16-200-regular.pth'\n",
        "\n",
        "shutil.copyfile(source_path, destination_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcGMtqQQ0JN3",
        "outputId": "b1d43f5b-c275-4073-80d2-c35064949bbd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = vgg16_bn()\n",
        "\n",
        "# Load pre-trained weights\n",
        "pretrained_weights_path = '/content/pytorch-cifar100/checkpoint/vgg16/Friday_01_December_2023_12h_46m_17s/vgg16-200-regular.pth'\n",
        "model.load_state_dict(torch.load(pretrained_weights_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAGOQKYn0JLx",
        "outputId": "e9357575-bcd3-4256-f532-76c8f0817e27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 169001437/169001437 [00:13<00:00, 12346764.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
            "Test set size: 10000\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "# Define data transformations\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# Load CIFAR-100 dataset\n",
        "cifar100_test = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "\n",
        "# Create PyTorch data loaders\n",
        "batch_size = 128\n",
        "val_loader = torch.utils.data.DataLoader(cifar100_test, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Check the number of samples in each set\n",
        "print(f\"Test set size: {len(cifar100_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUyudrZ20dtC",
        "outputId": "a79b8ef4-2553-4e10-9670-c7227f99a56a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Test Accuracy: 72.29%\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "correct_test = 0\n",
        "total_test = 0\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move the model to the device\n",
        "model.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in val_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_test += labels.size(0)\n",
        "        correct_test += (predicted == labels).sum().item()\n",
        "\n",
        "test_accuracy = 100 * correct_test / total_test\n",
        "print(f\"Final Test Accuracy: {test_accuracy:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQuzrjdi2YHY"
      },
      "source": [
        "# Load Model and Get Feature Latent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eEoZoxL8V_T",
        "outputId": "737d5ea7-29f6-4d68-ba5f-c5e1dbd4a0ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Balanced Train set size: 36000\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "random_seed = 42\n",
        "torch.manual_seed(random_seed)\n",
        "np.random.seed(random_seed)\n",
        "random.seed(random_seed)\n",
        "\n",
        "# Load the CIFAR-100 dataset and create a balanced subset\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "# Load CIFAR-100 dataset\n",
        "cifar100_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_size = int(0.8 * len(cifar100_dataset))\n",
        "val_size = len(cifar100_dataset) - train_size\n",
        "cifar100_traindataset, cifar100_valdataset = torch.utils.data.random_split(cifar100_dataset, [train_size, val_size])\n",
        "\n",
        "# Define the subset size\n",
        "subset_fraction = 0.9\n",
        "subset_size_train = int(subset_fraction * len(cifar100_traindataset))\n",
        "\n",
        "class_indices = list(range(len(cifar100_traindataset.dataset.classes)))\n",
        "class_subset_size = int(subset_size_train / len(cifar100_traindataset.dataset.classes))\n",
        "\n",
        "class_sampler_indices_train = []\n",
        "\n",
        "for class_index in class_indices:\n",
        "    class_indices_list_train = [i for i, label in enumerate(cifar100_traindataset.dataset.targets) if label == class_index]\n",
        "    class_sampler_indices_train.extend(class_indices_list_train[:class_subset_size])\n",
        "\n",
        "train_sampler = SubsetRandomSampler(class_sampler_indices_train)\n",
        "\n",
        "batch_size = 256\n",
        "train_loader = torch.utils.data.DataLoader(cifar100_traindataset, batch_size=batch_size, sampler=train_sampler)\n",
        "\n",
        "# Check the number of samples in the balanced train set\n",
        "print(f\"Balanced Train set size: {len(train_loader.sampler)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KLh8VcO1cJC",
        "outputId": "6c7431d2-bed7-440f-935f-457eb631d669"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 79/79 [00:07<00:00, 11.01it/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Instantiate the VGG16 model\n",
        "model = vgg16_bn()\n",
        "model.to(device)\n",
        "\n",
        "# Load pre-trained weights\n",
        "pretrained_weights_path = '/content/pytorch-cifar100/checkpoint/vgg16/Friday_01_December_2023_12h_46m_17s/vgg16-200-regular.pth'\n",
        "model.load_state_dict(torch.load(pretrained_weights_path))\n",
        "model = nn.Sequential(*list(model.children())[:-1])\n",
        "\n",
        "# Define your CIFAR-100 dataloader\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "features = []\n",
        "labels = []\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in tqdm(train_loader):\n",
        "        if torch.cuda.is_available():\n",
        "            inputs = inputs.to('cuda')\n",
        "\n",
        "        # Forward pass through the model.features\n",
        "        features_batch = model(inputs)\n",
        "\n",
        "        # Append the extracted features and labels\n",
        "        features.append(features_batch)\n",
        "        labels.append(targets)\n",
        "\n",
        "        # Release GPU memory\n",
        "        del inputs\n",
        "        torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTDsWMYg3BQL"
      },
      "outputs": [],
      "source": [
        "# Stack and reshape the extracted features\n",
        "features = torch.cat(features)\n",
        "features = features.view(features.size(0), -1)\n",
        "labels = torch.cat(labels)\n",
        "labels = labels.unsqueeze(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jb-Ge1iN1dWO"
      },
      "source": [
        "# Metrics for train set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSbR5vqc1cO9",
        "outputId": "876759a8-e57b-445c-f910-b8bbdb4e662c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'data_complexity_measures'...\n",
            "remote: Enumerating objects: 140, done.\u001b[K\n",
            "remote: Counting objects: 100% (140/140), done.\u001b[K\n",
            "remote: Compressing objects: 100% (127/127), done.\u001b[K\n",
            "remote: Total 140 (delta 80), reused 33 (delta 9), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (140/140), 144.00 KiB | 4.23 MiB/s, done.\n",
            "Resolving deltas: 100% (80/80), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Arhosseini77/data_complexity_measures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OqDad3h74jg6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "import numpy as np\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "from data_complexity_measures.models.ARH_SeparationIndex import ARH_SeparationIndex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qThhTR-c5dLM",
        "outputId": "1433778a-f16f-48ca-bc7e-d4651d86e2b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data has been normalized\n"
          ]
        }
      ],
      "source": [
        "# Create Instance of class\n",
        "si_calculator = ARH_SeparationIndex(features, labels, normalize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tw8fo7YZ5MG-"
      },
      "source": [
        "# SI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18uvoINGLZvC",
        "outputId": "b82c0325-b648-4a34-9360-524e719d6313"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating SI: 100%|██████████| 40000/40000 [00:02<00:00, 17628.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.338725\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "si_data = si_calculator.si()\n",
        "print(si_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAvldqd05pWF"
      },
      "source": [
        "# Calc High order SI (order = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4apkrlWS5ozd",
        "outputId": "0fb71b27-ef2d-496e-b1ed-d7b2ecb92fb0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing High Order SI: 100%|██████████| 20000/20000 [00:02<00:00, 7028.67it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.21879999339580536\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "si_high_order_2_data = si_calculator.high_order_si(order=2)\n",
        "print(si_high_order_2_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHTQRP-19Hpo"
      },
      "source": [
        "# High order soft SI (order=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5Sa5M2s5o_p",
        "outputId": "205e1402-0daf-47cb-b420-81c7991b6c8d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating Soft Order SI: 100%|██████████| 20000/20000 [00:01<00:00, 10047.12it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Soft Order(2) SI : 0.3128249943256378\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "si_soft_order_2_data = si_calculator.soft_order_si(order=2)\n",
        "print(\"Soft Order(2) SI :\", si_soft_order_2_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCS-Wdn39Ssq"
      },
      "source": [
        "# Center Based SI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrDfe1Pm9SL6",
        "outputId": "737ad1d4-2315-4619-a0ca-ced7c8c237f0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating Class Centers: 100%|██████████| 100/100 [00:00<00:00, 6109.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Center SI: 0.3668999969959259\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "center_si_data = si_calculator.center_si()\n",
        "print(\"Center SI:\", center_si_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mD2hw5z9VuK"
      },
      "source": [
        "# Anti SI (order = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfDd4F0m9SOL",
        "outputId": "a2e0b0c1-8e31-4f89-f5e6-a070a7fb15d6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating Anti-SI: 100%|██████████| 20000/20000 [00:01<00:00, 11169.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "anti_si: 0.5931499600410461\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "anti_si = si_calculator.anti_si(order=2)\n",
        "print(\"anti_si:\", anti_si)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1Hs5D03EsyI"
      },
      "source": [
        "# Metrics for test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4P8RIgrEuP7",
        "outputId": "e7a63536-c1f1-4113-f33d-c8ae95405312"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 169001437/169001437 [00:02<00:00, 66013335.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "# Define data transformations\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# Load CIFAR-100 dataset\n",
        "cifar100_test = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Create PyTorch data loaders\n",
        "batch_size = 128\n",
        "test_loader = torch.utils.data.DataLoader(cifar100_test, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQAiT0ECE7pp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Instantiate the VGG16 model\n",
        "model = vgg16_bn()\n",
        "model.to(device)\n",
        "\n",
        "# Load pre-trained weights\n",
        "pretrained_weights_path = '/content/pytorch-cifar100/checkpoint/vgg16/Friday_01_December_2023_12h_46m_17s/vgg16-200-regular.pth'\n",
        "model.load_state_dict(torch.load(pretrained_weights_path))\n",
        "model = nn.Sequential(*list(model.children())[:-1])\n",
        "\n",
        "# Define your CIFAR-100 dataloader\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "features = []\n",
        "labels = []\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in tqdm(test_loader):\n",
        "        if torch.cuda.is_available():\n",
        "            inputs = inputs.to('cuda')\n",
        "\n",
        "        # Forward pass through the model.features\n",
        "        features_batch = model(inputs)\n",
        "\n",
        "        # Append the extracted features and labels\n",
        "        features.append(features_batch)\n",
        "        labels.append(targets)\n",
        "\n",
        "        # Release GPU memory\n",
        "        del inputs\n",
        "        torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ighPhkNxFCBa"
      },
      "outputs": [],
      "source": [
        "# Stack and reshape the extracted features\n",
        "features = torch.cat(features)\n",
        "features = features.view(features.size(0), -1)\n",
        "labels = torch.cat(labels)\n",
        "labels = labels.unsqueeze(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHL7hFf5FIX6",
        "outputId": "9c66cce5-9be4-4017-e6f2-29dfd13ec288"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data has been normalized\n"
          ]
        }
      ],
      "source": [
        "# Create Instance of class\n",
        "si_calculator = ARH_SeparationIndex(features, labels, normalize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnrXVi3QFQ1R"
      },
      "source": [
        "# SI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0dQQrJpFPbL",
        "outputId": "e6fa7a9b-903c-4de5-c939-a3ae188c246c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating SI: 100%|██████████| 10000/10000 [00:00<00:00, 17869.21it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.5724\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "si_data = si_calculator.si()\n",
        "print(si_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhunzvhCFT5N"
      },
      "source": [
        "# Calc High order SI (order = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vPA_Av_FSmb",
        "outputId": "d4c119e9-562e-444a-a116-d72a0e5cb399"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing High Order SI: 100%|██████████| 10000/10000 [00:00<00:00, 13112.51it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.48099997639656067\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "si_high_order_2_data = si_calculator.high_order_si(order=2)\n",
        "print(si_high_order_2_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1U16nUyFX2A"
      },
      "source": [
        "# High order soft SI (order=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCqmDkzIFWwz",
        "outputId": "663b1064-2f40-4d80-953f-6f0d7d9e63a8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating Soft Order SI: 100%|██████████| 10000/10000 [00:01<00:00, 8485.99it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Soft Order(2) SI : 0.5684499740600586\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "si_soft_order_2_data = si_calculator.soft_order_si(order=2)\n",
        "print(\"Soft Order(2) SI :\", si_soft_order_2_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HD8MpcR5FcXQ"
      },
      "source": [
        "# Center Based SI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZRm2buqFbtO",
        "outputId": "e1173b34-dece-4ef6-f6ce-009bd064554a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating Class Centers: 100%|██████████| 100/100 [00:00<00:00, 3567.62it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Center SI: 0.6365000009536743\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "center_si_data = si_calculator.center_si()\n",
        "print(\"Center SI:\", center_si_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjMHLMxmFgLb"
      },
      "source": [
        "# Anti SI (order = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNpKOuNHFfFY",
        "outputId": "283f6a62-7b90-4555-f8ca-f8a4735fe173"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating Anti-SI: 100%|██████████| 10000/10000 [00:00<00:00, 12083.23it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "anti_si: 0.3440999984741211\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "anti_si = si_calculator.anti_si(order=2)\n",
        "print(\"anti_si:\", anti_si)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw96bAAWUTEn"
      },
      "source": [
        "# Pretrained VGG16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hElXrHYUVq8",
        "outputId": "ad88e712-f3d7-4a09-f236-6a5292be7e77"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR100\n",
        "\n",
        "# Load pre-trained models\n",
        "vgg16_model = models.vgg16(pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Lam5RHK_UePN"
      },
      "outputs": [],
      "source": [
        "# Remove fully connected layers\n",
        "vgg16_features = torch.nn.Sequential(*(list(vgg16_model.features.children())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ux10PEaUjCW",
        "outputId": "7ae3408b-c360-4b26-bd75-9cfc79b7ab35"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 141/141 [00:05<00:00, 26.12it/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def extract_features(model, dataloader):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in tqdm(dataloader):\n",
        "            images, targets = images.to(device), targets.to(device)\n",
        "            outputs = model(images)\n",
        "            features.append(outputs.squeeze())\n",
        "            labels.append(targets)\n",
        "\n",
        "    features = torch.cat(features)\n",
        "    labels = torch.cat(labels)\n",
        "\n",
        "    return features, labels\n",
        "\n",
        "# Move models to GPU\n",
        "vgg16_features.to(device)\n",
        "\n",
        "resnet18_features, resnet18_labels = extract_features(vgg16_features, train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CISdd1CJUnDR",
        "outputId": "2ba3f4b1-1edd-47a4-c896-1530f1ecabc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data has been normalized\n"
          ]
        }
      ],
      "source": [
        "si_calculator = ARH_SeparationIndex(resnet18_features, resnet18_labels, normalize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJEaOT5jUv2H"
      },
      "source": [
        "# SI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTpW0JkFUumj",
        "outputId": "06565c9b-5d80-476d-af0b-02945836414a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating SI: 100%|██████████| 18/18 [00:00<00:00, 1133.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.23000000417232513\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "si_data = si_calculator.si_batch(batch_size=2000)\n",
        "print(si_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_erLCiUUyve"
      },
      "source": [
        "# Calc High order SI (order = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v94tfyw-Uxle",
        "outputId": "5ae4bbc5-3ba9-46bd-d3b2-13bbbc5bb8b3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing High Order SI: 100%|██████████| 18/18 [00:00<00:00, 1102.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.10044444352388382\n"
          ]
        }
      ],
      "source": [
        "si_high_order_2_data = si_calculator.high_order_si_batch(order=2, batch_size=2000)\n",
        "print(si_high_order_2_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TVA4grFU90g"
      },
      "source": [
        "# High order soft SI (order=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uX-_4tSRU0yM",
        "outputId": "40e5f261-fb68-4e80-a3ba-8fbcbf7ce09a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating Soft Order SI: 100%|██████████| 18/18 [00:00<00:00, 1179.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Soft Order(2) SI : 0.21113888919353485\n"
          ]
        }
      ],
      "source": [
        "si_soft_order_2_data = si_calculator.soft_order_si_batch(order=2 , batch_size=2000)\n",
        "print(\"Soft Order(2) SI :\", si_soft_order_2_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xr5xD72WJHf"
      },
      "source": [
        "# Center Based SI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCP13PpiVAd1",
        "outputId": "792b79a8-686a-451e-f567-e39fcd65778f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating CSI: 100%|██████████| 18/18 [00:00<00:00, 2658.08it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Center SI: 0.277444452047348\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "center_si_data = si_calculator.center_si_batch(batch_size=2000)\n",
        "print(\"Center SI:\", center_si_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQTsT3XjWLwd"
      },
      "source": [
        "# Anti SI (order = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVStHIogWLSi",
        "outputId": "135b36f5-6520-410d-e530-a88d7a01ccf8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating Anti-SI: 100%|██████████| 8000/8000 [00:00<00:00, 14707.54it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "anti_si: 0.7393750548362732\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "anti_si = si_calculator.anti_si(order=2)\n",
        "print(\"anti_si:\", anti_si)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpoQc2Y7r0-L"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5rq67Q4r1eU"
      },
      "source": [
        "Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wlzI1Fgr4OM",
        "outputId": "67e8ab91-2f15-4353-fe09-06ba4524ba19"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 79/79 [00:09<00:00,  8.11it/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def extract_features(model, dataloader):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in tqdm(dataloader):\n",
        "            images, targets = images.to(device), targets.to(device)\n",
        "            outputs = model(images)\n",
        "            features.append(outputs.squeeze())\n",
        "            labels.append(targets)\n",
        "\n",
        "    features = torch.cat(features)\n",
        "    labels = torch.cat(labels)\n",
        "\n",
        "    return features, labels\n",
        "\n",
        "# Move models to GPU\n",
        "vgg16_features.to(device)\n",
        "\n",
        "resnet18_features, resnet18_labels = extract_features(vgg16_features, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJJb84R0r6Pt",
        "outputId": "cf16a501-d4f5-4456-f6b4-8b548105b877"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data has been normalized\n"
          ]
        }
      ],
      "source": [
        "si_calculator = ARH_SeparationIndex(resnet18_features, resnet18_labels, normalize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "746xIILJsEUf",
        "outputId": "45a65c05-711b-4446-afcd-f45c077cf2c7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating SI: 100%|██████████| 5/5 [00:00<00:00, 450.31it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ":\n",
            "0.22019998729228973\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "si_data = si_calculator.si_batch(batch_size = 2000)\n",
        "print(\":\")\n",
        "print(si_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59fIDeK9sL0V",
        "outputId": "7079e161-93c4-4fab-e75f-4a844fa0eaa0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing High Order SI: 100%|██████████| 5/5 [00:01<00:00,  4.40it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ":\n",
            "0.09479999542236328\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "si_high_order_2_data = si_calculator.high_order_si_batch(order=2 , batch_size = 2000)\n",
        "print(\":\")\n",
        "print(si_high_order_2_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gW6TfvwAscdo",
        "outputId": "4b4c7497-4601-466f-de66-fa56e131dd73"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating Soft Order SI: 100%|██████████| 5/5 [00:00<00:00, 561.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ":\n",
            "Soft Order(2) SI : 0.20469999313354492\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "si_soft_order_2_data = si_calculator.soft_order_si_batch(order=2 ,batch_size = 2000 )\n",
        "print(\":\")\n",
        "print(\"Soft Order(2) SI :\", si_soft_order_2_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGpwBl9qsyoR",
        "outputId": "a24891cb-bbd6-478a-8503-c0335f450a50"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating CSI: 100%|██████████| 50/50 [00:00<00:00, 3091.09it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ":\n",
            "Center SI: 0.3497999906539917\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "center_si_data = si_calculator.center_si_batch(batch_size = 200)\n",
        "print(\":\")\n",
        "print(\"Center SI:\", center_si_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnxqEf1ns7qR",
        "outputId": "b89bf5f0-8e9a-49d5-ae5e-0afa57037310"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating Anti-SI: 100%|██████████| 10000/10000 [00:00<00:00, 13853.41it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "anti_si: 0.6855999827384949\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "anti_si = si_calculator.anti_si(order=2)\n",
        "print(\"anti_si:\", anti_si)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0W7Fpd46s_fE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
