{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-dVis8v-_C0"
      },
      "source": [
        "# Resnet18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfHLdLju-6ku"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "\n",
        "        #residual function\n",
        "        self.residual_function = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels * BasicBlock.expansion, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
        "        )\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != BasicBlock.expansion * out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels * BasicBlock.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return nn.ReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))\n",
        "\n",
        "class BottleNeck(nn.Module):\n",
        "    \"\"\"Residual block for resnet over 50 layers\n",
        "\n",
        "    \"\"\"\n",
        "    expansion = 4\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "        self.residual_function = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, stride=stride, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n",
        "        )\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "\n",
        "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels * BottleNeck.expansion, stride=stride, kernel_size=1, bias=False),\n",
        "                nn.BatchNorm2d(out_channels * BottleNeck.expansion)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return nn.ReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, num_block, num_classes=100):\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_channels = 64\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True))\n",
        "        #we use a different inputsize than the original paper\n",
        "        #so conv2_x's stride is 1\n",
        "        self.conv2_x = self._make_layer(block, 64, num_block[0], 1)\n",
        "        self.conv3_x = self._make_layer(block, 128, num_block[1], 2)\n",
        "        self.conv4_x = self._make_layer(block, 256, num_block[2], 2)\n",
        "        self.conv5_x = self._make_layer(block, 512, num_block[3], 2)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
        "        \"\"\"make resnet layers(by layer i didnt mean this 'layer' was the\n",
        "        same as a neuron netowork layer, ex. conv layer), one layer may\n",
        "        contain more than one residual block\n",
        "\n",
        "        Args:\n",
        "            block: block type, basic block or bottle neck block\n",
        "            out_channels: output depth channel number of this layer\n",
        "            num_blocks: how many blocks per layer\n",
        "            stride: the stride of the first block of this layer\n",
        "\n",
        "        Return:\n",
        "            return a resnet layer\n",
        "        \"\"\"\n",
        "\n",
        "        # we have num_block blocks per layer, the first block\n",
        "        # could be 1 or 2, other blocks would always be 1\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels * block.expansion\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.conv1(x)\n",
        "        output = self.conv2_x(output)\n",
        "        output = self.conv3_x(output)\n",
        "        output = self.conv4_x(output)\n",
        "        output = self.conv5_x(output)\n",
        "        output = self.avg_pool(output)\n",
        "        output = output.view(output.size(0), -1)\n",
        "        output = self.fc(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "def resnet18():\n",
        "    \"\"\" return a ResNet 18 object\n",
        "    \"\"\"\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsVh-ebq_C28"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lo6PTeTEJh_A"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split, Subset\n",
        "\n",
        "# Define the data transformations\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# Download the CIFAR-100 dataset\n",
        "train_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "# Split the dataset into training, validation, and test sets\n",
        "# Adjust the proportions based on your preferences\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = int(0.1 * len(train_dataset))\n",
        "test_size = len(train_dataset) - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(train_dataset, [train_size, val_size, test_size])\n",
        "\n",
        "batch_size = 64  # Adjust as needed\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iLoNf4WxCJW",
        "outputId": "16a0ffc8-a07b-4c18-f134-f44eac08ef88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'pytorch-cifar100'...\n",
            "remote: Enumerating objects: 1043, done.\u001b[K\n",
            "remote: Counting objects: 100% (1043/1043), done.\u001b[K\n",
            "remote: Compressing objects: 100% (386/386), done.\u001b[K\n",
            "remote: Total 1043 (delta 658), reused 1008 (delta 645), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (1043/1043), 492.58 KiB | 4.44 MiB/s, done.\n",
            "Resolving deltas: 100% (658/658), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/K-Hooshanfar/pytorch-cifar100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lA9insc0Yad",
        "outputId": "3fb439fa-f095-4d11-e071-aa21ab840aba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/pytorch-cifar100\n"
          ]
        }
      ],
      "source": [
        "%cd pytorch-cifar100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TJmhdS4YixS",
        "outputId": "59da92de-2076-4fc6-bddb-cc535fddc7b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-12-01 12:43:14.467192: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-01 12:43:14.467261: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-01 12:43:14.467309: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-01 12:43:16.080052: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n",
            "100% 169001437/169001437 [00:01<00:00, 98021343.40it/s] \n",
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Files already downloaded and verified\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 1, Average loss: 0.0283, Accuracy: 0.1490, Time consumed:3.19s\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 2, Average loss: 0.0245, Accuracy: 0.2349, Time consumed:3.67s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 3, Average loss: 0.0201, Accuracy: 0.3405, Time consumed:3.94s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 4, Average loss: 0.0198, Accuracy: 0.3565, Time consumed:3.24s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 5, Average loss: 0.0169, Accuracy: 0.4356, Time consumed:3.26s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 6, Average loss: 0.0187, Accuracy: 0.3949, Time consumed:3.20s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 7, Average loss: 0.0173, Accuracy: 0.4370, Time consumed:3.53s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 8, Average loss: 0.0144, Accuracy: 0.5112, Time consumed:4.06s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 9, Average loss: 0.0148, Accuracy: 0.4935, Time consumed:3.24s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 10, Average loss: 0.0145, Accuracy: 0.5051, Time consumed:3.26s\n",
            "\n",
            "saving weights file to checkpoint/resnet18/Friday_01_December_2023_12h_43m_17s/resnet18-10-regular.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 11, Average loss: 0.0141, Accuracy: 0.5157, Time consumed:3.19s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 12, Average loss: 0.0160, Accuracy: 0.4739, Time consumed:3.37s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 13, Average loss: 0.0150, Accuracy: 0.5085, Time consumed:4.51s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 14, Average loss: 0.0148, Accuracy: 0.5162, Time consumed:3.23s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 15, Average loss: 0.0138, Accuracy: 0.5241, Time consumed:3.27s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 16, Average loss: 0.0146, Accuracy: 0.5140, Time consumed:3.22s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 17, Average loss: 0.0148, Accuracy: 0.5047, Time consumed:3.30s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 18, Average loss: 0.0136, Accuracy: 0.5368, Time consumed:4.40s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 19, Average loss: 0.0135, Accuracy: 0.5444, Time consumed:3.62s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 20, Average loss: 0.0144, Accuracy: 0.5288, Time consumed:3.23s\n",
            "\n",
            "saving weights file to checkpoint/resnet18/Friday_01_December_2023_12h_43m_17s/resnet18-20-regular.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 21, Average loss: 0.0127, Accuracy: 0.5690, Time consumed:3.20s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 22, Average loss: 0.0140, Accuracy: 0.5432, Time consumed:3.20s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 23, Average loss: 0.0128, Accuracy: 0.5637, Time consumed:3.40s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 24, Average loss: 0.0145, Accuracy: 0.5307, Time consumed:4.31s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 25, Average loss: 0.0126, Accuracy: 0.5673, Time consumed:3.39s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 26, Average loss: 0.0134, Accuracy: 0.5509, Time consumed:3.24s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 27, Average loss: 0.0129, Accuracy: 0.5709, Time consumed:3.20s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 28, Average loss: 0.0140, Accuracy: 0.5369, Time consumed:3.19s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 29, Average loss: 0.0141, Accuracy: 0.5420, Time consumed:3.50s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 30, Average loss: 0.0132, Accuracy: 0.5527, Time consumed:4.08s\n",
            "\n",
            "saving weights file to checkpoint/resnet18/Friday_01_December_2023_12h_43m_17s/resnet18-30-regular.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 31, Average loss: 0.0136, Accuracy: 0.5531, Time consumed:3.20s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 32, Average loss: 0.0130, Accuracy: 0.5704, Time consumed:3.19s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 33, Average loss: 0.0134, Accuracy: 0.5512, Time consumed:3.18s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 34, Average loss: 0.0135, Accuracy: 0.5482, Time consumed:3.18s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 35, Average loss: 0.0128, Accuracy: 0.5575, Time consumed:3.26s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 36, Average loss: 0.0125, Accuracy: 0.5759, Time consumed:4.40s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 37, Average loss: 0.0139, Accuracy: 0.5386, Time consumed:3.43s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 38, Average loss: 0.0131, Accuracy: 0.5646, Time consumed:3.33s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 39, Average loss: 0.0134, Accuracy: 0.5517, Time consumed:3.21s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 40, Average loss: 0.0129, Accuracy: 0.5699, Time consumed:3.16s\n",
            "\n",
            "saving weights file to checkpoint/resnet18/Friday_01_December_2023_12h_43m_17s/resnet18-40-regular.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 41, Average loss: 0.0121, Accuracy: 0.5927, Time consumed:3.34s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 42, Average loss: 0.0127, Accuracy: 0.5705, Time consumed:4.17s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 43, Average loss: 0.0122, Accuracy: 0.5865, Time consumed:3.36s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 44, Average loss: 0.0125, Accuracy: 0.5793, Time consumed:3.22s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 45, Average loss: 0.0137, Accuracy: 0.5406, Time consumed:3.15s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 46, Average loss: 0.0123, Accuracy: 0.5829, Time consumed:3.24s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 47, Average loss: 0.0125, Accuracy: 0.5734, Time consumed:4.29s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 48, Average loss: 0.0138, Accuracy: 0.5474, Time consumed:3.26s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 49, Average loss: 0.0128, Accuracy: 0.5705, Time consumed:3.15s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 50, Average loss: 0.0146, Accuracy: 0.5379, Time consumed:3.22s\n",
            "\n",
            "saving weights file to checkpoint/resnet18/Friday_01_December_2023_12h_43m_17s/resnet18-50-regular.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 51, Average loss: 0.0132, Accuracy: 0.5671, Time consumed:3.30s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 52, Average loss: 0.0122, Accuracy: 0.5925, Time consumed:4.11s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 53, Average loss: 0.0130, Accuracy: 0.5639, Time consumed:3.22s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 54, Average loss: 0.0130, Accuracy: 0.5642, Time consumed:3.16s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 55, Average loss: 0.0136, Accuracy: 0.5651, Time consumed:3.18s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 56, Average loss: 0.0140, Accuracy: 0.5441, Time consumed:3.22s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 57, Average loss: 0.0137, Accuracy: 0.5553, Time consumed:3.76s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 58, Average loss: 0.0152, Accuracy: 0.5194, Time consumed:3.85s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 59, Average loss: 0.0125, Accuracy: 0.5846, Time consumed:3.23s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 60, Average loss: 0.0077, Accuracy: 0.7230, Time consumed:3.25s\n",
            "\n",
            "saving weights file to checkpoint/resnet18/Friday_01_December_2023_12h_43m_17s/resnet18-60-regular.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 61, Average loss: 0.0077, Accuracy: 0.7269, Time consumed:3.21s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 62, Average loss: 0.0079, Accuracy: 0.7259, Time consumed:3.35s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 63, Average loss: 0.0080, Accuracy: 0.7244, Time consumed:4.19s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 64, Average loss: 0.0080, Accuracy: 0.7238, Time consumed:3.21s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 65, Average loss: 0.0083, Accuracy: 0.7210, Time consumed:3.20s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 66, Average loss: 0.0088, Accuracy: 0.7153, Time consumed:3.20s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 67, Average loss: 0.0088, Accuracy: 0.7140, Time consumed:3.23s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 68, Average loss: 0.0088, Accuracy: 0.7106, Time consumed:4.07s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 69, Average loss: 0.0091, Accuracy: 0.7074, Time consumed:3.82s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 70, Average loss: 0.0092, Accuracy: 0.7048, Time consumed:3.22s\n",
            "\n",
            "saving weights file to checkpoint/resnet18/Friday_01_December_2023_12h_43m_17s/resnet18-70-regular.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 71, Average loss: 0.0093, Accuracy: 0.7059, Time consumed:3.17s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 72, Average loss: 0.0096, Accuracy: 0.6995, Time consumed:3.28s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 73, Average loss: 0.0094, Accuracy: 0.7033, Time consumed:3.66s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 74, Average loss: 0.0096, Accuracy: 0.6989, Time consumed:3.98s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 75, Average loss: 0.0098, Accuracy: 0.6907, Time consumed:3.25s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 76, Average loss: 0.0097, Accuracy: 0.6958, Time consumed:3.19s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 77, Average loss: 0.0098, Accuracy: 0.6916, Time consumed:3.15s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 78, Average loss: 0.0103, Accuracy: 0.6838, Time consumed:3.32s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 79, Average loss: 0.0105, Accuracy: 0.6827, Time consumed:4.28s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 80, Average loss: 0.0098, Accuracy: 0.6961, Time consumed:3.54s\n",
            "\n",
            "saving weights file to checkpoint/resnet18/Friday_01_December_2023_12h_43m_17s/resnet18-80-regular.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 81, Average loss: 0.0099, Accuracy: 0.6918, Time consumed:3.13s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 82, Average loss: 0.0100, Accuracy: 0.6886, Time consumed:3.14s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 83, Average loss: 0.0109, Accuracy: 0.6732, Time consumed:3.17s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 84, Average loss: 0.0103, Accuracy: 0.6837, Time consumed:3.20s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 85, Average loss: 0.0106, Accuracy: 0.6808, Time consumed:4.09s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 86, Average loss: 0.0100, Accuracy: 0.6881, Time consumed:3.84s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 87, Average loss: 0.0103, Accuracy: 0.6915, Time consumed:3.17s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 88, Average loss: 0.0107, Accuracy: 0.6825, Time consumed:3.16s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 89, Average loss: 0.0108, Accuracy: 0.6696, Time consumed:3.24s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 90, Average loss: 0.0108, Accuracy: 0.6769, Time consumed:3.17s\n",
            "\n",
            "saving weights file to checkpoint/resnet18/Friday_01_December_2023_12h_43m_17s/resnet18-90-regular.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 91, Average loss: 0.0105, Accuracy: 0.6816, Time consumed:4.35s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 92, Average loss: 0.0105, Accuracy: 0.6773, Time consumed:3.71s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 93, Average loss: 0.0106, Accuracy: 0.6810, Time consumed:3.21s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 94, Average loss: 0.0109, Accuracy: 0.6749, Time consumed:3.16s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 95, Average loss: 0.0102, Accuracy: 0.6859, Time consumed:3.14s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 96, Average loss: 0.0109, Accuracy: 0.6736, Time consumed:3.11s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 97, Average loss: 0.0113, Accuracy: 0.6643, Time consumed:3.73s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 98, Average loss: 0.0108, Accuracy: 0.6720, Time consumed:4.07s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 99, Average loss: 0.0102, Accuracy: 0.6834, Time consumed:3.31s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 100, Average loss: 0.0104, Accuracy: 0.6885, Time consumed:3.16s\n",
            "\n",
            "saving weights file to checkpoint/resnet18/Friday_01_December_2023_12h_43m_17s/resnet18-100-regular.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 101, Average loss: 0.0106, Accuracy: 0.6902, Time consumed:3.13s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 102, Average loss: 0.0104, Accuracy: 0.6883, Time consumed:3.18s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 103, Average loss: 0.0105, Accuracy: 0.6772, Time consumed:3.32s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 104, Average loss: 0.0108, Accuracy: 0.6786, Time consumed:4.23s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 105, Average loss: 0.0110, Accuracy: 0.6717, Time consumed:3.21s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 106, Average loss: 0.0113, Accuracy: 0.6744, Time consumed:3.14s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 107, Average loss: 0.0112, Accuracy: 0.6650, Time consumed:3.14s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 108, Average loss: 0.0112, Accuracy: 0.6695, Time consumed:3.16s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 109, Average loss: 0.0110, Accuracy: 0.6793, Time consumed:4.01s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 110, Average loss: 0.0110, Accuracy: 0.6768, Time consumed:3.88s\n",
            "\n",
            "saving weights file to checkpoint/resnet18/Friday_01_December_2023_12h_43m_17s/resnet18-110-regular.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 111, Average loss: 0.0106, Accuracy: 0.6820, Time consumed:3.18s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 112, Average loss: 0.0111, Accuracy: 0.6755, Time consumed:3.18s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 113, Average loss: 0.0107, Accuracy: 0.6780, Time consumed:3.19s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 114, Average loss: 0.0117, Accuracy: 0.6578, Time consumed:3.27s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 115, Average loss: 0.0110, Accuracy: 0.6808, Time consumed:4.20s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 116, Average loss: 0.0111, Accuracy: 0.6677, Time consumed:3.25s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 117, Average loss: 0.0117, Accuracy: 0.6657, Time consumed:3.21s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 118, Average loss: 0.0113, Accuracy: 0.6737, Time consumed:3.19s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 119, Average loss: 0.0125, Accuracy: 0.6482, Time consumed:3.18s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 120, Average loss: 0.0085, Accuracy: 0.7406, Time consumed:3.58s\n",
            "\n",
            "saving weights file to checkpoint/resnet18/Friday_01_December_2023_12h_43m_17s/resnet18-120-regular.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 121, Average loss: 0.0084, Accuracy: 0.7436, Time consumed:3.93s\n",
            "\n",
            "saving weights file to checkpoint/resnet18/Friday_01_December_2023_12h_43m_17s/resnet18-121-best.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 122, Average loss: 0.0084, Accuracy: 0.7476, Time consumed:3.23s\n",
            "\n",
            "saving weights file to checkpoint/resnet18/Friday_01_December_2023_12h_43m_17s/resnet18-122-best.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 123, Average loss: 0.0083, Accuracy: 0.7457, Time consumed:3.15s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 124, Average loss: 0.0083, Accuracy: 0.7447, Time consumed:3.18s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 125, Average loss: 0.0083, Accuracy: 0.7455, Time consumed:3.28s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 126, Average loss: 0.0083, Accuracy: 0.7479, Time consumed:4.16s\n",
            "\n",
            "saving weights file to checkpoint/resnet18/Friday_01_December_2023_12h_43m_17s/resnet18-126-best.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 127, Average loss: 0.0082, Accuracy: 0.7522, Time consumed:3.26s\n",
            "\n",
            "saving weights file to checkpoint/resnet18/Friday_01_December_2023_12h_43m_17s/resnet18-127-best.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 128, Average loss: 0.0081, Accuracy: 0.7513, Time consumed:3.19s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 129, Average loss: 0.0082, Accuracy: 0.7499, Time consumed:3.21s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 130, Average loss: 0.0081, Accuracy: 0.7487, Time consumed:3.22s\n",
            "\n",
            "saving weights file to checkpoint/resnet18/Friday_01_December_2023_12h_43m_17s/resnet18-130-regular.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 131, Average loss: 0.0080, Accuracy: 0.7496, Time consumed:3.46s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 132, Average loss: 0.0081, Accuracy: 0.7486, Time consumed:4.01s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 133, Average loss: 0.0081, Accuracy: 0.7495, Time consumed:3.23s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 134, Average loss: 0.0081, Accuracy: 0.7504, Time consumed:3.16s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 135, Average loss: 0.0080, Accuracy: 0.7525, Time consumed:3.20s\n",
            "\n",
            "saving weights file to checkpoint/resnet18/Friday_01_December_2023_12h_43m_17s/resnet18-135-best.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 136, Average loss: 0.0080, Accuracy: 0.7536, Time consumed:3.41s\n",
            "\n",
            "saving weights file to checkpoint/resnet18/Friday_01_December_2023_12h_43m_17s/resnet18-136-best.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 137, Average loss: 0.0081, Accuracy: 0.7510, Time consumed:4.18s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 138, Average loss: 0.0081, Accuracy: 0.7518, Time consumed:3.53s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 139, Average loss: 0.0079, Accuracy: 0.7517, Time consumed:3.17s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 140, Average loss: 0.0080, Accuracy: 0.7533, Time consumed:3.16s\n",
            "\n",
            "saving weights file to checkpoint/resnet18/Friday_01_December_2023_12h_43m_17s/resnet18-140-regular.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 141, Average loss: 0.0081, Accuracy: 0.7476, Time consumed:3.18s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 142, Average loss: 0.0080, Accuracy: 0.7517, Time consumed:3.19s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 143, Average loss: 0.0079, Accuracy: 0.7557, Time consumed:4.18s\n",
            "\n",
            "saving weights file to checkpoint/resnet18/Friday_01_December_2023_12h_43m_17s/resnet18-143-best.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 144, Average loss: 0.0079, Accuracy: 0.7520, Time consumed:3.78s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 145, Average loss: 0.0079, Accuracy: 0.7567, Time consumed:3.16s\n",
            "\n",
            "saving weights file to checkpoint/resnet18/Friday_01_December_2023_12h_43m_17s/resnet18-145-best.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 146, Average loss: 0.0079, Accuracy: 0.7547, Time consumed:3.22s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 147, Average loss: 0.0078, Accuracy: 0.7564, Time consumed:3.19s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 148, Average loss: 0.0078, Accuracy: 0.7547, Time consumed:3.17s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 149, Average loss: 0.0079, Accuracy: 0.7556, Time consumed:3.68s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 150, Average loss: 0.0078, Accuracy: 0.7545, Time consumed:3.92s\n",
            "\n",
            "saving weights file to checkpoint/resnet18/Friday_01_December_2023_12h_43m_17s/resnet18-150-regular.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 151, Average loss: 0.0078, Accuracy: 0.7518, Time consumed:3.32s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 152, Average loss: 0.0078, Accuracy: 0.7522, Time consumed:3.23s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 153, Average loss: 0.0078, Accuracy: 0.7574, Time consumed:3.17s\n",
            "\n",
            "saving weights file to checkpoint/resnet18/Friday_01_December_2023_12h_43m_17s/resnet18-153-best.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 154, Average loss: 0.0078, Accuracy: 0.7534, Time consumed:3.21s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 155, Average loss: 0.0077, Accuracy: 0.7559, Time consumed:4.05s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 156, Average loss: 0.0078, Accuracy: 0.7579, Time consumed:3.20s\n",
            "\n",
            "saving weights file to checkpoint/resnet18/Friday_01_December_2023_12h_43m_17s/resnet18-156-best.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 157, Average loss: 0.0077, Accuracy: 0.7551, Time consumed:3.13s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 158, Average loss: 0.0078, Accuracy: 0.7552, Time consumed:3.20s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 159, Average loss: 0.0078, Accuracy: 0.7560, Time consumed:3.39s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 160, Average loss: 0.0077, Accuracy: 0.7583, Time consumed:4.07s\n",
            "\n",
            "saving weights file to checkpoint/resnet18/Friday_01_December_2023_12h_43m_17s/resnet18-160-best.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 161, Average loss: 0.0077, Accuracy: 0.7587, Time consumed:3.14s\n",
            "\n",
            "saving weights file to checkpoint/resnet18/Friday_01_December_2023_12h_43m_17s/resnet18-161-best.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 162, Average loss: 0.0078, Accuracy: 0.7601, Time consumed:3.18s\n",
            "\n",
            "saving weights file to checkpoint/resnet18/Friday_01_December_2023_12h_43m_17s/resnet18-162-best.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 163, Average loss: 0.0077, Accuracy: 0.7599, Time consumed:3.15s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 164, Average loss: 0.0076, Accuracy: 0.7593, Time consumed:3.23s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 165, Average loss: 0.0077, Accuracy: 0.7585, Time consumed:3.98s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 166, Average loss: 0.0076, Accuracy: 0.7593, Time consumed:3.16s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 167, Average loss: 0.0077, Accuracy: 0.7591, Time consumed:3.15s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 168, Average loss: 0.0077, Accuracy: 0.7606, Time consumed:3.09s\n",
            "\n",
            "saving weights file to checkpoint/resnet18/Friday_01_December_2023_12h_43m_17s/resnet18-168-best.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 169, Average loss: 0.0076, Accuracy: 0.7601, Time consumed:3.14s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 170, Average loss: 0.0077, Accuracy: 0.7612, Time consumed:3.80s\n",
            "\n",
            "saving weights file to checkpoint/resnet18/Friday_01_December_2023_12h_43m_17s/resnet18-170-best.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 171, Average loss: 0.0077, Accuracy: 0.7604, Time consumed:3.84s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 172, Average loss: 0.0077, Accuracy: 0.7593, Time consumed:3.18s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 173, Average loss: 0.0076, Accuracy: 0.7603, Time consumed:3.18s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 174, Average loss: 0.0077, Accuracy: 0.7583, Time consumed:3.15s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 175, Average loss: 0.0076, Accuracy: 0.7584, Time consumed:3.41s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 176, Average loss: 0.0077, Accuracy: 0.7608, Time consumed:4.13s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 177, Average loss: 0.0076, Accuracy: 0.7597, Time consumed:3.15s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 178, Average loss: 0.0077, Accuracy: 0.7606, Time consumed:3.15s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 179, Average loss: 0.0077, Accuracy: 0.7596, Time consumed:3.28s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 180, Average loss: 0.0077, Accuracy: 0.7610, Time consumed:3.17s\n",
            "\n",
            "saving weights file to checkpoint/resnet18/Friday_01_December_2023_12h_43m_17s/resnet18-180-regular.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 181, Average loss: 0.0076, Accuracy: 0.7591, Time consumed:3.80s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 182, Average loss: 0.0077, Accuracy: 0.7603, Time consumed:3.76s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 183, Average loss: 0.0077, Accuracy: 0.7610, Time consumed:3.21s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 184, Average loss: 0.0077, Accuracy: 0.7599, Time consumed:3.18s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 185, Average loss: 0.0076, Accuracy: 0.7587, Time consumed:3.16s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 186, Average loss: 0.0077, Accuracy: 0.7601, Time consumed:3.39s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 187, Average loss: 0.0076, Accuracy: 0.7593, Time consumed:3.95s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 188, Average loss: 0.0077, Accuracy: 0.7598, Time consumed:3.14s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 189, Average loss: 0.0077, Accuracy: 0.7598, Time consumed:3.16s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 190, Average loss: 0.0076, Accuracy: 0.7609, Time consumed:3.24s\n",
            "\n",
            "saving weights file to checkpoint/resnet18/Friday_01_December_2023_12h_43m_17s/resnet18-190-regular.pth\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 191, Average loss: 0.0077, Accuracy: 0.7592, Time consumed:3.47s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 192, Average loss: 0.0077, Accuracy: 0.7596, Time consumed:4.03s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 193, Average loss: 0.0076, Accuracy: 0.7603, Time consumed:3.13s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 194, Average loss: 0.0076, Accuracy: 0.7607, Time consumed:3.13s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 195, Average loss: 0.0077, Accuracy: 0.7612, Time consumed:3.13s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 196, Average loss: 0.0076, Accuracy: 0.7608, Time consumed:3.39s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 197, Average loss: 0.0077, Accuracy: 0.7595, Time consumed:4.09s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 198, Average loss: 0.0076, Accuracy: 0.7601, Time consumed:3.17s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 199, Average loss: 0.0077, Accuracy: 0.7611, Time consumed:3.15s\n",
            "\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 200, Average loss: 0.0076, Accuracy: 0.7610, Time consumed:3.15s\n",
            "\n",
            "saving weights file to checkpoint/resnet18/Friday_01_December_2023_12h_43m_17s/resnet18-200-regular.pth\n",
            "Figure(1000x500)\n"
          ]
        }
      ],
      "source": [
        "!python train.py -net resnet18 -gpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnMf4bso_HhM"
      },
      "source": [
        "# Save weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgnHz4bE_ISL",
        "outputId": "c15ac7a0-8aa7-450f-e684-9cc7f756b27e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "FG-AJHi3_JNO",
        "outputId": "cda29340-8924-47dd-cfc9-89f04a9b0617"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/gdrive/My Drive/resnet18-200-regular.pth'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import shutil\n",
        "\n",
        "source_path = '/content/pytorch-cifar100/checkpoint/resnet18/Friday_01_December_2023_12h_43m_17s/resnet18-200-regular.pth'\n",
        "destination_path = '/content/gdrive/My Drive/resnet18-200-regular.pth'\n",
        "\n",
        "shutil.copyfile(source_path, destination_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hlEzakd_Tcl",
        "outputId": "27f36ff7-0591-472c-a51e-ed9e723d2ba7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = resnet18()\n",
        "\n",
        "# Load pre-trained weights\n",
        "pretrained_weights_path = '/content/pytorch-cifar100/checkpoint/resnet18/Friday_01_December_2023_12h_43m_17s/resnet18-200-regular.pth'\n",
        "model.load_state_dict(torch.load(pretrained_weights_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6qBwnGT_VN2",
        "outputId": "82ac6354-84ef-46f9-b86a-d129f017c181"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Test set size: 10000\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "# Define data transformations\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# Load CIFAR-100 dataset\n",
        "cifar100_test = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "\n",
        "# Create PyTorch data loaders\n",
        "batch_size = 128\n",
        "val_loader = torch.utils.data.DataLoader(cifar100_test, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Check the number of samples in each set\n",
        "print(f\"Test set size: {len(cifar100_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJ6RJiqY_hoD",
        "outputId": "56059f61-3aee-496f-ad03-cf92d1c68bb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Test Accuracy: 76.28%\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "correct_test = 0\n",
        "total_test = 0\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move the model to the device\n",
        "model.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in val_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_test += labels.size(0)\n",
        "        correct_test += (predicted == labels).sum().item()\n",
        "\n",
        "test_accuracy = 100 * correct_test / total_test\n",
        "print(f\"Final Test Accuracy: {test_accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tExJpDC2_l_W"
      },
      "source": [
        "# Load Model and Get Feature Latent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYYcP-4z_j-l",
        "outputId": "ebc21114-cf32-4f46-c7a9-feacbca88a6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Balanced Train set size: 36000\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "random_seed = 42\n",
        "torch.manual_seed(random_seed)\n",
        "np.random.seed(random_seed)\n",
        "random.seed(random_seed)\n",
        "\n",
        "# Load the CIFAR-100 dataset and create a balanced subset\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "# Load CIFAR-100 dataset\n",
        "cifar100_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_size = int(0.8 * len(cifar100_dataset))\n",
        "val_size = len(cifar100_dataset) - train_size\n",
        "cifar100_traindataset, cifar100_valdataset = torch.utils.data.random_split(cifar100_dataset, [train_size, val_size])\n",
        "\n",
        "# Define the subset size\n",
        "subset_fraction = 0.9\n",
        "subset_size_train = int(subset_fraction * len(cifar100_traindataset))\n",
        "\n",
        "class_indices = list(range(len(cifar100_traindataset.dataset.classes)))\n",
        "class_subset_size = int(subset_size_train / len(cifar100_traindataset.dataset.classes))\n",
        "\n",
        "class_sampler_indices_train = []\n",
        "\n",
        "for class_index in class_indices:\n",
        "    class_indices_list_train = [i for i, label in enumerate(cifar100_traindataset.dataset.targets) if label == class_index]\n",
        "    class_sampler_indices_train.extend(class_indices_list_train[:class_subset_size])\n",
        "\n",
        "train_sampler = SubsetRandomSampler(class_sampler_indices_train)\n",
        "\n",
        "batch_size = 256\n",
        "train_loader = torch.utils.data.DataLoader(cifar100_traindataset, batch_size=batch_size, sampler=train_sampler)\n",
        "\n",
        "# Check the number of samples in the balanced train set\n",
        "print(f\"Balanced Train set size: {len(train_loader.sampler)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1XD2s_A_oW2",
        "outputId": "0fab97df-e5e7-47b9-e8d2-eeedb113ea1b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 79/79 [00:13<00:00,  5.88it/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Instantiate the model\n",
        "model = resnet18()\n",
        "model.to(device)\n",
        "\n",
        "# Load pre-trained weights\n",
        "pretrained_weights_path = '/content/pytorch-cifar100/checkpoint/resnet18/Friday_01_December_2023_12h_43m_17s/resnet18-200-regular.pth'\n",
        "model.load_state_dict(torch.load(pretrained_weights_path))\n",
        "\n",
        "# Remove the fully connected layer\n",
        "model = nn.Sequential(*(list(model.children())[:-1]))\n",
        "\n",
        "features = []\n",
        "labels = []\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in tqdm(train_loader):\n",
        "        if torch.cuda.is_available():\n",
        "            inputs = inputs.to('cuda')\n",
        "\n",
        "        # Forward pass through the model\n",
        "        features_batch = model(inputs)\n",
        "\n",
        "        # Append the extracted features and labels\n",
        "        features.append(features_batch)  # Assuming you want to save features as numpy arrays\n",
        "        labels.append(targets)\n",
        "\n",
        "        # Release GPU memory\n",
        "        del inputs\n",
        "        torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hmuj01Sa_tSY"
      },
      "outputs": [],
      "source": [
        "# Stack and reshape the extracted features\n",
        "features = torch.cat(features)\n",
        "features = features.view(features.size(0), -1)\n",
        "labels = torch.cat(labels)\n",
        "labels = labels.unsqueeze(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGBNrq3bBg_r"
      },
      "source": [
        "# Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKwwbfZABgME",
        "outputId": "b93979e5-6e63-43f2-f4a2-02ab5d28a3a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'data_complexity_measures' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Arhosseini77/data_complexity_measures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FYMaTOC-BjAD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "import numpy as np\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "from data_complexity_measures.models.ARH_SeparationIndex import ARH_SeparationIndex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6czo-WCBsTZ",
        "outputId": "596f471d-eba5-4661-cd61-fe586f0dcafd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data has been normalized\n"
          ]
        }
      ],
      "source": [
        "# Create Instance of class\n",
        "si_calculator = ARH_SeparationIndex(features, labels, normalize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIdrjhlpBuTj"
      },
      "source": [
        "# SI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjycn01wBtTU",
        "outputId": "d933c987-3e56-406f-8cbe-294fb5e84ca7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating SI: 100%|██████████| 20000/20000 [00:01<00:00, 13264.24it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.4858\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "si_data = si_calculator.si()\n",
        "print(si_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scuUYrfrBw_T"
      },
      "source": [
        "# Calc High order SI (order = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1C9hByHzBwP4",
        "outputId": "aed6c362-3ae2-4cce-d992-851b5f68baa9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing High Order SI: 100%|██████████| 20000/20000 [00:02<00:00, 8999.25it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.3387500047683716\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "si_high_order_2_data = si_calculator.high_order_si(order=2)\n",
        "print(si_high_order_2_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpGs6G42B1Gf"
      },
      "source": [
        "# High order soft SI (order=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0O0BL11kB0fz",
        "outputId": "4c20159f-d5df-458c-c088-6f1330a4b431"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating Soft Order SI: 100%|██████████| 20000/20000 [00:01<00:00, 11608.36it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Soft Order(2) SI : 0.46242499351501465\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "si_soft_order_2_data = si_calculator.soft_order_si(order=2)\n",
        "print(\"Soft Order(2) SI :\", si_soft_order_2_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gtGlR1pB5Wj"
      },
      "source": [
        "# Center Based SI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBtPb7RFB3nh",
        "outputId": "e947dfc1-b033-49fd-dd7a-3d86aafd10a0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating Class Centers: 100%|██████████| 100/100 [00:00<00:00, 5365.28it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Center SI: 0.5596500039100647\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "center_si_data = si_calculator.center_si()\n",
        "print(\"Center SI:\", center_si_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ew9wlFuB8E7"
      },
      "source": [
        "# Anti SI (order = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTmcJDRvB7DE",
        "outputId": "25a65d4c-2719-4ede-c910-eebbb1f0bdfb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating Anti-SI: 100%|██████████| 20000/20000 [00:01<00:00, 12144.45it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "anti_si: 0.4138999879360199\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "anti_si = si_calculator.anti_si(order=2)\n",
        "print(\"anti_si:\", anti_si)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SV1Dd6MXFmka"
      },
      "source": [
        "# Metrics for test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJbFd8SCFoLu",
        "outputId": "9eca2304-c1e6-464a-fbf5-b686cf0be5ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "# Define data transformations\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# Load CIFAR-100 dataset\n",
        "cifar100_test = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Create PyTorch data loaders\n",
        "batch_size = 128\n",
        "test_loader = torch.utils.data.DataLoader(cifar100_test, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0W-3CfoFqMf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Instantiate the model\n",
        "model = resnet18()\n",
        "model.to(device)\n",
        "\n",
        "# Load pre-trained weights\n",
        "pretrained_weights_path = '/content/pytorch-cifar100/checkpoint/resnet18/Friday_01_December_2023_12h_43m_17s/resnet18-200-regular.pth'\n",
        "model.load_state_dict(torch.load(pretrained_weights_path))\n",
        "\n",
        "# Remove the fully connected layer\n",
        "model = nn.Sequential(*(list(model.children())[:-1]))\n",
        "\n",
        "features = []\n",
        "labels = []\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in tqdm(test_loader):\n",
        "        if torch.cuda.is_available():\n",
        "            inputs = inputs.to('cuda')\n",
        "\n",
        "        # Forward pass through the model\n",
        "        features_batch = model(inputs)\n",
        "\n",
        "        # Append the extracted features and labels\n",
        "        features.append(features_batch)  # Assuming you want to save features as numpy arrays\n",
        "        labels.append(targets)\n",
        "\n",
        "        # Release GPU memory\n",
        "        del inputs\n",
        "        torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUpAWv_sFw06"
      },
      "outputs": [],
      "source": [
        "# Stack and reshape the extracted features\n",
        "features = torch.cat(features)\n",
        "features = features.view(features.size(0), -1)\n",
        "labels = torch.cat(labels)\n",
        "labels = labels.unsqueeze(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AcNb2cgFx-3",
        "outputId": "0acbe13a-0f55-4785-8a7b-d451a77a837e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data has been normalized\n"
          ]
        }
      ],
      "source": [
        "# Create Instance of class\n",
        "si_calculator = ARH_SeparationIndex(features, labels, normalize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBYT18ytFzQr"
      },
      "source": [
        "# SI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sr-7euI6Fy4Y",
        "outputId": "2f016c27-4b04-4459-bdde-472515f2c5bf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating SI: 100%|██████████| 10000/10000 [00:00<00:00, 13251.85it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.599\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "si_data = si_calculator.si()\n",
        "print(si_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpIPqJ-dF1iS"
      },
      "source": [
        "# Calc High order SI (order = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jy_NzPwEF1G5",
        "outputId": "82923dda-e8e0-4837-ea2b-b1a83728f39d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing High Order SI: 100%|██████████| 10000/10000 [00:00<00:00, 18445.21it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.4852999746799469\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "si_high_order_2_data = si_calculator.high_order_si(order=2)\n",
        "print(si_high_order_2_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0k3a6fvGLcr"
      },
      "source": [
        "# High order soft SI (order=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLWzfzirF36_",
        "outputId": "cdea00de-8232-40b3-caf2-b317b7aa4759"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating Soft Order SI: 100%|██████████| 10000/10000 [00:01<00:00, 6000.65it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Soft Order(2) SI : 0.5898500084877014\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "si_soft_order_2_data = si_calculator.soft_order_si(order=2)\n",
        "print(\"Soft Order(2) SI :\", si_soft_order_2_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQGLHVdPGN-L"
      },
      "source": [
        "# Center Based SI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dmbSE2fGNe4",
        "outputId": "2a8c4050-5b05-4f22-880a-b28746505ecc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating Class Centers: 100%|██████████| 100/100 [00:00<00:00, 2519.49it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Center SI: 0.7026000022888184\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "center_si_data = si_calculator.center_si()\n",
        "print(\"Center SI:\", center_si_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OIZfmGMGRCJ"
      },
      "source": [
        "# Anti SI (order = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgjFK59yGQVq",
        "outputId": "609235f4-3b18-4889-c81f-9a510a5d86eb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating Anti-SI: 100%|██████████| 10000/10000 [00:00<00:00, 11461.91it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "anti_si: 0.30559998750686646\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "anti_si = si_calculator.anti_si(order=2)\n",
        "print(\"anti_si:\", anti_si)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyssB6FyN90D"
      },
      "source": [
        "# Pretrained Resnet18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JQCux5mOhqq",
        "outputId": "f8629058-28fe-4abf-86e7-ff0d2312d5a6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR100\n",
        "\n",
        "# Load pre-trained models\n",
        "resnet18_model = models.resnet18(pretrained=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pvCxYp2iOmM7"
      },
      "outputs": [],
      "source": [
        "# Remove fully connected layers\n",
        "resnet18_features = torch.nn.Sequential(*(list(resnet18_model.children())[:-1]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHfWKPXVOr12",
        "outputId": "482d76b3-4bc0-4625-902d-b7688fd1e845"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 141/141 [00:05<00:00, 23.73it/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def extract_features(model, dataloader):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in tqdm(dataloader):\n",
        "            images, targets = images.to(device), targets.to(device)\n",
        "            outputs = model(images)\n",
        "            features.append(outputs.squeeze())\n",
        "            labels.append(targets)\n",
        "\n",
        "    features = torch.cat(features)\n",
        "    labels = torch.cat(labels)\n",
        "\n",
        "    return features, labels\n",
        "\n",
        "# Move models to GPU\n",
        "resnet18_features.to(device)\n",
        "\n",
        "resnet18_features, resnet18_labels = extract_features(resnet18_features, train_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_MztKNPO-c7",
        "outputId": "ca45c534-676d-44ab-d4a0-47a74a2b7694"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data has been normalized\n"
          ]
        }
      ],
      "source": [
        "si_calculator = ARH_SeparationIndex(resnet18_features, resnet18_labels, normalize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dcz7hQmMPM8h"
      },
      "source": [
        "# SI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwDgIc0UPKtH",
        "outputId": "796ba722-4eba-4956-c3d6-ec42fec1796c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating SI: 100%|██████████| 18/18 [00:00<00:00, 1816.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.22875000536441803\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "si_data = si_calculator.si_batch(batch_size=2000)\n",
        "print(si_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hVdTHSkPX_R"
      },
      "source": [
        "# Calc High order SI (order = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bGHdiHOPWST",
        "outputId": "76585fd9-49a9-4056-8f44-575908dd4487"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing High Order SI: 100%|██████████| 18/18 [00:01<00:00, 17.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.09644444286823273\n"
          ]
        }
      ],
      "source": [
        "si_high_order_2_data = si_calculator.high_order_si_batch(order=2, batch_size = 2000)\n",
        "print(si_high_order_2_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfsF5BykPnku"
      },
      "source": [
        "# High order soft SI (order=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVAykgXkPZCk",
        "outputId": "b31bba07-bc7a-487c-ddf9-04e6e8773dc7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating Soft Order SI: 100%|██████████| 18/18 [00:00<00:00, 1097.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Soft Order(2) SI : 0.2048194408416748\n"
          ]
        }
      ],
      "source": [
        "si_soft_order_2_data = si_calculator.soft_order_si_batch(order=2,batch_size=2000)\n",
        "print(\"Soft Order(2) SI :\", si_soft_order_2_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFaez1m0Q6AC"
      },
      "source": [
        "# Center Based SI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfeipvQdPp_K",
        "outputId": "1c5ab6e9-05a3-49ca-eaf1-6f345aaab6e7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating CSI: 100%|██████████| 18/18 [00:00<00:00, 2524.58it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Center SI: 0.234333336353302\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "center_si_data = si_calculator.center_si_batch(batch_size=2000)\n",
        "print(\"Center SI:\", center_si_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJsiYcJIQ-Tr"
      },
      "source": [
        "# Anti SI (order = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUswjBleQ9Xz",
        "outputId": "f7eca409-a57a-455f-b458-03aa35fe1f46"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating Anti-SI: 100%|██████████| 8000/8000 [00:00<00:00, 14096.75it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "anti_si: 0.7402500510215759\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "anti_si = si_calculator.anti_si(order=2)\n",
        "print(\"anti_si:\", anti_si)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AhQ67EzyM1y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gicn4kqCyNeo"
      },
      "source": [
        "Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9sZGSD4xwD2",
        "outputId": "37a6ba50-75a1-4d45-a11a-e516dae41233"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 79/79 [00:04<00:00, 19.63it/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def extract_features(model, dataloader):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in tqdm(dataloader):\n",
        "            images, targets = images.to(device), targets.to(device)\n",
        "            outputs = model(images)\n",
        "            features.append(outputs.squeeze())\n",
        "            labels.append(targets)\n",
        "\n",
        "    features = torch.cat(features)\n",
        "    labels = torch.cat(labels)\n",
        "\n",
        "    return features, labels\n",
        "\n",
        "# Move models to GPU\n",
        "resnet18_features.to(device)\n",
        "\n",
        "resnet18_features, resnet18_labels = extract_features(resnet18_features, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_0PCbVRyo-V",
        "outputId": "1147ed53-8837-4d90-85fb-ae11befad158"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data has been normalized\n"
          ]
        }
      ],
      "source": [
        "si_calculator = ARH_SeparationIndex(resnet18_features, resnet18_labels, normalize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dp20Du-7ysHz",
        "outputId": "32a5843c-f5b1-4f2b-a1f6-129dce76ab25"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating SI: 100%|██████████| 5/5 [00:00<00:00, 3038.47it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ":\n",
            "0.21709999442100525\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "si_data = si_calculator.si_batch(batch_size=2000)\n",
        "print(\":\")\n",
        "print(si_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkuPxVjIyvwV",
        "outputId": "b6ae740f-4790-402c-eaea-afccbf921a6f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing High Order SI: 100%|██████████| 5/5 [00:00<00:00, 126.27it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ":\n",
            "0.08919999748468399\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "si_high_order_2_data = si_calculator.high_order_si_batch(order=2, batch_size = 2000)\n",
        "print(\":\")\n",
        "print(si_high_order_2_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDWtoHYfyzrx",
        "outputId": "35757cf8-9e44-4e3a-8429-defb2d9f04bf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating Soft Order SI: 100%|██████████| 5/5 [00:00<00:00, 700.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ":\n",
            "Soft Order(2) SI : 0.19589999318122864\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "si_soft_order_2_data = si_calculator.soft_order_si_batch(order=2,batch_size=2000)\n",
        "print(\":\")\n",
        "print(\"Soft Order(2) SI :\", si_soft_order_2_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkmtENkLy3S9",
        "outputId": "d918f2b8-4ca5-4fc6-a189-308562916755"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating CSI: 100%|██████████| 5/5 [00:00<00:00, 1171.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ":\n",
            "Center SI: 0.29089999198913574\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "center_si_data = si_calculator.center_si_batch(batch_size=2000)\n",
        "print(\":\")\n",
        "print(\"Center SI:\", center_si_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hINSsQdIy8F1",
        "outputId": "bda39c4d-07c7-43de-dc2b-5d5cd7f87119"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating Anti-SI: 100%|██████████| 10000/10000 [00:01<00:00, 9306.41it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ":\n",
            "anti_si: 0.6976000070571899\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "anti_si = si_calculator.anti_si(order=2)\n",
        "print(\":\")\n",
        "print(\"anti_si:\", anti_si)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xl8nprSxzDan"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
