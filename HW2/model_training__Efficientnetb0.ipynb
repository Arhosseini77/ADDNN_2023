{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgmBELJRsTma"
      },
      "source": [
        "# Efficientnetb0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vs1_OWRGzVaq"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class conv_bn_act(nn.Module):\n",
        "    def __init__(self, inchannels, outchannels, kernelsize, stride=1, dilation=1, groups=1, bias=False, bn_momentum=0.99):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            SameConv(inchannels, outchannels, kernelsize, stride, dilation, groups, bias=bias),\n",
        "            nn.BatchNorm2d(outchannels, momentum=1-bn_momentum),\n",
        "            swish()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "class SameConv(nn.Conv2d):\n",
        "    def __init__(self, inchannels, outchannels, kernelsize, stride=1, dilation=1, groups=1, bias=False):\n",
        "        super().__init__(inchannels, outchannels, kernelsize, stride,\n",
        "                         padding=0, dilation=dilation, groups=groups, bias=bias)\n",
        "\n",
        "    def how_padding(self, n, kernel, stride, dilation):\n",
        "        out_size = (n + stride - 1) // stride\n",
        "        real_kernel = (kernel - 1) * dilation + 1\n",
        "        padding_needed = max(0, (out_size - 1) * stride + real_kernel - n)\n",
        "        is_odd = padding_needed % 2\n",
        "        return padding_needed, is_odd\n",
        "\n",
        "    def forward(self, x):\n",
        "        row_padding_needed, row_is_odd = self.how_padding(x.size(2), self.weight.size(2), self.stride[0], self.dilation[0])\n",
        "        col_padding_needed, col_is_odd = self.how_padding(x.size(3), self.weight.size(3), self.stride[1], self.dilation[1])\n",
        "        if row_is_odd or col_is_odd:\n",
        "            x = F.pad(x, [0, col_is_odd, 0, row_is_odd])\n",
        "\n",
        "        return F.conv2d(x, self.weight, self.bias, self.stride,\n",
        "                        (row_padding_needed//2, col_padding_needed//2), self.dilation, self.groups)\n",
        "\n",
        "class swish(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * torch.sigmoid(x)\n",
        "\n",
        "\n",
        "class SE(nn.Module):\n",
        "    def __init__(self, inchannels, mid):\n",
        "        super().__init__()\n",
        "        self.AvgPool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.SEblock = nn.Sequential(\n",
        "            nn.Linear(inchannels, mid),\n",
        "            swish(),\n",
        "            nn.Linear(mid, inchannels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.AvgPool(x)\n",
        "        out = out.view(x.size(0), -1)\n",
        "        out = self.SEblock(out)\n",
        "        out = out.view(x.size(0), x.size(1), 1, 1)\n",
        "        return x * torch.sigmoid(out)\n",
        "\n",
        "\n",
        "class drop_connect(nn.Module):\n",
        "    def __init__(self, survival=0.8):\n",
        "        super().__init__()\n",
        "        self.survival = survival\n",
        "\n",
        "    def forward(self, x):\n",
        "        if not self.training:\n",
        "            return x\n",
        "\n",
        "        random = torch.rand((x.size(0), 1, 1, 1), device=x.device)\n",
        "        random += self.survival\n",
        "        random.requires_grad = False\n",
        "        return x / self.survival * torch.floor(random)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o885Vy-NzRyT"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class MBConv(nn.Module):\n",
        "    def __init__(self, inchannels, outchannels, expan, kernelsize, stride, se_ratio=4,\n",
        "                 is_skip=True, dc_ratio=(1-0.8), bn_momentum=0.90):\n",
        "        super().__init__()\n",
        "        mid = expan * inchannels\n",
        "        self.pointwise1 = conv_bn_act(inchannels, mid, 1) if expan != 1 else nn.Identity()\n",
        "        self.depthwise = conv_bn_act(mid, mid, kernelsize, stride=stride, groups=mid)\n",
        "        self.se = SE(mid, int(inchannels/se_ratio))\n",
        "        self.pointwise2 = nn.Sequential(\n",
        "            SameConv(mid, outchannels, 1),\n",
        "            nn.BatchNorm2d(outchannels, 1-bn_momentum)\n",
        "        )\n",
        "        self.skip = is_skip and inchannels == outchannels and stride == 1\n",
        "        self.dc = nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = self.pointwise1(x)\n",
        "        residual = self.depthwise(residual)\n",
        "        residual = self.se(residual)\n",
        "        residual = self.pointwise2(residual)\n",
        "        if self.skip:\n",
        "            residual = self.dc(residual)\n",
        "            out = residual + x\n",
        "        else:\n",
        "            out = residual\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class MBblock(nn.Module):\n",
        "    def __init__(self, inchannels, outchannels, expan, kernelsize, stride, se_ratio, repeat,\n",
        "                 is_skip, dc_ratio=(1-0.8), bn_momentum=0.90):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = []\n",
        "        layers.append(MBConv(inchannels, outchannels, expan, kernelsize, stride,\n",
        "                             se_ratio, is_skip, dc_ratio, bn_momentum))\n",
        "        while repeat-1:\n",
        "            layers.append(MBConv(outchannels, outchannels, expan, kernelsize, 1,\n",
        "                                 se_ratio, is_skip, dc_ratio, bn_momentum))\n",
        "            repeat = repeat - 1\n",
        "\n",
        "        self.block = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "\n",
        "class EfficientNet(nn.Module):\n",
        "    def __init__(self, width_multipler, depth_multipler, do_ratio, min_width=0, width_divisor=8,\n",
        "                 se_ratio=4, dc_ratio=(1-0.8), bn_momentum=0.90, num_class=100):\n",
        "        super().__init__()\n",
        "\n",
        "        def renew_width(x):\n",
        "            min = max(min_width, width_divisor)\n",
        "            x *= width_multipler\n",
        "            new_x = max(min, int((x + width_divisor/2) // width_divisor * width_divisor))\n",
        "\n",
        "            if new_x < 0.9 * x:\n",
        "                new_x += width_divisor\n",
        "            return int(new_x)\n",
        "\n",
        "        def renew_depth(x):\n",
        "            return int(math.ceil(x * depth_multipler))\n",
        "\n",
        "        self.stage1 = nn.Sequential(\n",
        "            SameConv(3, renew_width(32), 3),\n",
        "            nn.BatchNorm2d(renew_width(32), momentum=bn_momentum),\n",
        "            swish()\n",
        "        )\n",
        "        self.stage2 = nn.Sequential(\n",
        "                    # inchannels     outchannels  expand k  s(mobilenetv2)  repeat      is_skip\n",
        "            MBblock(renew_width(32), renew_width(16), 1, 3, 1, se_ratio, renew_depth(1), True, dc_ratio, bn_momentum),\n",
        "            MBblock(renew_width(16), renew_width(24), 6, 3, 2, se_ratio, renew_depth(2), True, dc_ratio, bn_momentum),\n",
        "            MBblock(renew_width(24), renew_width(40), 6, 5, 2, se_ratio, renew_depth(2), True, dc_ratio, bn_momentum),\n",
        "            MBblock(renew_width(40), renew_width(80), 6, 3, 2, se_ratio, renew_depth(3), True, dc_ratio, bn_momentum),\n",
        "            MBblock(renew_width(80), renew_width(112), 6, 5, 1, se_ratio, renew_depth(3), True, dc_ratio, bn_momentum),\n",
        "            MBblock(renew_width(112), renew_width(192), 6, 5, 1, se_ratio, renew_depth(4), True, dc_ratio, bn_momentum),\n",
        "            MBblock(renew_width(192), renew_width(320), 6, 3, 1, se_ratio, renew_depth(1), True, dc_ratio, bn_momentum)\n",
        "        )\n",
        "        self.stage3 = nn.Sequential(\n",
        "            SameConv(renew_width(320), renew_width(1280), 1, stride=1),\n",
        "            nn.BatchNorm2d(renew_width(1280), bn_momentum),\n",
        "            swish(),\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Dropout(do_ratio)\n",
        "        )\n",
        "        self.FC = nn.Linear(renew_width(1280), num_class)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, SameConv):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                bound = 1/int(math.sqrt(m.weight.size(1)))\n",
        "                nn.init.uniform(m.weight, -bound, bound)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.stage1(x)\n",
        "        out = self.stage2(out)\n",
        "        out = self.stage3(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.FC(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def efficientnet(width_multipler, depth_multipler, num_class=100, bn_momentum=0.90, do_ratio=0.2):\n",
        "    return EfficientNet(width_multipler, depth_multipler,\n",
        "                        num_class=num_class, bn_momentum=bn_momentum, do_ratio=do_ratio)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Rn-VFRwzZ7r"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhJloyb5zfUy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split, Subset\n",
        "\n",
        "# Define the data transformations\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# Download the CIFAR-100 dataset\n",
        "train_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "# Split the dataset into training, validation, and test sets\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = int(0.1 * len(train_dataset))\n",
        "test_size = len(train_dataset) - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(train_dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# Create balanced DataLoader for training, validation, and test sets\n",
        "batch_size = 64\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOcWPV9W0wc2",
        "outputId": "d704b841-17ef-4ada-cf8f-6dbe521a27d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'small-net-cifar100'...\n",
            "remote: Enumerating objects: 487, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 487 (delta 0), reused 0 (delta 0), pack-reused 484\u001b[K\n",
            "Receiving objects: 100% (487/487), 186.82 MiB | 15.91 MiB/s, done.\n",
            "Resolving deltas: 100% (29/29), done.\n",
            "Updating files: 100% (424/424), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/K-Hooshanfar/small-net-cifar100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLC2jpgH5WDB",
        "outputId": "7d662f33-0130-4358-bca4-5eac798894b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/small-net-cifar100\n"
          ]
        }
      ],
      "source": [
        "%cd small-net-cifar100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KGQHUF15mer",
        "outputId": "f755dc27-7347-4544-fa81-f532af207cec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch:84\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:1.206\n",
            "Epoch:84\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:1.462\n",
            "Epoch:84\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:1.826\n",
            "Epoch:84\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:1.274\n",
            "Epoch:84\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:1.175\n",
            "Epoch:84\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:1.188\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:85\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:1.342\n",
            "Epoch:85\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:1.181\n",
            "Epoch:85\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:1.217\n",
            "Epoch:85\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:1.321\n",
            "Epoch:85\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:1.164\n",
            "Epoch:85\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:1.139\n",
            "Epoch:85\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:1.321\n",
            "Epoch:85\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:1.184\n",
            "Epoch:85\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:1.247\n",
            "Epoch:85\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:1.270\n",
            "Epoch:85\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:1.159\n",
            "Epoch:85\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:1.117\n",
            "Epoch:85\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:1.240\n",
            "Epoch:85\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:1.247\n",
            "Epoch:85\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:1.225\n",
            "Epoch:85\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:1.098\n",
            "Epoch:85\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:1.171\n",
            "Epoch:85\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:1.346\n",
            "Epoch:85\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:1.343\n",
            "Epoch:85\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:1.125\n",
            "Epoch:85\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:1.174\n",
            "Epoch:85\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:1.147\n",
            "Epoch:85\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:1.189\n",
            "Epoch:85\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:1.177\n",
            "Epoch:85\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:1.107\n",
            "Epoch:85\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:1.016\n",
            "Epoch:85\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:1.295\n",
            "Epoch:85\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:1.194\n",
            "Epoch:85\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:1.492\n",
            "Epoch:85\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.992\n",
            "Epoch:85\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:1.120\n",
            "Epoch:85\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:1.301\n",
            "Epoch:85\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:1.382\n",
            "Epoch:85\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:1.184\n",
            "Epoch:85\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:1.290\n",
            "Epoch:85\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:1.454\n",
            "Epoch:85\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:1.525\n",
            "Epoch:85\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:1.219\n",
            "Epoch:85\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:1.374\n",
            "Epoch:85\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:1.148\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:86\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.998\n",
            "Epoch:86\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:1.223\n",
            "Epoch:86\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:1.120\n",
            "Epoch:86\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:1.049\n",
            "Epoch:86\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:1.247\n",
            "Epoch:86\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:1.061\n",
            "Epoch:86\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:1.305\n",
            "Epoch:86\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:1.195\n",
            "Epoch:86\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:1.081\n",
            "Epoch:86\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:1.228\n",
            "Epoch:86\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:1.188\n",
            "Epoch:86\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:1.247\n",
            "Epoch:86\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:1.143\n",
            "Epoch:86\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:1.139\n",
            "Epoch:86\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:1.257\n",
            "Epoch:86\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:1.148\n",
            "Epoch:86\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:1.374\n",
            "Epoch:86\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:1.285\n",
            "Epoch:86\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:1.252\n",
            "Epoch:86\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:1.218\n",
            "Epoch:86\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:1.172\n",
            "Epoch:86\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:1.273\n",
            "Epoch:86\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:1.252\n",
            "Epoch:86\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:1.398\n",
            "Epoch:86\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:1.179\n",
            "Epoch:86\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:1.074\n",
            "Epoch:86\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:1.291\n",
            "Epoch:86\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:1.418\n",
            "Epoch:86\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:1.242\n",
            "Epoch:86\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:1.214\n",
            "Epoch:86\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:1.076\n",
            "Epoch:86\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:1.381\n",
            "Epoch:86\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:1.337\n",
            "Epoch:86\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:1.290\n",
            "Epoch:86\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.981\n",
            "Epoch:86\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:1.037\n",
            "Epoch:86\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:1.288\n",
            "Epoch:86\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:1.301\n",
            "Epoch:86\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:1.392\n",
            "Epoch:86\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:1.027\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:87\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:1.091\n",
            "Epoch:87\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:1.056\n",
            "Epoch:87\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:1.250\n",
            "Epoch:87\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:1.295\n",
            "Epoch:87\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:1.049\n",
            "Epoch:87\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:1.187\n",
            "Epoch:87\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:1.074\n",
            "Epoch:87\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:1.061\n",
            "Epoch:87\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:1.313\n",
            "Epoch:87\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:1.102\n",
            "Epoch:87\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:1.079\n",
            "Epoch:87\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:1.195\n",
            "Epoch:87\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.970\n",
            "Epoch:87\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:1.173\n",
            "Epoch:87\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:1.235\n",
            "Epoch:87\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:1.339\n",
            "Epoch:87\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:1.271\n",
            "Epoch:87\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:1.450\n",
            "Epoch:87\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:1.207\n",
            "Epoch:87\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:1.184\n",
            "Epoch:87\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:1.264\n",
            "Epoch:87\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:1.400\n",
            "Epoch:87\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:1.231\n",
            "Epoch:87\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:1.348\n",
            "Epoch:87\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:1.119\n",
            "Epoch:87\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:1.262\n",
            "Epoch:87\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:1.057\n",
            "Epoch:87\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:1.347\n",
            "Epoch:87\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:1.269\n",
            "Epoch:87\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:1.380\n",
            "Epoch:87\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:1.327\n",
            "Epoch:87\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:1.460\n",
            "Epoch:87\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.981\n",
            "Epoch:87\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:1.008\n",
            "Epoch:87\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:1.229\n",
            "Epoch:87\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:1.122\n",
            "Epoch:87\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:1.313\n",
            "Epoch:87\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:1.427\n",
            "Epoch:87\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:1.427\n",
            "Epoch:87\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:1.180\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:88\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:1.219\n",
            "Epoch:88\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:1.159\n",
            "Epoch:88\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:1.071\n",
            "Epoch:88\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:1.277\n",
            "Epoch:88\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:1.140\n",
            "Epoch:88\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:1.283\n",
            "Epoch:88\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:1.004\n",
            "Epoch:88\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:1.067\n",
            "Epoch:88\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:1.053\n",
            "Epoch:88\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:1.191\n",
            "Epoch:88\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:1.359\n",
            "Epoch:88\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:1.190\n",
            "Epoch:88\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:1.083\n",
            "Epoch:88\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:1.132\n",
            "Epoch:88\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:1.412\n",
            "Epoch:88\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:1.220\n",
            "Epoch:88\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:1.316\n",
            "Epoch:88\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.875\n",
            "Epoch:88\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:1.193\n",
            "Epoch:88\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:1.430\n",
            "Epoch:88\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:1.440\n",
            "Epoch:88\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:1.075\n",
            "Epoch:88\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:1.329\n",
            "Epoch:88\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:1.102\n",
            "Epoch:88\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:1.326\n",
            "Epoch:88\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:1.179\n",
            "Epoch:88\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:1.120\n",
            "Epoch:88\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:1.087\n",
            "Epoch:88\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:1.451\n",
            "Epoch:88\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:1.320\n",
            "Epoch:88\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:1.094\n",
            "Epoch:88\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:1.142\n",
            "Epoch:88\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:1.404\n",
            "Epoch:88\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:1.150\n",
            "Epoch:88\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:1.052\n",
            "Epoch:88\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:1.265\n",
            "Epoch:88\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:1.270\n",
            "Epoch:88\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.996\n",
            "Epoch:88\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:1.228\n",
            "Epoch:88\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:1.369\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:89\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:1.146\n",
            "Epoch:89\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:1.024\n",
            "Epoch:89\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:1.223\n",
            "Epoch:89\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:1.270\n",
            "Epoch:89\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.926\n",
            "Epoch:89\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:1.245\n",
            "Epoch:89\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:1.238\n",
            "Epoch:89\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.978\n",
            "Epoch:89\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:1.204\n",
            "Epoch:89\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:1.243\n",
            "Epoch:89\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:1.148\n",
            "Epoch:89\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:1.219\n",
            "Epoch:89\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:1.244\n",
            "Epoch:89\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:1.265\n",
            "Epoch:89\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:1.264\n",
            "Epoch:89\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:1.348\n",
            "Epoch:89\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:1.348\n",
            "Epoch:89\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:1.145\n",
            "Epoch:89\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:1.109\n",
            "Epoch:89\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:1.225\n",
            "Epoch:89\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.982\n",
            "Epoch:89\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:1.237\n",
            "Epoch:89\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:1.295\n",
            "Epoch:89\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:1.057\n",
            "Epoch:89\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:1.336\n",
            "Epoch:89\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:1.173\n",
            "Epoch:89\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:1.003\n",
            "Epoch:89\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:1.147\n",
            "Epoch:89\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.993\n",
            "Epoch:89\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:1.172\n",
            "Epoch:89\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:1.263\n",
            "Epoch:89\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:1.083\n",
            "Epoch:89\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:1.297\n",
            "Epoch:89\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:1.003\n",
            "Epoch:89\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:1.295\n",
            "Epoch:89\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:1.304\n",
            "Epoch:89\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:1.405\n",
            "Epoch:89\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:1.390\n",
            "Epoch:89\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:1.484\n",
            "Epoch:89\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:1.256\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:90\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:1.164\n",
            "Epoch:90\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:1.070\n",
            "Epoch:90\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:1.122\n",
            "Epoch:90\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:1.112\n",
            "Epoch:90\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:1.201\n",
            "Epoch:90\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:1.238\n",
            "Epoch:90\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:1.153\n",
            "Epoch:90\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:1.186\n",
            "Epoch:90\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:1.288\n",
            "Epoch:90\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:1.092\n",
            "Epoch:90\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:1.013\n",
            "Epoch:90\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:1.095\n",
            "Epoch:90\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.991\n",
            "Epoch:90\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:1.227\n",
            "Epoch:90\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:1.097\n",
            "Epoch:90\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:1.173\n",
            "Epoch:90\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:1.233\n",
            "Epoch:90\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:1.099\n",
            "Epoch:90\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.984\n",
            "Epoch:90\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:1.328\n",
            "Epoch:90\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.983\n",
            "Epoch:90\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:1.288\n",
            "Epoch:90\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:1.344\n",
            "Epoch:90\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:1.237\n",
            "Epoch:90\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:1.009\n",
            "Epoch:90\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:1.460\n",
            "Epoch:90\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:1.346\n",
            "Epoch:90\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:1.280\n",
            "Epoch:90\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:1.065\n",
            "Epoch:90\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:1.410\n",
            "Epoch:90\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:1.007\n",
            "Epoch:90\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:1.075\n",
            "Epoch:90\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:1.203\n",
            "Epoch:90\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:1.389\n",
            "Epoch:90\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:1.110\n",
            "Epoch:90\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:1.182\n",
            "Epoch:90\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:1.229\n",
            "Epoch:90\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:1.101\n",
            "Epoch:90\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:1.120\n",
            "Epoch:90\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:1.126\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:91\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:1.073\n",
            "Epoch:91\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.997\n",
            "Epoch:91\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:1.115\n",
            "Epoch:91\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:1.087\n",
            "Epoch:91\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:1.298\n",
            "Epoch:91\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:1.162\n",
            "Epoch:91\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:1.246\n",
            "Epoch:91\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:1.241\n",
            "Epoch:91\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:1.226\n",
            "Epoch:91\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:1.083\n",
            "Epoch:91\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:1.333\n",
            "Epoch:91\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:1.041\n",
            "Epoch:91\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:1.177\n",
            "Epoch:91\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:1.271\n",
            "Epoch:91\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:1.177\n",
            "Epoch:91\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:1.107\n",
            "Epoch:91\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:1.223\n",
            "Epoch:91\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:1.193\n",
            "Epoch:91\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:1.141\n",
            "Epoch:91\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:1.076\n",
            "Epoch:91\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:1.233\n",
            "Epoch:91\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:1.166\n",
            "Epoch:91\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:1.433\n",
            "Epoch:91\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:1.285\n",
            "Epoch:91\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:1.066\n",
            "Epoch:91\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:1.100\n",
            "Epoch:91\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.977\n",
            "Epoch:91\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:1.082\n",
            "Epoch:91\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.960\n",
            "Epoch:91\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:1.065\n",
            "Epoch:91\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:1.072\n",
            "Epoch:91\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:1.193\n",
            "Epoch:91\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.943\n",
            "Epoch:91\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:1.106\n",
            "Epoch:91\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:1.167\n",
            "Epoch:91\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:1.110\n",
            "Epoch:91\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:1.074\n",
            "Epoch:91\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:1.327\n",
            "Epoch:91\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:1.075\n",
            "Epoch:91\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:1.126\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:92\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:1.260\n",
            "Epoch:92\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:1.141\n",
            "Epoch:92\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:1.189\n",
            "Epoch:92\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:1.210\n",
            "Epoch:92\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:1.155\n",
            "Epoch:92\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:1.125\n",
            "Epoch:92\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.943\n",
            "Epoch:92\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:1.137\n",
            "Epoch:92\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:1.177\n",
            "Epoch:92\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:1.204\n",
            "Epoch:92\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:1.282\n",
            "Epoch:92\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:1.178\n",
            "Epoch:92\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:1.197\n",
            "Epoch:92\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:1.223\n",
            "Epoch:92\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:1.263\n",
            "Epoch:92\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.927\n",
            "Epoch:92\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:1.277\n",
            "Epoch:92\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:1.201\n",
            "Epoch:92\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:1.062\n",
            "Epoch:92\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:1.142\n",
            "Epoch:92\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.850\n",
            "Epoch:92\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:1.284\n",
            "Epoch:92\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:1.220\n",
            "Epoch:92\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:1.438\n",
            "Epoch:92\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:1.393\n",
            "Epoch:92\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:1.127\n",
            "Epoch:92\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:1.162\n",
            "Epoch:92\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:1.147\n",
            "Epoch:92\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:1.088\n",
            "Epoch:92\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:1.184\n",
            "Epoch:92\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:1.311\n",
            "Epoch:92\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:1.124\n",
            "Epoch:92\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:1.253\n",
            "Epoch:92\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:1.323\n",
            "Epoch:92\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:1.035\n",
            "Epoch:92\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:1.219\n",
            "Epoch:92\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:1.240\n",
            "Epoch:92\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:1.125\n",
            "Epoch:92\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:1.062\n",
            "Epoch:92\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:1.518\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:93\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.962\n",
            "Epoch:93\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:1.285\n",
            "Epoch:93\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:1.250\n",
            "Epoch:93\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:1.077\n",
            "Epoch:93\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:1.117\n",
            "Epoch:93\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:1.227\n",
            "Epoch:93\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:1.100\n",
            "Epoch:93\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:1.296\n",
            "Epoch:93\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.975\n",
            "Epoch:93\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.949\n",
            "Epoch:93\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:1.006\n",
            "Epoch:93\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:1.086\n",
            "Epoch:93\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:1.231\n",
            "Epoch:93\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:1.138\n",
            "Epoch:93\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.935\n",
            "Epoch:93\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:1.160\n",
            "Epoch:93\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:1.112\n",
            "Epoch:93\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:1.172\n",
            "Epoch:93\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:1.065\n",
            "Epoch:93\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:1.137\n",
            "Epoch:93\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:1.200\n",
            "Epoch:93\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:1.124\n",
            "Epoch:93\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:1.212\n",
            "Epoch:93\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:1.141\n",
            "Epoch:93\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:1.141\n",
            "Epoch:93\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:1.188\n",
            "Epoch:93\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:1.352\n",
            "Epoch:93\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:1.264\n",
            "Epoch:93\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:1.093\n",
            "Epoch:93\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:1.387\n",
            "Epoch:93\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:1.317\n",
            "Epoch:93\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:1.273\n",
            "Epoch:93\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:1.263\n",
            "Epoch:93\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:1.246\n",
            "Epoch:93\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:1.274\n",
            "Epoch:93\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:1.134\n",
            "Epoch:93\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:1.274\n",
            "Epoch:93\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:1.208\n",
            "Epoch:93\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:1.345\n",
            "Epoch:93\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:1.314\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:94\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:1.116\n",
            "Epoch:94\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.976\n",
            "Epoch:94\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:1.125\n",
            "Epoch:94\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:1.335\n",
            "Epoch:94\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:1.081\n",
            "Epoch:94\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:1.052\n",
            "Epoch:94\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:1.112\n",
            "Epoch:94\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:1.255\n",
            "Epoch:94\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.908\n",
            "Epoch:94\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:1.115\n",
            "Epoch:94\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:1.216\n",
            "Epoch:94\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:1.264\n",
            "Epoch:94\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:1.070\n",
            "Epoch:94\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:1.133\n",
            "Epoch:94\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:1.229\n",
            "Epoch:94\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:1.178\n",
            "Epoch:94\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:1.298\n",
            "Epoch:94\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:1.077\n",
            "Epoch:94\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:1.032\n",
            "Epoch:94\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:1.182\n",
            "Epoch:94\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:1.074\n",
            "Epoch:94\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:1.160\n",
            "Epoch:94\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:1.409\n",
            "Epoch:94\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:1.057\n",
            "Epoch:94\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:1.020\n",
            "Epoch:94\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:1.115\n",
            "Epoch:94\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:1.028\n",
            "Epoch:94\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:1.311\n",
            "Epoch:94\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.958\n",
            "Epoch:94\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:1.197\n",
            "Epoch:94\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.976\n",
            "Epoch:94\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:1.166\n",
            "Epoch:94\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.967\n",
            "Epoch:94\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:1.125\n",
            "Epoch:94\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:1.319\n",
            "Epoch:94\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:1.235\n",
            "Epoch:94\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:1.357\n",
            "Epoch:94\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:1.295\n",
            "Epoch:94\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:1.105\n",
            "Epoch:94\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.963\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:95\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.988\n",
            "Epoch:95\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:1.164\n",
            "Epoch:95\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.968\n",
            "Epoch:95\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:1.099\n",
            "Epoch:95\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:1.349\n",
            "Epoch:95\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:1.106\n",
            "Epoch:95\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:1.089\n",
            "Epoch:95\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:1.096\n",
            "Epoch:95\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:1.193\n",
            "Epoch:95\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:1.230\n",
            "Epoch:95\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.993\n",
            "Epoch:95\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:1.196\n",
            "Epoch:95\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:1.090\n",
            "Epoch:95\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:1.451\n",
            "Epoch:95\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:1.051\n",
            "Epoch:95\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:1.397\n",
            "Epoch:95\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:1.050\n",
            "Epoch:95\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.964\n",
            "Epoch:95\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:1.264\n",
            "Epoch:95\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:1.152\n",
            "Epoch:95\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:1.126\n",
            "Epoch:95\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:1.294\n",
            "Epoch:95\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:1.034\n",
            "Epoch:95\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:1.052\n",
            "Epoch:95\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:1.239\n",
            "Epoch:95\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:1.309\n",
            "Epoch:95\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:1.282\n",
            "Epoch:95\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:1.142\n",
            "Epoch:95\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.995\n",
            "Epoch:95\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:1.141\n",
            "Epoch:95\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:1.166\n",
            "Epoch:95\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:1.296\n",
            "Epoch:95\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:1.419\n",
            "Epoch:95\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:1.281\n",
            "Epoch:95\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:1.380\n",
            "Epoch:95\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:1.072\n",
            "Epoch:95\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:1.448\n",
            "Epoch:95\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:1.210\n",
            "Epoch:95\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:1.337\n",
            "Epoch:95\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:1.156\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:96\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:1.286\n",
            "Epoch:96\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:1.002\n",
            "Epoch:96\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.935\n",
            "Epoch:96\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:1.030\n",
            "Epoch:96\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:1.007\n",
            "Epoch:96\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:1.003\n",
            "Epoch:96\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:1.039\n",
            "Epoch:96\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:1.013\n",
            "Epoch:96\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:1.081\n",
            "Epoch:96\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:1.216\n",
            "Epoch:96\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:1.113\n",
            "Epoch:96\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:1.137\n",
            "Epoch:96\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:1.216\n",
            "Epoch:96\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:1.329\n",
            "Epoch:96\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:1.067\n",
            "Epoch:96\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:1.192\n",
            "Epoch:96\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:1.174\n",
            "Epoch:96\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:1.291\n",
            "Epoch:96\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:1.103\n",
            "Epoch:96\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:1.047\n",
            "Epoch:96\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:1.240\n",
            "Epoch:96\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:1.044\n",
            "Epoch:96\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:1.248\n",
            "Epoch:96\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:1.149\n",
            "Epoch:96\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:1.173\n",
            "Epoch:96\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:1.400\n",
            "Epoch:96\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:1.123\n",
            "Epoch:96\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:1.072\n",
            "Epoch:96\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:1.211\n",
            "Epoch:96\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.992\n",
            "Epoch:96\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:1.251\n",
            "Epoch:96\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:1.205\n",
            "Epoch:96\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:1.103\n",
            "Epoch:96\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.967\n",
            "Epoch:96\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:1.342\n",
            "Epoch:96\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:1.299\n",
            "Epoch:96\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:1.165\n",
            "Epoch:96\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:1.132\n",
            "Epoch:96\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:1.232\n",
            "Epoch:96\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:1.126\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:97\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:1.126\n",
            "Epoch:97\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.979\n",
            "Epoch:97\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:1.145\n",
            "Epoch:97\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:1.222\n",
            "Epoch:97\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:1.208\n",
            "Epoch:97\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:1.194\n",
            "Epoch:97\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.961\n",
            "Epoch:97\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:1.190\n",
            "Epoch:97\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:1.106\n",
            "Epoch:97\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:1.120\n",
            "Epoch:97\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:1.178\n",
            "Epoch:97\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:1.447\n",
            "Epoch:97\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:1.248\n",
            "Epoch:97\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:1.222\n",
            "Epoch:97\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:1.366\n",
            "Epoch:97\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:1.127\n",
            "Epoch:97\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:1.109\n",
            "Epoch:97\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:1.004\n",
            "Epoch:97\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:1.167\n",
            "Epoch:97\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:1.130\n",
            "Epoch:97\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:1.234\n",
            "Epoch:97\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:1.064\n",
            "Epoch:97\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:1.292\n",
            "Epoch:97\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:1.136\n",
            "Epoch:97\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:1.135\n",
            "Epoch:97\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:1.009\n",
            "Epoch:97\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:1.257\n",
            "Epoch:97\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:1.122\n",
            "Epoch:97\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:1.354\n",
            "Epoch:97\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:1.319\n",
            "Epoch:97\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:1.157\n",
            "Epoch:97\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:1.083\n",
            "Epoch:97\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:1.306\n",
            "Epoch:97\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.944\n",
            "Epoch:97\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:1.111\n",
            "Epoch:97\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:1.293\n",
            "Epoch:97\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:1.218\n",
            "Epoch:97\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:1.023\n",
            "Epoch:97\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:1.253\n",
            "Epoch:97\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:1.092\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:98\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:1.174\n",
            "Epoch:98\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:1.107\n",
            "Epoch:98\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:1.357\n",
            "Epoch:98\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:1.238\n",
            "Epoch:98\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.969\n",
            "Epoch:98\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:1.096\n",
            "Epoch:98\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.819\n",
            "Epoch:98\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:1.122\n",
            "Epoch:98\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:1.209\n",
            "Epoch:98\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:1.144\n",
            "Epoch:98\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:1.177\n",
            "Epoch:98\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:1.280\n",
            "Epoch:98\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:1.112\n",
            "Epoch:98\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:1.337\n",
            "Epoch:98\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:1.125\n",
            "Epoch:98\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:1.214\n",
            "Epoch:98\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:1.177\n",
            "Epoch:98\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:1.080\n",
            "Epoch:98\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:1.298\n",
            "Epoch:98\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:1.057\n",
            "Epoch:98\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:1.367\n",
            "Epoch:98\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:1.102\n",
            "Epoch:98\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:1.114\n",
            "Epoch:98\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:1.262\n",
            "Epoch:98\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:1.251\n",
            "Epoch:98\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.918\n",
            "Epoch:98\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:1.138\n",
            "Epoch:98\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:1.291\n",
            "Epoch:98\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:1.139\n",
            "Epoch:98\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.983\n",
            "Epoch:98\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:1.132\n",
            "Epoch:98\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:1.078\n",
            "Epoch:98\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:1.421\n",
            "Epoch:98\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:1.304\n",
            "Epoch:98\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:1.025\n",
            "Epoch:98\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:1.336\n",
            "Epoch:98\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:1.266\n",
            "Epoch:98\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:1.214\n",
            "Epoch:98\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:1.214\n",
            "Epoch:98\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:1.088\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:99\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:1.209\n",
            "Epoch:99\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:1.161\n",
            "Epoch:99\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.934\n",
            "Epoch:99\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:1.097\n",
            "Epoch:99\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:1.164\n",
            "Epoch:99\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:1.066\n",
            "Epoch:99\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.983\n",
            "Epoch:99\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:1.138\n",
            "Epoch:99\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:1.054\n",
            "Epoch:99\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:1.245\n",
            "Epoch:99\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.794\n",
            "Epoch:99\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:1.187\n",
            "Epoch:99\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.974\n",
            "Epoch:99\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:1.164\n",
            "Epoch:99\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:1.106\n",
            "Epoch:99\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:1.307\n",
            "Epoch:99\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.967\n",
            "Epoch:99\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:1.164\n",
            "Epoch:99\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:1.133\n",
            "Epoch:99\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:1.153\n",
            "Epoch:99\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.992\n",
            "Epoch:99\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:1.144\n",
            "Epoch:99\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:1.411\n",
            "Epoch:99\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:1.244\n",
            "Epoch:99\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.985\n",
            "Epoch:99\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:1.229\n",
            "Epoch:99\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.978\n",
            "Epoch:99\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:1.043\n",
            "Epoch:99\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:1.124\n",
            "Epoch:99\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:1.253\n",
            "Epoch:99\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:1.138\n",
            "Epoch:99\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:1.221\n",
            "Epoch:99\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:1.258\n",
            "Epoch:99\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:1.079\n",
            "Epoch:99\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:1.053\n",
            "Epoch:99\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:1.256\n",
            "Epoch:99\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:1.202\n",
            "Epoch:99\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:1.256\n",
            "Epoch:99\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:1.202\n",
            "Epoch:99\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:1.316\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:100\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.929\n",
            "Epoch:100\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:1.299\n",
            "Epoch:100\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:1.177\n",
            "Epoch:100\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:1.208\n",
            "Epoch:100\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.895\n",
            "Epoch:100\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:1.065\n",
            "Epoch:100\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:1.186\n",
            "Epoch:100\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:1.104\n",
            "Epoch:100\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:1.204\n",
            "Epoch:100\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.862\n",
            "Epoch:100\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:1.108\n",
            "Epoch:100\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:1.127\n",
            "Epoch:100\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.898\n",
            "Epoch:100\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:1.167\n",
            "Epoch:100\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:1.137\n",
            "Epoch:100\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:1.089\n",
            "Epoch:100\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.972\n",
            "Epoch:100\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:1.354\n",
            "Epoch:100\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:1.126\n",
            "Epoch:100\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:1.349\n",
            "Epoch:100\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.995\n",
            "Epoch:100\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:1.159\n",
            "Epoch:100\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:1.124\n",
            "Epoch:100\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:1.216\n",
            "Epoch:100\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.993\n",
            "Epoch:100\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:1.110\n",
            "Epoch:100\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:1.317\n",
            "Epoch:100\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:1.237\n",
            "Epoch:100\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:1.285\n",
            "Epoch:100\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:1.312\n",
            "Epoch:100\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:1.196\n",
            "Epoch:100\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:1.330\n",
            "Epoch:100\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.857\n",
            "Epoch:100\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:1.030\n",
            "Epoch:100\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:1.120\n",
            "Epoch:100\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:1.285\n",
            "Epoch:100\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:1.298\n",
            "Epoch:100\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:1.078\n",
            "Epoch:100\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:1.222\n",
            "Epoch:100\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:1.561\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:101\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:1.137\n",
            "Epoch:101\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:1.049\n",
            "Epoch:101\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:1.022\n",
            "Epoch:101\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:1.384\n",
            "Epoch:101\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:1.160\n",
            "Epoch:101\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:1.106\n",
            "Epoch:101\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:1.079\n",
            "Epoch:101\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:1.087\n",
            "Epoch:101\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:1.004\n",
            "Epoch:101\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:1.064\n",
            "Epoch:101\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:1.173\n",
            "Epoch:101\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:1.354\n",
            "Epoch:101\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:1.112\n",
            "Epoch:101\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:1.141\n",
            "Epoch:101\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.864\n",
            "Epoch:101\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.981\n",
            "Epoch:101\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:1.093\n",
            "Epoch:101\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:1.104\n",
            "Epoch:101\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:1.200\n",
            "Epoch:101\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:1.102\n",
            "Epoch:101\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:1.063\n",
            "Epoch:101\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:1.324\n",
            "Epoch:101\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:1.004\n",
            "Epoch:101\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:1.059\n",
            "Epoch:101\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:1.305\n",
            "Epoch:101\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:1.011\n",
            "Epoch:101\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:1.005\n",
            "Epoch:101\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:1.053\n",
            "Epoch:101\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:1.334\n",
            "Epoch:101\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:1.059\n",
            "Epoch:101\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:1.220\n",
            "Epoch:101\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:1.087\n",
            "Epoch:101\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:1.123\n",
            "Epoch:101\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:1.233\n",
            "Epoch:101\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:1.119\n",
            "Epoch:101\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:1.159\n",
            "Epoch:101\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:1.072\n",
            "Epoch:101\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:1.063\n",
            "Epoch:101\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:1.156\n",
            "Epoch:101\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:1.368\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:102\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.982\n",
            "Epoch:102\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.898\n",
            "Epoch:102\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:1.033\n",
            "Epoch:102\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:1.072\n",
            "Epoch:102\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:1.232\n",
            "Epoch:102\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.938\n",
            "Epoch:102\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:1.111\n",
            "Epoch:102\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:1.033\n",
            "Epoch:102\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:1.010\n",
            "Epoch:102\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:1.264\n",
            "Epoch:102\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:1.072\n",
            "Epoch:102\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:1.196\n",
            "Epoch:102\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:1.081\n",
            "Epoch:102\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:1.035\n",
            "Epoch:102\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:1.283\n",
            "Epoch:102\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:1.152\n",
            "Epoch:102\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:1.177\n",
            "Epoch:102\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.999\n",
            "Epoch:102\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:1.211\n",
            "Epoch:102\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:1.158\n",
            "Epoch:102\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:1.385\n",
            "Epoch:102\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:1.418\n",
            "Epoch:102\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:1.166\n",
            "Epoch:102\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:1.333\n",
            "Epoch:102\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:1.033\n",
            "Epoch:102\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:1.281\n",
            "Epoch:102\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:1.313\n",
            "Epoch:102\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:1.058\n",
            "Epoch:102\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:1.168\n",
            "Epoch:102\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:1.133\n",
            "Epoch:102\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:1.293\n",
            "Epoch:102\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:1.209\n",
            "Epoch:102\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:1.370\n",
            "Epoch:102\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:1.436\n",
            "Epoch:102\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:1.186\n",
            "Epoch:102\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:1.042\n",
            "Epoch:102\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:1.249\n",
            "Epoch:102\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:1.119\n",
            "Epoch:102\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:1.141\n",
            "Epoch:102\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:1.084\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:103\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:1.068\n",
            "Epoch:103\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.794\n",
            "Epoch:103\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:1.062\n",
            "Epoch:103\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.998\n",
            "Epoch:103\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:1.285\n",
            "Epoch:103\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:1.081\n",
            "Epoch:103\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:1.059\n",
            "Epoch:103\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:1.130\n",
            "Epoch:103\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:1.217\n",
            "Epoch:103\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:1.075\n",
            "Epoch:103\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:1.338\n",
            "Epoch:103\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:1.041\n",
            "Epoch:103\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:1.209\n",
            "Epoch:103\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:1.022\n",
            "Epoch:103\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:1.337\n",
            "Epoch:103\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:1.114\n",
            "Epoch:103\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:1.441\n",
            "Epoch:103\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:1.222\n",
            "Epoch:103\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:1.329\n",
            "Epoch:103\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:1.032\n",
            "Epoch:103\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:1.291\n",
            "Epoch:103\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:1.260\n",
            "Epoch:103\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:1.201\n",
            "Epoch:103\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:1.121\n",
            "Epoch:103\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:1.030\n",
            "Epoch:103\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:1.274\n",
            "Epoch:103\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:1.332\n",
            "Epoch:103\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:1.051\n",
            "Epoch:103\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:1.228\n",
            "Epoch:103\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:1.100\n",
            "Epoch:103\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:1.151\n",
            "Epoch:103\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:1.458\n",
            "Epoch:103\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:1.117\n",
            "Epoch:103\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:1.106\n",
            "Epoch:103\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:1.111\n",
            "Epoch:103\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:1.126\n",
            "Epoch:103\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:1.071\n",
            "Epoch:103\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:1.185\n",
            "Epoch:103\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:1.088\n",
            "Epoch:103\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:1.453\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:104\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:1.028\n",
            "Epoch:104\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:1.033\n",
            "Epoch:104\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.997\n",
            "Epoch:104\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:1.164\n",
            "Epoch:104\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:1.089\n",
            "Epoch:104\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:1.169\n",
            "Epoch:104\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:1.003\n",
            "Epoch:104\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:1.092\n",
            "Epoch:104\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:1.093\n",
            "Epoch:104\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:1.172\n",
            "Epoch:104\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:1.114\n",
            "Epoch:104\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:1.157\n",
            "Epoch:104\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:1.120\n",
            "Epoch:104\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:1.268\n",
            "Epoch:104\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.993\n",
            "Epoch:104\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:1.104\n",
            "Epoch:104\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.943\n",
            "Epoch:104\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:1.121\n",
            "Epoch:104\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:1.160\n",
            "Epoch:104\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.988\n",
            "Epoch:104\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:1.190\n",
            "Epoch:104\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:1.000\n",
            "Epoch:104\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:1.440\n",
            "Epoch:104\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:1.177\n",
            "Epoch:104\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:1.159\n",
            "Epoch:104\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:1.079\n",
            "Epoch:104\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:1.203\n",
            "Epoch:104\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:1.270\n",
            "Epoch:104\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:1.158\n",
            "Epoch:104\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:1.144\n",
            "Epoch:104\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:1.141\n",
            "Epoch:104\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:1.052\n",
            "Epoch:104\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.965\n",
            "Epoch:104\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.900\n",
            "Epoch:104\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:1.246\n",
            "Epoch:104\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:1.283\n",
            "Epoch:104\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:1.156\n",
            "Epoch:104\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:1.355\n",
            "Epoch:104\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:1.128\n",
            "Epoch:104\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:1.094\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:105\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.923\n",
            "Epoch:105\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:1.201\n",
            "Epoch:105\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:1.079\n",
            "Epoch:105\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:1.349\n",
            "Epoch:105\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.901\n",
            "Epoch:105\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.983\n",
            "Epoch:105\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.992\n",
            "Epoch:105\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:1.148\n",
            "Epoch:105\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:1.303\n",
            "Epoch:105\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:1.168\n",
            "Epoch:105\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:1.418\n",
            "Epoch:105\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:1.057\n",
            "Epoch:105\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:1.175\n",
            "Epoch:105\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:1.169\n",
            "Epoch:105\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:1.128\n",
            "Epoch:105\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:1.011\n",
            "Epoch:105\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.993\n",
            "Epoch:105\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:1.129\n",
            "Epoch:105\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:1.173\n",
            "Epoch:105\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:1.287\n",
            "Epoch:105\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:1.003\n",
            "Epoch:105\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:1.214\n",
            "Epoch:105\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:1.164\n",
            "Epoch:105\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:1.250\n",
            "Epoch:105\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:1.086\n",
            "Epoch:105\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.914\n",
            "Epoch:105\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:1.313\n",
            "Epoch:105\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:1.095\n",
            "Epoch:105\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:1.396\n",
            "Epoch:105\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:1.030\n",
            "Epoch:105\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:1.011\n",
            "Epoch:105\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:1.406\n",
            "Epoch:105\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:1.212\n",
            "Epoch:105\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:1.191\n",
            "Epoch:105\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.960\n",
            "Epoch:105\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.980\n",
            "Epoch:105\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:1.303\n",
            "Epoch:105\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:1.212\n",
            "Epoch:105\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:1.271\n",
            "Epoch:105\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:1.259\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:106\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:1.062\n",
            "Epoch:106\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:1.102\n",
            "Epoch:106\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.929\n",
            "Epoch:106\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:1.017\n",
            "Epoch:106\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:1.141\n",
            "Epoch:106\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:1.273\n",
            "Epoch:106\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:1.032\n",
            "Epoch:106\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:1.028\n",
            "Epoch:106\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:1.223\n",
            "Epoch:106\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.994\n",
            "Epoch:106\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:1.317\n",
            "Epoch:106\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:1.175\n",
            "Epoch:106\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:1.010\n",
            "Epoch:106\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:1.064\n",
            "Epoch:106\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:1.069\n",
            "Epoch:106\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.982\n",
            "Epoch:106\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:1.266\n",
            "Epoch:106\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:1.190\n",
            "Epoch:106\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:1.105\n",
            "Epoch:106\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:1.127\n",
            "Epoch:106\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:1.116\n",
            "Epoch:106\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.981\n",
            "Epoch:106\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:1.073\n",
            "Epoch:106\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:1.177\n",
            "Epoch:106\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:1.290\n",
            "Epoch:106\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:1.306\n",
            "Epoch:106\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:1.025\n",
            "Epoch:106\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:1.404\n",
            "Epoch:106\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:1.195\n",
            "Epoch:106\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:1.263\n",
            "Epoch:106\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:1.129\n",
            "Epoch:106\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:1.078\n",
            "Epoch:106\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:1.416\n",
            "Epoch:106\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.909\n",
            "Epoch:106\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:1.086\n",
            "Epoch:106\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:1.096\n",
            "Epoch:106\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:1.163\n",
            "Epoch:106\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:1.095\n",
            "Epoch:106\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:1.004\n",
            "Epoch:106\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.971\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:107\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:1.130\n",
            "Epoch:107\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.944\n",
            "Epoch:107\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:1.199\n",
            "Epoch:107\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:1.359\n",
            "Epoch:107\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:1.116\n",
            "Epoch:107\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:1.295\n",
            "Epoch:107\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.920\n",
            "Epoch:107\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:1.047\n",
            "Epoch:107\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:1.139\n",
            "Epoch:107\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:1.074\n",
            "Epoch:107\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:1.108\n",
            "Epoch:107\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:1.039\n",
            "Epoch:107\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.954\n",
            "Epoch:107\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:1.139\n",
            "Epoch:107\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:1.227\n",
            "Epoch:107\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:1.220\n",
            "Epoch:107\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:1.098\n",
            "Epoch:107\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:1.008\n",
            "Epoch:107\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:1.065\n",
            "Epoch:107\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:1.485\n",
            "Epoch:107\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:1.063\n",
            "Epoch:107\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:1.197\n",
            "Epoch:107\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.869\n",
            "Epoch:107\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:1.026\n",
            "Epoch:107\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:1.291\n",
            "Epoch:107\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:1.184\n",
            "Epoch:107\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:1.011\n",
            "Epoch:107\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:1.174\n",
            "Epoch:107\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:1.141\n",
            "Epoch:107\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:1.315\n",
            "Epoch:107\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:1.111\n",
            "Epoch:107\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:1.273\n",
            "Epoch:107\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:1.167\n",
            "Epoch:107\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:1.185\n",
            "Epoch:107\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:1.170\n",
            "Epoch:107\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:1.134\n",
            "Epoch:107\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:1.330\n",
            "Epoch:107\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:1.101\n",
            "Epoch:107\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:1.296\n",
            "Epoch:107\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:1.390\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:108\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:1.147\n",
            "Epoch:108\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.985\n",
            "Epoch:108\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:1.211\n",
            "Epoch:108\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:1.057\n",
            "Epoch:108\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:1.103\n",
            "Epoch:108\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:1.166\n",
            "Epoch:108\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.971\n",
            "Epoch:108\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:1.116\n",
            "Epoch:108\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:1.166\n",
            "Epoch:108\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:1.029\n",
            "Epoch:108\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:1.095\n",
            "Epoch:108\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:1.068\n",
            "Epoch:108\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:1.096\n",
            "Epoch:108\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:1.011\n",
            "Epoch:108\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.891\n",
            "Epoch:108\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.933\n",
            "Epoch:108\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.988\n",
            "Epoch:108\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:1.033\n",
            "Epoch:108\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:1.309\n",
            "Epoch:108\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:1.078\n",
            "Epoch:108\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:1.210\n",
            "Epoch:108\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:1.263\n",
            "Epoch:108\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:1.321\n",
            "Epoch:108\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:1.047\n",
            "Epoch:108\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:1.264\n",
            "Epoch:108\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:1.413\n",
            "Epoch:108\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:1.137\n",
            "Epoch:108\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:1.002\n",
            "Epoch:108\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:1.138\n",
            "Epoch:108\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:1.265\n",
            "Epoch:108\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:1.312\n",
            "Epoch:108\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.972\n",
            "Epoch:108\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:1.347\n",
            "Epoch:108\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:1.104\n",
            "Epoch:108\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.997\n",
            "Epoch:108\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:1.194\n",
            "Epoch:108\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:1.239\n",
            "Epoch:108\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:1.200\n",
            "Epoch:108\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:1.127\n",
            "Epoch:108\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:1.197\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:109\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:1.317\n",
            "Epoch:109\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:1.181\n",
            "Epoch:109\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:1.138\n",
            "Epoch:109\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.902\n",
            "Epoch:109\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.933\n",
            "Epoch:109\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:1.098\n",
            "Epoch:109\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:1.208\n",
            "Epoch:109\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:1.086\n",
            "Epoch:109\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.840\n",
            "Epoch:109\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:1.022\n",
            "Epoch:109\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:1.157\n",
            "Epoch:109\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:1.320\n",
            "Epoch:109\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:1.113\n",
            "Epoch:109\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.938\n",
            "Epoch:109\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:1.076\n",
            "Epoch:109\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:1.120\n",
            "Epoch:109\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:1.081\n",
            "Epoch:109\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:1.114\n",
            "Epoch:109\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:1.088\n",
            "Epoch:109\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:1.114\n",
            "Epoch:109\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:1.302\n",
            "Epoch:109\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:1.070\n",
            "Epoch:109\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:1.091\n",
            "Epoch:109\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:1.133\n",
            "Epoch:109\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:1.076\n",
            "Epoch:109\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:1.240\n",
            "Epoch:109\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:1.157\n",
            "Epoch:109\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.963\n",
            "Epoch:109\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:1.263\n",
            "Epoch:109\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:1.166\n",
            "Epoch:109\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:1.284\n",
            "Epoch:109\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:1.363\n",
            "Epoch:109\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:1.056\n",
            "Epoch:109\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:1.239\n",
            "Epoch:109\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:1.249\n",
            "Epoch:109\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:1.277\n",
            "Epoch:109\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:1.452\n",
            "Epoch:109\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:1.291\n",
            "Epoch:109\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:1.009\n",
            "Epoch:109\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:1.087\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:110\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:1.266\n",
            "Epoch:110\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.879\n",
            "Epoch:110\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:1.071\n",
            "Epoch:110\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:1.115\n",
            "Epoch:110\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:1.046\n",
            "Epoch:110\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.860\n",
            "Epoch:110\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.956\n",
            "Epoch:110\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:1.013\n",
            "Epoch:110\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:1.019\n",
            "Epoch:110\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:1.061\n",
            "Epoch:110\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:1.147\n",
            "Epoch:110\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:1.239\n",
            "Epoch:110\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.846\n",
            "Epoch:110\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.989\n",
            "Epoch:110\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:1.030\n",
            "Epoch:110\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:1.282\n",
            "Epoch:110\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:1.189\n",
            "Epoch:110\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:1.198\n",
            "Epoch:110\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.997\n",
            "Epoch:110\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:1.315\n",
            "Epoch:110\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:1.319\n",
            "Epoch:110\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:1.042\n",
            "Epoch:110\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:1.288\n",
            "Epoch:110\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.890\n",
            "Epoch:110\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.942\n",
            "Epoch:110\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:1.152\n",
            "Epoch:110\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.998\n",
            "Epoch:110\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.856\n",
            "Epoch:110\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:1.014\n",
            "Epoch:110\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:1.044\n",
            "Epoch:110\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:1.278\n",
            "Epoch:110\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:1.192\n",
            "Epoch:110\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:1.022\n",
            "Epoch:110\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:1.223\n",
            "Epoch:110\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:1.040\n",
            "Epoch:110\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:1.050\n",
            "Epoch:110\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.990\n",
            "Epoch:110\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:1.242\n",
            "Epoch:110\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:1.190\n",
            "Epoch:110\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:1.065\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:111\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.930\n",
            "Epoch:111\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:1.159\n",
            "Epoch:111\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:1.030\n",
            "Epoch:111\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.999\n",
            "Epoch:111\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:1.149\n",
            "Epoch:111\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:1.149\n",
            "Epoch:111\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:1.029\n",
            "Epoch:111\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:1.159\n",
            "Epoch:111\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:1.282\n",
            "Epoch:111\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:1.134\n",
            "Epoch:111\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:1.013\n",
            "Epoch:111\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:1.230\n",
            "Epoch:111\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:1.182\n",
            "Epoch:111\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:1.134\n",
            "Epoch:111\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:1.163\n",
            "Epoch:111\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:1.006\n",
            "Epoch:111\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:1.019\n",
            "Epoch:111\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:1.012\n",
            "Epoch:111\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:1.004\n",
            "Epoch:111\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:1.267\n",
            "Epoch:111\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:1.074\n",
            "Epoch:111\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:1.186\n",
            "Epoch:111\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:1.164\n",
            "Epoch:111\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:1.042\n",
            "Epoch:111\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:1.239\n",
            "Epoch:111\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:1.191\n",
            "Epoch:111\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:1.227\n",
            "Epoch:111\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:1.270\n",
            "Epoch:111\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.984\n",
            "Epoch:111\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:1.198\n",
            "Epoch:111\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:1.150\n",
            "Epoch:111\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.864\n",
            "Epoch:111\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:1.314\n",
            "Epoch:111\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.978\n",
            "Epoch:111\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:1.066\n",
            "Epoch:111\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:1.177\n",
            "Epoch:111\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:1.123\n",
            "Epoch:111\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:1.108\n",
            "Epoch:111\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:1.329\n",
            "Epoch:111\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:1.223\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:112\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:1.084\n",
            "Epoch:112\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:1.042\n",
            "Epoch:112\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:1.086\n",
            "Epoch:112\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:1.108\n",
            "Epoch:112\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:1.151\n",
            "Epoch:112\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:1.114\n",
            "Epoch:112\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.988\n",
            "Epoch:112\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:1.002\n",
            "Epoch:112\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:1.136\n",
            "Epoch:112\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:1.190\n",
            "Epoch:112\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:1.207\n",
            "Epoch:112\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:1.027\n",
            "Epoch:112\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:1.164\n",
            "Epoch:112\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:1.065\n",
            "Epoch:112\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:1.109\n",
            "Epoch:112\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:1.138\n",
            "Epoch:112\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:1.276\n",
            "Epoch:112\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:1.207\n",
            "Epoch:112\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:1.007\n",
            "Epoch:112\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:1.269\n",
            "Epoch:112\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:1.152\n",
            "Epoch:112\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:1.205\n",
            "Epoch:112\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:1.059\n",
            "Epoch:112\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:1.160\n",
            "Epoch:112\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:1.137\n",
            "Epoch:112\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:1.116\n",
            "Epoch:112\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:1.153\n",
            "Epoch:112\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:1.136\n",
            "Epoch:112\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:1.025\n",
            "Epoch:112\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.974\n",
            "Epoch:112\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.973\n",
            "Epoch:112\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:1.176\n",
            "Epoch:112\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:1.041\n",
            "Epoch:112\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:1.042\n",
            "Epoch:112\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:1.207\n",
            "Epoch:112\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.950\n",
            "Epoch:112\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:1.189\n",
            "Epoch:112\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:1.177\n",
            "Epoch:112\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:1.164\n",
            "Epoch:112\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:1.436\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:113\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:1.087\n",
            "Epoch:113\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.969\n",
            "Epoch:113\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:1.043\n",
            "Epoch:113\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:1.007\n",
            "Epoch:113\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:1.040\n",
            "Epoch:113\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:1.090\n",
            "Epoch:113\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:1.054\n",
            "Epoch:113\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.974\n",
            "Epoch:113\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:1.014\n",
            "Epoch:113\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:1.159\n",
            "Epoch:113\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:1.113\n",
            "Epoch:113\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:1.048\n",
            "Epoch:113\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:1.017\n",
            "Epoch:113\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.705\n",
            "Epoch:113\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:1.210\n",
            "Epoch:113\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:1.101\n",
            "Epoch:113\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:1.084\n",
            "Epoch:113\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.950\n",
            "Epoch:113\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.978\n",
            "Epoch:113\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:1.033\n",
            "Epoch:113\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.891\n",
            "Epoch:113\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:1.318\n",
            "Epoch:113\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:1.170\n",
            "Epoch:113\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.910\n",
            "Epoch:113\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:1.062\n",
            "Epoch:113\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:1.000\n",
            "Epoch:113\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:1.066\n",
            "Epoch:113\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.975\n",
            "Epoch:113\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:1.256\n",
            "Epoch:113\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:1.260\n",
            "Epoch:113\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:1.248\n",
            "Epoch:113\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:1.248\n",
            "Epoch:113\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:1.335\n",
            "Epoch:113\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:1.092\n",
            "Epoch:113\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:1.003\n",
            "Epoch:113\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:1.341\n",
            "Epoch:113\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:1.058\n",
            "Epoch:113\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:1.368\n",
            "Epoch:113\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.936\n",
            "Epoch:113\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:1.420\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:114\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.887\n",
            "Epoch:114\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.995\n",
            "Epoch:114\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:1.122\n",
            "Epoch:114\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:1.018\n",
            "Epoch:114\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:1.088\n",
            "Epoch:114\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:1.220\n",
            "Epoch:114\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:1.080\n",
            "Epoch:114\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:1.141\n",
            "Epoch:114\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:1.236\n",
            "Epoch:114\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:1.103\n",
            "Epoch:114\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:1.118\n",
            "Epoch:114\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:1.201\n",
            "Epoch:114\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:1.227\n",
            "Epoch:114\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.971\n",
            "Epoch:114\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:1.176\n",
            "Epoch:114\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:1.206\n",
            "Epoch:114\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:1.020\n",
            "Epoch:114\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:1.102\n",
            "Epoch:114\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:1.109\n",
            "Epoch:114\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:1.026\n",
            "Epoch:114\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:1.061\n",
            "Epoch:114\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:1.021\n",
            "Epoch:114\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:1.289\n",
            "Epoch:114\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:1.319\n",
            "Epoch:114\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:1.062\n",
            "Epoch:114\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:1.293\n",
            "Epoch:114\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:1.025\n",
            "Epoch:114\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.967\n",
            "Epoch:114\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:1.007\n",
            "Epoch:114\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:1.048\n",
            "Epoch:114\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:1.091\n",
            "Epoch:114\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:1.353\n",
            "Epoch:114\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:1.067\n",
            "Epoch:114\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:1.084\n",
            "Epoch:114\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:1.067\n",
            "Epoch:114\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.901\n",
            "Epoch:114\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:1.193\n",
            "Epoch:114\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:1.060\n",
            "Epoch:114\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:1.308\n",
            "Epoch:114\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:1.126\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:115\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:1.443\n",
            "Epoch:115\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:1.062\n",
            "Epoch:115\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.820\n",
            "Epoch:115\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.871\n",
            "Epoch:115\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:1.249\n",
            "Epoch:115\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:1.305\n",
            "Epoch:115\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:1.267\n",
            "Epoch:115\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:1.180\n",
            "Epoch:115\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:1.086\n",
            "Epoch:115\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:1.230\n",
            "Epoch:115\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:1.229\n",
            "Epoch:115\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.991\n",
            "Epoch:115\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:1.063\n",
            "Epoch:115\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:1.071\n",
            "Epoch:115\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:1.055\n",
            "Epoch:115\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:1.092\n",
            "Epoch:115\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:1.057\n",
            "Epoch:115\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.908\n",
            "Epoch:115\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:1.156\n",
            "Epoch:115\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:1.298\n",
            "Epoch:115\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:1.305\n",
            "Epoch:115\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:1.077\n",
            "Epoch:115\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.995\n",
            "Epoch:115\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:1.126\n",
            "Epoch:115\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:1.075\n",
            "Epoch:115\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.960\n",
            "Epoch:115\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:1.100\n",
            "Epoch:115\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:1.018\n",
            "Epoch:115\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:1.191\n",
            "Epoch:115\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:1.298\n",
            "Epoch:115\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:1.178\n",
            "Epoch:115\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:1.226\n",
            "Epoch:115\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:1.076\n",
            "Epoch:115\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:1.229\n",
            "Epoch:115\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:1.014\n",
            "Epoch:115\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:1.022\n",
            "Epoch:115\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:1.224\n",
            "Epoch:115\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:1.180\n",
            "Epoch:115\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:1.096\n",
            "Epoch:115\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.930\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:116\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.873\n",
            "Epoch:116\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.946\n",
            "Epoch:116\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:1.192\n",
            "Epoch:116\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.964\n",
            "Epoch:116\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:1.215\n",
            "Epoch:116\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.924\n",
            "Epoch:116\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:1.149\n",
            "Epoch:116\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:1.055\n",
            "Epoch:116\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.886\n",
            "Epoch:116\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:1.148\n",
            "Epoch:116\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:1.210\n",
            "Epoch:116\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:1.240\n",
            "Epoch:116\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:1.070\n",
            "Epoch:116\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:1.036\n",
            "Epoch:116\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:1.047\n",
            "Epoch:116\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:1.068\n",
            "Epoch:116\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.971\n",
            "Epoch:116\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:1.217\n",
            "Epoch:116\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:1.032\n",
            "Epoch:116\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.827\n",
            "Epoch:116\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:1.270\n",
            "Epoch:116\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:1.302\n",
            "Epoch:116\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:1.101\n",
            "Epoch:116\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:1.136\n",
            "Epoch:116\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.952\n",
            "Epoch:116\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:1.163\n",
            "Epoch:116\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.985\n",
            "Epoch:116\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.883\n",
            "Epoch:116\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:1.331\n",
            "Epoch:116\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:1.090\n",
            "Epoch:116\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:1.082\n",
            "Epoch:116\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:1.136\n",
            "Epoch:116\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:1.029\n",
            "Epoch:116\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:1.153\n",
            "Epoch:116\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:1.240\n",
            "Epoch:116\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:1.078\n",
            "Epoch:116\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:1.121\n",
            "Epoch:116\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:1.185\n",
            "Epoch:116\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.991\n",
            "Epoch:116\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:1.172\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:117\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:1.093\n",
            "Epoch:117\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:1.104\n",
            "Epoch:117\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.934\n",
            "Epoch:117\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.912\n",
            "Epoch:117\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:1.218\n",
            "Epoch:117\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:1.108\n",
            "Epoch:117\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.961\n",
            "Epoch:117\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.907\n",
            "Epoch:117\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:1.214\n",
            "Epoch:117\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:1.169\n",
            "Epoch:117\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:1.104\n",
            "Epoch:117\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:1.013\n",
            "Epoch:117\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:1.326\n",
            "Epoch:117\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:1.356\n",
            "Epoch:117\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:1.005\n",
            "Epoch:117\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:1.105\n",
            "Epoch:117\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:1.137\n",
            "Epoch:117\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:1.039\n",
            "Epoch:117\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:1.288\n",
            "Epoch:117\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:1.031\n",
            "Epoch:117\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:1.049\n",
            "Epoch:117\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:1.168\n",
            "Epoch:117\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:1.350\n",
            "Epoch:117\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:1.164\n",
            "Epoch:117\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.958\n",
            "Epoch:117\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:1.056\n",
            "Epoch:117\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:1.038\n",
            "Epoch:117\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.997\n",
            "Epoch:117\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:1.086\n",
            "Epoch:117\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:1.097\n",
            "Epoch:117\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:1.234\n",
            "Epoch:117\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:1.041\n",
            "Epoch:117\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.931\n",
            "Epoch:117\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:1.196\n",
            "Epoch:117\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.874\n",
            "Epoch:117\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:1.166\n",
            "Epoch:117\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.876\n",
            "Epoch:117\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:1.456\n",
            "Epoch:117\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:1.064\n",
            "Epoch:117\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:1.194\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:118\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:1.016\n",
            "Epoch:118\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.804\n",
            "Epoch:118\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:1.128\n",
            "Epoch:118\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.958\n",
            "Epoch:118\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:1.210\n",
            "Epoch:118\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:1.055\n",
            "Epoch:118\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:1.030\n",
            "Epoch:118\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.993\n",
            "Epoch:118\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.892\n",
            "Epoch:118\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:1.204\n",
            "Epoch:118\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.892\n",
            "Epoch:118\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.998\n",
            "Epoch:118\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:1.102\n",
            "Epoch:118\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:1.280\n",
            "Epoch:118\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:1.190\n",
            "Epoch:118\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:1.005\n",
            "Epoch:118\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:1.290\n",
            "Epoch:118\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:1.045\n",
            "Epoch:118\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:1.207\n",
            "Epoch:118\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:1.162\n",
            "Epoch:118\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:1.118\n",
            "Epoch:118\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:1.177\n",
            "Epoch:118\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:1.218\n",
            "Epoch:118\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:1.098\n",
            "Epoch:118\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:1.059\n",
            "Epoch:118\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:1.171\n",
            "Epoch:118\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:1.112\n",
            "Epoch:118\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:1.181\n",
            "Epoch:118\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:1.197\n",
            "Epoch:118\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:1.061\n",
            "Epoch:118\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:1.227\n",
            "Epoch:118\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.860\n",
            "Epoch:118\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:1.307\n",
            "Epoch:118\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.975\n",
            "Epoch:118\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:1.182\n",
            "Epoch:118\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:1.334\n",
            "Epoch:118\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:1.108\n",
            "Epoch:118\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:1.077\n",
            "Epoch:118\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:1.105\n",
            "Epoch:118\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:1.226\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:119\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:1.012\n",
            "Epoch:119\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:1.023\n",
            "Epoch:119\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.965\n",
            "Epoch:119\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.973\n",
            "Epoch:119\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:1.131\n",
            "Epoch:119\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:1.001\n",
            "Epoch:119\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:1.192\n",
            "Epoch:119\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:1.046\n",
            "Epoch:119\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:1.134\n",
            "Epoch:119\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:1.259\n",
            "Epoch:119\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.968\n",
            "Epoch:119\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:1.106\n",
            "Epoch:119\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.908\n",
            "Epoch:119\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.969\n",
            "Epoch:119\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:1.040\n",
            "Epoch:119\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.912\n",
            "Epoch:119\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:1.035\n",
            "Epoch:119\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:1.033\n",
            "Epoch:119\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:1.063\n",
            "Epoch:119\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:1.030\n",
            "Epoch:119\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:1.321\n",
            "Epoch:119\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:1.303\n",
            "Epoch:119\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:1.062\n",
            "Epoch:119\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:1.100\n",
            "Epoch:119\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:1.021\n",
            "Epoch:119\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.947\n",
            "Epoch:119\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.841\n",
            "Epoch:119\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:1.017\n",
            "Epoch:119\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:1.066\n",
            "Epoch:119\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:1.088\n",
            "Epoch:119\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:1.097\n",
            "Epoch:119\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:1.091\n",
            "Epoch:119\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:1.267\n",
            "Epoch:119\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:1.192\n",
            "Epoch:119\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:1.058\n",
            "Epoch:119\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:1.198\n",
            "Epoch:119\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:1.267\n",
            "Epoch:119\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.880\n",
            "Epoch:119\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:1.115\n",
            "Epoch:119\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:1.127\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:120\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:1.034\n",
            "Epoch:120\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:1.063\n",
            "Epoch:120\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.965\n",
            "Epoch:120\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:1.019\n",
            "Epoch:120\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:1.487\n",
            "Epoch:120\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:1.180\n",
            "Epoch:120\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.994\n",
            "Epoch:120\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.854\n",
            "Epoch:120\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.953\n",
            "Epoch:120\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:1.238\n",
            "Epoch:120\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:1.028\n",
            "Epoch:120\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:1.262\n",
            "Epoch:120\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.991\n",
            "Epoch:120\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:1.039\n",
            "Epoch:120\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:1.105\n",
            "Epoch:120\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:1.190\n",
            "Epoch:120\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:1.171\n",
            "Epoch:120\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:1.151\n",
            "Epoch:120\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.833\n",
            "Epoch:120\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:1.184\n",
            "Epoch:120\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:1.115\n",
            "Epoch:120\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:1.126\n",
            "Epoch:120\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.946\n",
            "Epoch:120\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.992\n",
            "Epoch:120\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.918\n",
            "Epoch:120\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:1.074\n",
            "Epoch:120\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:1.053\n",
            "Epoch:120\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:1.238\n",
            "Epoch:120\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:1.261\n",
            "Epoch:120\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:1.171\n",
            "Epoch:120\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:1.029\n",
            "Epoch:120\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:1.069\n",
            "Epoch:120\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.970\n",
            "Epoch:120\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:1.191\n",
            "Epoch:120\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:1.067\n",
            "Epoch:120\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:1.155\n",
            "Epoch:120\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:1.066\n",
            "Epoch:120\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:1.035\n",
            "Epoch:120\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:1.069\n",
            "Epoch:120\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:1.167\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:121\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.842\n",
            "Epoch:121\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.885\n",
            "Epoch:121\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:1.064\n",
            "Epoch:121\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.866\n",
            "Epoch:121\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.969\n",
            "Epoch:121\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:1.000\n",
            "Epoch:121\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.908\n",
            "Epoch:121\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.886\n",
            "Epoch:121\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.715\n",
            "Epoch:121\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.792\n",
            "Epoch:121\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.830\n",
            "Epoch:121\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.777\n",
            "Epoch:121\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.731\n",
            "Epoch:121\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.852\n",
            "Epoch:121\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.797\n",
            "Epoch:121\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.832\n",
            "Epoch:121\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.806\n",
            "Epoch:121\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.764\n",
            "Epoch:121\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.947\n",
            "Epoch:121\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.926\n",
            "Epoch:121\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.752\n",
            "Epoch:121\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.948\n",
            "Epoch:121\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.830\n",
            "Epoch:121\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:1.050\n",
            "Epoch:121\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.739\n",
            "Epoch:121\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.725\n",
            "Epoch:121\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.692\n",
            "Epoch:121\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.681\n",
            "Epoch:121\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.792\n",
            "Epoch:121\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.915\n",
            "Epoch:121\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.683\n",
            "Epoch:121\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:1.150\n",
            "Epoch:121\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.790\n",
            "Epoch:121\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.635\n",
            "Epoch:121\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.752\n",
            "Epoch:121\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.661\n",
            "Epoch:121\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.810\n",
            "Epoch:121\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.898\n",
            "Epoch:121\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.716\n",
            "Epoch:121\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.951\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:122\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.734\n",
            "Epoch:122\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.790\n",
            "Epoch:122\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.587\n",
            "Epoch:122\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.703\n",
            "Epoch:122\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.746\n",
            "Epoch:122\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.861\n",
            "Epoch:122\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.760\n",
            "Epoch:122\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.779\n",
            "Epoch:122\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.718\n",
            "Epoch:122\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.626\n",
            "Epoch:122\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.778\n",
            "Epoch:122\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.603\n",
            "Epoch:122\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.664\n",
            "Epoch:122\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.906\n",
            "Epoch:122\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.743\n",
            "Epoch:122\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.892\n",
            "Epoch:122\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.772\n",
            "Epoch:122\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.685\n",
            "Epoch:122\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.628\n",
            "Epoch:122\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.708\n",
            "Epoch:122\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.796\n",
            "Epoch:122\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.976\n",
            "Epoch:122\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.508\n",
            "Epoch:122\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.744\n",
            "Epoch:122\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.642\n",
            "Epoch:122\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.626\n",
            "Epoch:122\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.749\n",
            "Epoch:122\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.625\n",
            "Epoch:122\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.644\n",
            "Epoch:122\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.918\n",
            "Epoch:122\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.808\n",
            "Epoch:122\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.708\n",
            "Epoch:122\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.678\n",
            "Epoch:122\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.769\n",
            "Epoch:122\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.697\n",
            "Epoch:122\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.926\n",
            "Epoch:122\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.834\n",
            "Epoch:122\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.801\n",
            "Epoch:122\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.710\n",
            "Epoch:122\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.779\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:123\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.727\n",
            "Epoch:123\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.711\n",
            "Epoch:123\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.745\n",
            "Epoch:123\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.760\n",
            "Epoch:123\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.562\n",
            "Epoch:123\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.728\n",
            "Epoch:123\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.574\n",
            "Epoch:123\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.971\n",
            "Epoch:123\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.774\n",
            "Epoch:123\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.692\n",
            "Epoch:123\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.613\n",
            "Epoch:123\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.633\n",
            "Epoch:123\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.612\n",
            "Epoch:123\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.722\n",
            "Epoch:123\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.551\n",
            "Epoch:123\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.592\n",
            "Epoch:123\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.813\n",
            "Epoch:123\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.799\n",
            "Epoch:123\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.747\n",
            "Epoch:123\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.815\n",
            "Epoch:123\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.831\n",
            "Epoch:123\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.744\n",
            "Epoch:123\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.758\n",
            "Epoch:123\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.755\n",
            "Epoch:123\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.753\n",
            "Epoch:123\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.674\n",
            "Epoch:123\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.805\n",
            "Epoch:123\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.596\n",
            "Epoch:123\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.848\n",
            "Epoch:123\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.752\n",
            "Epoch:123\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.666\n",
            "Epoch:123\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.652\n",
            "Epoch:123\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.540\n",
            "Epoch:123\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.788\n",
            "Epoch:123\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.692\n",
            "Epoch:123\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.666\n",
            "Epoch:123\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.781\n",
            "Epoch:123\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.653\n",
            "Epoch:123\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.737\n",
            "Epoch:123\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.746\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:124\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.612\n",
            "Epoch:124\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.681\n",
            "Epoch:124\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.633\n",
            "Epoch:124\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.766\n",
            "Epoch:124\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.578\n",
            "Epoch:124\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.560\n",
            "Epoch:124\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.766\n",
            "Epoch:124\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.829\n",
            "Epoch:124\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.615\n",
            "Epoch:124\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.678\n",
            "Epoch:124\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.720\n",
            "Epoch:124\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.592\n",
            "Epoch:124\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.814\n",
            "Epoch:124\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.635\n",
            "Epoch:124\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.667\n",
            "Epoch:124\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.685\n",
            "Epoch:124\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.670\n",
            "Epoch:124\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.612\n",
            "Epoch:124\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.746\n",
            "Epoch:124\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.633\n",
            "Epoch:124\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.479\n",
            "Epoch:124\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.712\n",
            "Epoch:124\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.710\n",
            "Epoch:124\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.733\n",
            "Epoch:124\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.710\n",
            "Epoch:124\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.826\n",
            "Epoch:124\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.777\n",
            "Epoch:124\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.807\n",
            "Epoch:124\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.698\n",
            "Epoch:124\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.745\n",
            "Epoch:124\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.591\n",
            "Epoch:124\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.822\n",
            "Epoch:124\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.849\n",
            "Epoch:124\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.654\n",
            "Epoch:124\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.669\n",
            "Epoch:124\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.823\n",
            "Epoch:124\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.888\n",
            "Epoch:124\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.747\n",
            "Epoch:124\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.576\n",
            "Epoch:124\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.677\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:125\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.614\n",
            "Epoch:125\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.664\n",
            "Epoch:125\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.631\n",
            "Epoch:125\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.543\n",
            "Epoch:125\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.998\n",
            "Epoch:125\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.652\n",
            "Epoch:125\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.693\n",
            "Epoch:125\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.724\n",
            "Epoch:125\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.683\n",
            "Epoch:125\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.752\n",
            "Epoch:125\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.684\n",
            "Epoch:125\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.790\n",
            "Epoch:125\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.583\n",
            "Epoch:125\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.827\n",
            "Epoch:125\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.709\n",
            "Epoch:125\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.501\n",
            "Epoch:125\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.646\n",
            "Epoch:125\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.559\n",
            "Epoch:125\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.728\n",
            "Epoch:125\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.540\n",
            "Epoch:125\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.713\n",
            "Epoch:125\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.808\n",
            "Epoch:125\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.757\n",
            "Epoch:125\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.619\n",
            "Epoch:125\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.725\n",
            "Epoch:125\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.860\n",
            "Epoch:125\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.791\n",
            "Epoch:125\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.742\n",
            "Epoch:125\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.818\n",
            "Epoch:125\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.741\n",
            "Epoch:125\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.659\n",
            "Epoch:125\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.797\n",
            "Epoch:125\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.581\n",
            "Epoch:125\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.710\n",
            "Epoch:125\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.629\n",
            "Epoch:125\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.602\n",
            "Epoch:125\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.637\n",
            "Epoch:125\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.617\n",
            "Epoch:125\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.971\n",
            "Epoch:125\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.880\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:126\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.740\n",
            "Epoch:126\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.566\n",
            "Epoch:126\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.646\n",
            "Epoch:126\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.612\n",
            "Epoch:126\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.649\n",
            "Epoch:126\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.657\n",
            "Epoch:126\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.547\n",
            "Epoch:126\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.540\n",
            "Epoch:126\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.631\n",
            "Epoch:126\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.673\n",
            "Epoch:126\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.587\n",
            "Epoch:126\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.676\n",
            "Epoch:126\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.656\n",
            "Epoch:126\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.598\n",
            "Epoch:126\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.692\n",
            "Epoch:126\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.714\n",
            "Epoch:126\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.673\n",
            "Epoch:126\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.538\n",
            "Epoch:126\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.659\n",
            "Epoch:126\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.682\n",
            "Epoch:126\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.805\n",
            "Epoch:126\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.492\n",
            "Epoch:126\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.622\n",
            "Epoch:126\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.484\n",
            "Epoch:126\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.571\n",
            "Epoch:126\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.662\n",
            "Epoch:126\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.681\n",
            "Epoch:126\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.689\n",
            "Epoch:126\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.818\n",
            "Epoch:126\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.754\n",
            "Epoch:126\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.626\n",
            "Epoch:126\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.689\n",
            "Epoch:126\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.734\n",
            "Epoch:126\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.614\n",
            "Epoch:126\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.906\n",
            "Epoch:126\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.970\n",
            "Epoch:126\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.686\n",
            "Epoch:126\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.743\n",
            "Epoch:126\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.582\n",
            "Epoch:126\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.706\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:127\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.773\n",
            "Epoch:127\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.645\n",
            "Epoch:127\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.539\n",
            "Epoch:127\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.620\n",
            "Epoch:127\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.634\n",
            "Epoch:127\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.538\n",
            "Epoch:127\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.645\n",
            "Epoch:127\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.671\n",
            "Epoch:127\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.655\n",
            "Epoch:127\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.653\n",
            "Epoch:127\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.595\n",
            "Epoch:127\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.610\n",
            "Epoch:127\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.638\n",
            "Epoch:127\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.776\n",
            "Epoch:127\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.644\n",
            "Epoch:127\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.625\n",
            "Epoch:127\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.663\n",
            "Epoch:127\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.558\n",
            "Epoch:127\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.717\n",
            "Epoch:127\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.681\n",
            "Epoch:127\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.686\n",
            "Epoch:127\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.877\n",
            "Epoch:127\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.627\n",
            "Epoch:127\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.652\n",
            "Epoch:127\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.630\n",
            "Epoch:127\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.674\n",
            "Epoch:127\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.627\n",
            "Epoch:127\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.614\n",
            "Epoch:127\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.709\n",
            "Epoch:127\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.548\n",
            "Epoch:127\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.670\n",
            "Epoch:127\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.521\n",
            "Epoch:127\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.697\n",
            "Epoch:127\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.726\n",
            "Epoch:127\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.666\n",
            "Epoch:127\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.847\n",
            "Epoch:127\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.617\n",
            "Epoch:127\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.701\n",
            "Epoch:127\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.583\n",
            "Epoch:127\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.703\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:128\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.575\n",
            "Epoch:128\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.659\n",
            "Epoch:128\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.733\n",
            "Epoch:128\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.678\n",
            "Epoch:128\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.851\n",
            "Epoch:128\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.732\n",
            "Epoch:128\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.709\n",
            "Epoch:128\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.570\n",
            "Epoch:128\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.672\n",
            "Epoch:128\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.677\n",
            "Epoch:128\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.675\n",
            "Epoch:128\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.642\n",
            "Epoch:128\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.688\n",
            "Epoch:128\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.770\n",
            "Epoch:128\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.713\n",
            "Epoch:128\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.560\n",
            "Epoch:128\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.817\n",
            "Epoch:128\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.577\n",
            "Epoch:128\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.443\n",
            "Epoch:128\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.668\n",
            "Epoch:128\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.874\n",
            "Epoch:128\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.663\n",
            "Epoch:128\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.643\n",
            "Epoch:128\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.673\n",
            "Epoch:128\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.792\n",
            "Epoch:128\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.766\n",
            "Epoch:128\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.636\n",
            "Epoch:128\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.567\n",
            "Epoch:128\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.677\n",
            "Epoch:128\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.601\n",
            "Epoch:128\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.576\n",
            "Epoch:128\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.678\n",
            "Epoch:128\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.599\n",
            "Epoch:128\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.612\n",
            "Epoch:128\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.569\n",
            "Epoch:128\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.698\n",
            "Epoch:128\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.656\n",
            "Epoch:128\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.696\n",
            "Epoch:128\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.582\n",
            "Epoch:128\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.731\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:129\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.628\n",
            "Epoch:129\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.724\n",
            "Epoch:129\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.493\n",
            "Epoch:129\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.652\n",
            "Epoch:129\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.699\n",
            "Epoch:129\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.642\n",
            "Epoch:129\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.612\n",
            "Epoch:129\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.642\n",
            "Epoch:129\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.650\n",
            "Epoch:129\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.666\n",
            "Epoch:129\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.716\n",
            "Epoch:129\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.819\n",
            "Epoch:129\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.676\n",
            "Epoch:129\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.576\n",
            "Epoch:129\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.719\n",
            "Epoch:129\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.742\n",
            "Epoch:129\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.738\n",
            "Epoch:129\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.654\n",
            "Epoch:129\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.673\n",
            "Epoch:129\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.637\n",
            "Epoch:129\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.689\n",
            "Epoch:129\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.597\n",
            "Epoch:129\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.952\n",
            "Epoch:129\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.652\n",
            "Epoch:129\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.632\n",
            "Epoch:129\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.647\n",
            "Epoch:129\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.588\n",
            "Epoch:129\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.578\n",
            "Epoch:129\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.785\n",
            "Epoch:129\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.664\n",
            "Epoch:129\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.600\n",
            "Epoch:129\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.650\n",
            "Epoch:129\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.560\n",
            "Epoch:129\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.753\n",
            "Epoch:129\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.930\n",
            "Epoch:129\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.727\n",
            "Epoch:129\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.677\n",
            "Epoch:129\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.711\n",
            "Epoch:129\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.693\n",
            "Epoch:129\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.641\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:130\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.590\n",
            "Epoch:130\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.806\n",
            "Epoch:130\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.551\n",
            "Epoch:130\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.699\n",
            "Epoch:130\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.421\n",
            "Epoch:130\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.469\n",
            "Epoch:130\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.637\n",
            "Epoch:130\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.752\n",
            "Epoch:130\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.632\n",
            "Epoch:130\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.596\n",
            "Epoch:130\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.613\n",
            "Epoch:130\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.696\n",
            "Epoch:130\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.701\n",
            "Epoch:130\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.891\n",
            "Epoch:130\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.723\n",
            "Epoch:130\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.569\n",
            "Epoch:130\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.597\n",
            "Epoch:130\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.541\n",
            "Epoch:130\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.656\n",
            "Epoch:130\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.770\n",
            "Epoch:130\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.604\n",
            "Epoch:130\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.584\n",
            "Epoch:130\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.714\n",
            "Epoch:130\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.665\n",
            "Epoch:130\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.562\n",
            "Epoch:130\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.652\n",
            "Epoch:130\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.673\n",
            "Epoch:130\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.579\n",
            "Epoch:130\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.739\n",
            "Epoch:130\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.554\n",
            "Epoch:130\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.500\n",
            "Epoch:130\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.668\n",
            "Epoch:130\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.755\n",
            "Epoch:130\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.691\n",
            "Epoch:130\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.574\n",
            "Epoch:130\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.558\n",
            "Epoch:130\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.662\n",
            "Epoch:130\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.687\n",
            "Epoch:130\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.565\n",
            "Epoch:130\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.857\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:131\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.503\n",
            "Epoch:131\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.761\n",
            "Epoch:131\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.779\n",
            "Epoch:131\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.750\n",
            "Epoch:131\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.570\n",
            "Epoch:131\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.639\n",
            "Epoch:131\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.534\n",
            "Epoch:131\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.647\n",
            "Epoch:131\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.628\n",
            "Epoch:131\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.617\n",
            "Epoch:131\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.799\n",
            "Epoch:131\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.485\n",
            "Epoch:131\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.584\n",
            "Epoch:131\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.495\n",
            "Epoch:131\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.597\n",
            "Epoch:131\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.558\n",
            "Epoch:131\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.664\n",
            "Epoch:131\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.628\n",
            "Epoch:131\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.598\n",
            "Epoch:131\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.708\n",
            "Epoch:131\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.526\n",
            "Epoch:131\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.572\n",
            "Epoch:131\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.606\n",
            "Epoch:131\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.607\n",
            "Epoch:131\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.509\n",
            "Epoch:131\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.537\n",
            "Epoch:131\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.666\n",
            "Epoch:131\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.659\n",
            "Epoch:131\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.673\n",
            "Epoch:131\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.780\n",
            "Epoch:131\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.721\n",
            "Epoch:131\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.847\n",
            "Epoch:131\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.586\n",
            "Epoch:131\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.868\n",
            "Epoch:131\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.717\n",
            "Epoch:131\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.759\n",
            "Epoch:131\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.673\n",
            "Epoch:131\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.628\n",
            "Epoch:131\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.868\n",
            "Epoch:131\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.722\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:132\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.462\n",
            "Epoch:132\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.591\n",
            "Epoch:132\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.575\n",
            "Epoch:132\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.583\n",
            "Epoch:132\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.664\n",
            "Epoch:132\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.815\n",
            "Epoch:132\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.633\n",
            "Epoch:132\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.659\n",
            "Epoch:132\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.744\n",
            "Epoch:132\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.599\n",
            "Epoch:132\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.683\n",
            "Epoch:132\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.493\n",
            "Epoch:132\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.666\n",
            "Epoch:132\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.540\n",
            "Epoch:132\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.586\n",
            "Epoch:132\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.830\n",
            "Epoch:132\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.706\n",
            "Epoch:132\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.704\n",
            "Epoch:132\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.869\n",
            "Epoch:132\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.773\n",
            "Epoch:132\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.567\n",
            "Epoch:132\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.506\n",
            "Epoch:132\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.702\n",
            "Epoch:132\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.639\n",
            "Epoch:132\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.641\n",
            "Epoch:132\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.781\n",
            "Epoch:132\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.726\n",
            "Epoch:132\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.502\n",
            "Epoch:132\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.567\n",
            "Epoch:132\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.563\n",
            "Epoch:132\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.884\n",
            "Epoch:132\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.649\n",
            "Epoch:132\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.787\n",
            "Epoch:132\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.633\n",
            "Epoch:132\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.578\n",
            "Epoch:132\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.752\n",
            "Epoch:132\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.611\n",
            "Epoch:132\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.744\n",
            "Epoch:132\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.763\n",
            "Epoch:132\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.740\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:133\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.532\n",
            "Epoch:133\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.575\n",
            "Epoch:133\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.559\n",
            "Epoch:133\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.496\n",
            "Epoch:133\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.778\n",
            "Epoch:133\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.717\n",
            "Epoch:133\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.593\n",
            "Epoch:133\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.625\n",
            "Epoch:133\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.539\n",
            "Epoch:133\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.790\n",
            "Epoch:133\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.595\n",
            "Epoch:133\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.748\n",
            "Epoch:133\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.780\n",
            "Epoch:133\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.541\n",
            "Epoch:133\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.484\n",
            "Epoch:133\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.716\n",
            "Epoch:133\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.585\n",
            "Epoch:133\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.534\n",
            "Epoch:133\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.595\n",
            "Epoch:133\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.604\n",
            "Epoch:133\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.674\n",
            "Epoch:133\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.568\n",
            "Epoch:133\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.592\n",
            "Epoch:133\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.674\n",
            "Epoch:133\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.614\n",
            "Epoch:133\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.601\n",
            "Epoch:133\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.592\n",
            "Epoch:133\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.610\n",
            "Epoch:133\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.566\n",
            "Epoch:133\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.636\n",
            "Epoch:133\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.638\n",
            "Epoch:133\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.627\n",
            "Epoch:133\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.561\n",
            "Epoch:133\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.633\n",
            "Epoch:133\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.600\n",
            "Epoch:133\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.782\n",
            "Epoch:133\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.768\n",
            "Epoch:133\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.638\n",
            "Epoch:133\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.545\n",
            "Epoch:133\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.573\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:134\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.644\n",
            "Epoch:134\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.490\n",
            "Epoch:134\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.691\n",
            "Epoch:134\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.531\n",
            "Epoch:134\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.577\n",
            "Epoch:134\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.636\n",
            "Epoch:134\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.658\n",
            "Epoch:134\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.688\n",
            "Epoch:134\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.617\n",
            "Epoch:134\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.471\n",
            "Epoch:134\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.830\n",
            "Epoch:134\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.506\n",
            "Epoch:134\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.614\n",
            "Epoch:134\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.639\n",
            "Epoch:134\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.548\n",
            "Epoch:134\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.560\n",
            "Epoch:134\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.634\n",
            "Epoch:134\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.634\n",
            "Epoch:134\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.621\n",
            "Epoch:134\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.700\n",
            "Epoch:134\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.657\n",
            "Epoch:134\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.616\n",
            "Epoch:134\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.569\n",
            "Epoch:134\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.883\n",
            "Epoch:134\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.629\n",
            "Epoch:134\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.610\n",
            "Epoch:134\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.700\n",
            "Epoch:134\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.598\n",
            "Epoch:134\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.540\n",
            "Epoch:134\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.506\n",
            "Epoch:134\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.721\n",
            "Epoch:134\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.672\n",
            "Epoch:134\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.955\n",
            "Epoch:134\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.560\n",
            "Epoch:134\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.712\n",
            "Epoch:134\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.698\n",
            "Epoch:134\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.636\n",
            "Epoch:134\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.821\n",
            "Epoch:134\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.735\n",
            "Epoch:134\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.639\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:135\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.474\n",
            "Epoch:135\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.545\n",
            "Epoch:135\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.612\n",
            "Epoch:135\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.624\n",
            "Epoch:135\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.650\n",
            "Epoch:135\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.607\n",
            "Epoch:135\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.620\n",
            "Epoch:135\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.714\n",
            "Epoch:135\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.651\n",
            "Epoch:135\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.774\n",
            "Epoch:135\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.554\n",
            "Epoch:135\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.538\n",
            "Epoch:135\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.745\n",
            "Epoch:135\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.617\n",
            "Epoch:135\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.416\n",
            "Epoch:135\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.727\n",
            "Epoch:135\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.557\n",
            "Epoch:135\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.659\n",
            "Epoch:135\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.618\n",
            "Epoch:135\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.536\n",
            "Epoch:135\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.568\n",
            "Epoch:135\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.567\n",
            "Epoch:135\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.734\n",
            "Epoch:135\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.843\n",
            "Epoch:135\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.767\n",
            "Epoch:135\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.622\n",
            "Epoch:135\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.722\n",
            "Epoch:135\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.667\n",
            "Epoch:135\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.670\n",
            "Epoch:135\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.714\n",
            "Epoch:135\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.774\n",
            "Epoch:135\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.474\n",
            "Epoch:135\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.582\n",
            "Epoch:135\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.695\n",
            "Epoch:135\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.742\n",
            "Epoch:135\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.644\n",
            "Epoch:135\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.646\n",
            "Epoch:135\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.785\n",
            "Epoch:135\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.533\n",
            "Epoch:135\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.845\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:136\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.698\n",
            "Epoch:136\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.641\n",
            "Epoch:136\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.480\n",
            "Epoch:136\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.685\n",
            "Epoch:136\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.720\n",
            "Epoch:136\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.563\n",
            "Epoch:136\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.658\n",
            "Epoch:136\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.592\n",
            "Epoch:136\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.666\n",
            "Epoch:136\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.441\n",
            "Epoch:136\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.618\n",
            "Epoch:136\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.726\n",
            "Epoch:136\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.671\n",
            "Epoch:136\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.662\n",
            "Epoch:136\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.574\n",
            "Epoch:136\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.599\n",
            "Epoch:136\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.700\n",
            "Epoch:136\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.676\n",
            "Epoch:136\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.621\n",
            "Epoch:136\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.563\n",
            "Epoch:136\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.584\n",
            "Epoch:136\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.556\n",
            "Epoch:136\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.442\n",
            "Epoch:136\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.602\n",
            "Epoch:136\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.710\n",
            "Epoch:136\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.550\n",
            "Epoch:136\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.575\n",
            "Epoch:136\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.582\n",
            "Epoch:136\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.588\n",
            "Epoch:136\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.757\n",
            "Epoch:136\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.659\n",
            "Epoch:136\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.603\n",
            "Epoch:136\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.516\n",
            "Epoch:136\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.766\n",
            "Epoch:136\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.595\n",
            "Epoch:136\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.531\n",
            "Epoch:136\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.570\n",
            "Epoch:136\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.749\n",
            "Epoch:136\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.550\n",
            "Epoch:136\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.767\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:137\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.527\n",
            "Epoch:137\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.610\n",
            "Epoch:137\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.572\n",
            "Epoch:137\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.489\n",
            "Epoch:137\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.510\n",
            "Epoch:137\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.622\n",
            "Epoch:137\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.558\n",
            "Epoch:137\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.578\n",
            "Epoch:137\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.420\n",
            "Epoch:137\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.709\n",
            "Epoch:137\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.441\n",
            "Epoch:137\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.702\n",
            "Epoch:137\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.492\n",
            "Epoch:137\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.722\n",
            "Epoch:137\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.831\n",
            "Epoch:137\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.559\n",
            "Epoch:137\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.625\n",
            "Epoch:137\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.534\n",
            "Epoch:137\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.718\n",
            "Epoch:137\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.499\n",
            "Epoch:137\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.805\n",
            "Epoch:137\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.574\n",
            "Epoch:137\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.488\n",
            "Epoch:137\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.556\n",
            "Epoch:137\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.851\n",
            "Epoch:137\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.867\n",
            "Epoch:137\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.612\n",
            "Epoch:137\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.589\n",
            "Epoch:137\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.482\n",
            "Epoch:137\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.449\n",
            "Epoch:137\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.593\n",
            "Epoch:137\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.608\n",
            "Epoch:137\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.742\n",
            "Epoch:137\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.870\n",
            "Epoch:137\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.551\n",
            "Epoch:137\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.657\n",
            "Epoch:137\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.584\n",
            "Epoch:137\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.752\n",
            "Epoch:137\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.504\n",
            "Epoch:137\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.885\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:138\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.484\n",
            "Epoch:138\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.554\n",
            "Epoch:138\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.492\n",
            "Epoch:138\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.717\n",
            "Epoch:138\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.487\n",
            "Epoch:138\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.647\n",
            "Epoch:138\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.495\n",
            "Epoch:138\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.539\n",
            "Epoch:138\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.618\n",
            "Epoch:138\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.702\n",
            "Epoch:138\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.715\n",
            "Epoch:138\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.593\n",
            "Epoch:138\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.504\n",
            "Epoch:138\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.574\n",
            "Epoch:138\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.517\n",
            "Epoch:138\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.831\n",
            "Epoch:138\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.721\n",
            "Epoch:138\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.649\n",
            "Epoch:138\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.786\n",
            "Epoch:138\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.477\n",
            "Epoch:138\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.432\n",
            "Epoch:138\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.476\n",
            "Epoch:138\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.759\n",
            "Epoch:138\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.578\n",
            "Epoch:138\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.650\n",
            "Epoch:138\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.675\n",
            "Epoch:138\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.539\n",
            "Epoch:138\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.707\n",
            "Epoch:138\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.657\n",
            "Epoch:138\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.618\n",
            "Epoch:138\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.568\n",
            "Epoch:138\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.787\n",
            "Epoch:138\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.742\n",
            "Epoch:138\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.851\n",
            "Epoch:138\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.833\n",
            "Epoch:138\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.486\n",
            "Epoch:138\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.581\n",
            "Epoch:138\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.551\n",
            "Epoch:138\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.630\n",
            "Epoch:138\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.462\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:139\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.475\n",
            "Epoch:139\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.618\n",
            "Epoch:139\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.535\n",
            "Epoch:139\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.629\n",
            "Epoch:139\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.521\n",
            "Epoch:139\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.525\n",
            "Epoch:139\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.631\n",
            "Epoch:139\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.549\n",
            "Epoch:139\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.583\n",
            "Epoch:139\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.784\n",
            "Epoch:139\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.531\n",
            "Epoch:139\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.627\n",
            "Epoch:139\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.730\n",
            "Epoch:139\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.661\n",
            "Epoch:139\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.652\n",
            "Epoch:139\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.533\n",
            "Epoch:139\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.737\n",
            "Epoch:139\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.639\n",
            "Epoch:139\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.692\n",
            "Epoch:139\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.719\n",
            "Epoch:139\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.601\n",
            "Epoch:139\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.744\n",
            "Epoch:139\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.631\n",
            "Epoch:139\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.670\n",
            "Epoch:139\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.471\n",
            "Epoch:139\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.852\n",
            "Epoch:139\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.687\n",
            "Epoch:139\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.497\n",
            "Epoch:139\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.553\n",
            "Epoch:139\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.603\n",
            "Epoch:139\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.664\n",
            "Epoch:139\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.518\n",
            "Epoch:139\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.772\n",
            "Epoch:139\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.516\n",
            "Epoch:139\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.823\n",
            "Epoch:139\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.497\n",
            "Epoch:139\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.625\n",
            "Epoch:139\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.702\n",
            "Epoch:139\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.610\n",
            "Epoch:139\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.503\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:140\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.654\n",
            "Epoch:140\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.557\n",
            "Epoch:140\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.636\n",
            "Epoch:140\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.724\n",
            "Epoch:140\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.584\n",
            "Epoch:140\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.663\n",
            "Epoch:140\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.645\n",
            "Epoch:140\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.544\n",
            "Epoch:140\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.716\n",
            "Epoch:140\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.454\n",
            "Epoch:140\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.571\n",
            "Epoch:140\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.555\n",
            "Epoch:140\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.740\n",
            "Epoch:140\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.582\n",
            "Epoch:140\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.566\n",
            "Epoch:140\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.520\n",
            "Epoch:140\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.529\n",
            "Epoch:140\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.547\n",
            "Epoch:140\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.749\n",
            "Epoch:140\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.657\n",
            "Epoch:140\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.628\n",
            "Epoch:140\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.784\n",
            "Epoch:140\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.776\n",
            "Epoch:140\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.675\n",
            "Epoch:140\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.658\n",
            "Epoch:140\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.632\n",
            "Epoch:140\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.624\n",
            "Epoch:140\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.621\n",
            "Epoch:140\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.627\n",
            "Epoch:140\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.749\n",
            "Epoch:140\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.641\n",
            "Epoch:140\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.764\n",
            "Epoch:140\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.919\n",
            "Epoch:140\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.549\n",
            "Epoch:140\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.726\n",
            "Epoch:140\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.692\n",
            "Epoch:140\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.676\n",
            "Epoch:140\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.511\n",
            "Epoch:140\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.755\n",
            "Epoch:140\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.645\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:141\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.569\n",
            "Epoch:141\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.599\n",
            "Epoch:141\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.636\n",
            "Epoch:141\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.600\n",
            "Epoch:141\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.514\n",
            "Epoch:141\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.543\n",
            "Epoch:141\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.700\n",
            "Epoch:141\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.657\n",
            "Epoch:141\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.563\n",
            "Epoch:141\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.725\n",
            "Epoch:141\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.669\n",
            "Epoch:141\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.539\n",
            "Epoch:141\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.566\n",
            "Epoch:141\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.742\n",
            "Epoch:141\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.588\n",
            "Epoch:141\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.659\n",
            "Epoch:141\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.556\n",
            "Epoch:141\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.583\n",
            "Epoch:141\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.620\n",
            "Epoch:141\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.551\n",
            "Epoch:141\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.641\n",
            "Epoch:141\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.593\n",
            "Epoch:141\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.600\n",
            "Epoch:141\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.832\n",
            "Epoch:141\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.610\n",
            "Epoch:141\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.574\n",
            "Epoch:141\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.584\n",
            "Epoch:141\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.530\n",
            "Epoch:141\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.526\n",
            "Epoch:141\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.473\n",
            "Epoch:141\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.576\n",
            "Epoch:141\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.716\n",
            "Epoch:141\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.840\n",
            "Epoch:141\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.612\n",
            "Epoch:141\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.650\n",
            "Epoch:141\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.656\n",
            "Epoch:141\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.538\n",
            "Epoch:141\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.527\n",
            "Epoch:141\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.631\n",
            "Epoch:141\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.657\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:142\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.674\n",
            "Epoch:142\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.604\n",
            "Epoch:142\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.590\n",
            "Epoch:142\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.634\n",
            "Epoch:142\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.651\n",
            "Epoch:142\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.729\n",
            "Epoch:142\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.456\n",
            "Epoch:142\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.682\n",
            "Epoch:142\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.674\n",
            "Epoch:142\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.551\n",
            "Epoch:142\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.687\n",
            "Epoch:142\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.544\n",
            "Epoch:142\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.726\n",
            "Epoch:142\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.693\n",
            "Epoch:142\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.549\n",
            "Epoch:142\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.790\n",
            "Epoch:142\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.667\n",
            "Epoch:142\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.629\n",
            "Epoch:142\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.534\n",
            "Epoch:142\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.707\n",
            "Epoch:142\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.686\n",
            "Epoch:142\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.661\n",
            "Epoch:142\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.622\n",
            "Epoch:142\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.705\n",
            "Epoch:142\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.626\n",
            "Epoch:142\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.514\n",
            "Epoch:142\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.668\n",
            "Epoch:142\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.613\n",
            "Epoch:142\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.410\n",
            "Epoch:142\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.528\n",
            "Epoch:142\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.522\n",
            "Epoch:142\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.672\n",
            "Epoch:142\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.824\n",
            "Epoch:142\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.552\n",
            "Epoch:142\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.633\n",
            "Epoch:142\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.679\n",
            "Epoch:142\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.527\n",
            "Epoch:142\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.541\n",
            "Epoch:142\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.533\n",
            "Epoch:142\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.926\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:143\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.625\n",
            "Epoch:143\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.441\n",
            "Epoch:143\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.688\n",
            "Epoch:143\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.623\n",
            "Epoch:143\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.629\n",
            "Epoch:143\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.523\n",
            "Epoch:143\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.645\n",
            "Epoch:143\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.633\n",
            "Epoch:143\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.604\n",
            "Epoch:143\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.646\n",
            "Epoch:143\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.515\n",
            "Epoch:143\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.423\n",
            "Epoch:143\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.510\n",
            "Epoch:143\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.560\n",
            "Epoch:143\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.604\n",
            "Epoch:143\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.559\n",
            "Epoch:143\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.619\n",
            "Epoch:143\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.754\n",
            "Epoch:143\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.410\n",
            "Epoch:143\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.641\n",
            "Epoch:143\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.727\n",
            "Epoch:143\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.553\n",
            "Epoch:143\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.503\n",
            "Epoch:143\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.658\n",
            "Epoch:143\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.650\n",
            "Epoch:143\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.625\n",
            "Epoch:143\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.703\n",
            "Epoch:143\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.570\n",
            "Epoch:143\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.521\n",
            "Epoch:143\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.625\n",
            "Epoch:143\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.651\n",
            "Epoch:143\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.498\n",
            "Epoch:143\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.760\n",
            "Epoch:143\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.663\n",
            "Epoch:143\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.754\n",
            "Epoch:143\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.673\n",
            "Epoch:143\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.586\n",
            "Epoch:143\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.632\n",
            "Epoch:143\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.750\n",
            "Epoch:143\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.644\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:144\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.478\n",
            "Epoch:144\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.737\n",
            "Epoch:144\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.584\n",
            "Epoch:144\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.599\n",
            "Epoch:144\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.480\n",
            "Epoch:144\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.615\n",
            "Epoch:144\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.722\n",
            "Epoch:144\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.600\n",
            "Epoch:144\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.743\n",
            "Epoch:144\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.573\n",
            "Epoch:144\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.545\n",
            "Epoch:144\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.424\n",
            "Epoch:144\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.573\n",
            "Epoch:144\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.578\n",
            "Epoch:144\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.705\n",
            "Epoch:144\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.669\n",
            "Epoch:144\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.600\n",
            "Epoch:144\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.784\n",
            "Epoch:144\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.633\n",
            "Epoch:144\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.794\n",
            "Epoch:144\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.592\n",
            "Epoch:144\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.524\n",
            "Epoch:144\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.529\n",
            "Epoch:144\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.576\n",
            "Epoch:144\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.631\n",
            "Epoch:144\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.750\n",
            "Epoch:144\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.558\n",
            "Epoch:144\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.589\n",
            "Epoch:144\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.675\n",
            "Epoch:144\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.727\n",
            "Epoch:144\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.799\n",
            "Epoch:144\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.558\n",
            "Epoch:144\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.579\n",
            "Epoch:144\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.880\n",
            "Epoch:144\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.727\n",
            "Epoch:144\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.793\n",
            "Epoch:144\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.518\n",
            "Epoch:144\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.672\n",
            "Epoch:144\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.754\n",
            "Epoch:144\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.627\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:145\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.461\n",
            "Epoch:145\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.611\n",
            "Epoch:145\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.620\n",
            "Epoch:145\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.459\n",
            "Epoch:145\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.539\n",
            "Epoch:145\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.401\n",
            "Epoch:145\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.436\n",
            "Epoch:145\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.627\n",
            "Epoch:145\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.671\n",
            "Epoch:145\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.436\n",
            "Epoch:145\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.558\n",
            "Epoch:145\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.697\n",
            "Epoch:145\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.548\n",
            "Epoch:145\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.653\n",
            "Epoch:145\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.644\n",
            "Epoch:145\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.532\n",
            "Epoch:145\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.547\n",
            "Epoch:145\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.680\n",
            "Epoch:145\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.550\n",
            "Epoch:145\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.682\n",
            "Epoch:145\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.617\n",
            "Epoch:145\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.670\n",
            "Epoch:145\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.653\n",
            "Epoch:145\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.784\n",
            "Epoch:145\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.649\n",
            "Epoch:145\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.562\n",
            "Epoch:145\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.672\n",
            "Epoch:145\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.724\n",
            "Epoch:145\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.580\n",
            "Epoch:145\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.563\n",
            "Epoch:145\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.578\n",
            "Epoch:145\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.593\n",
            "Epoch:145\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.567\n",
            "Epoch:145\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.597\n",
            "Epoch:145\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.641\n",
            "Epoch:145\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.437\n",
            "Epoch:145\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.648\n",
            "Epoch:145\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.595\n",
            "Epoch:145\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.549\n",
            "Epoch:145\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.897\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:146\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.593\n",
            "Epoch:146\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.588\n",
            "Epoch:146\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.673\n",
            "Epoch:146\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.635\n",
            "Epoch:146\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.645\n",
            "Epoch:146\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.642\n",
            "Epoch:146\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.686\n",
            "Epoch:146\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.587\n",
            "Epoch:146\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.534\n",
            "Epoch:146\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.576\n",
            "Epoch:146\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.601\n",
            "Epoch:146\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.554\n",
            "Epoch:146\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.630\n",
            "Epoch:146\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.692\n",
            "Epoch:146\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.555\n",
            "Epoch:146\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.553\n",
            "Epoch:146\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.626\n",
            "Epoch:146\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.593\n",
            "Epoch:146\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.460\n",
            "Epoch:146\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.460\n",
            "Epoch:146\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.571\n",
            "Epoch:146\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.571\n",
            "Epoch:146\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.579\n",
            "Epoch:146\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.544\n",
            "Epoch:146\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.628\n",
            "Epoch:146\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.593\n",
            "Epoch:146\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.599\n",
            "Epoch:146\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.648\n",
            "Epoch:146\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.520\n",
            "Epoch:146\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.734\n",
            "Epoch:146\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.612\n",
            "Epoch:146\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.573\n",
            "Epoch:146\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.750\n",
            "Epoch:146\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.602\n",
            "Epoch:146\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.594\n",
            "Epoch:146\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.544\n",
            "Epoch:146\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.610\n",
            "Epoch:146\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.874\n",
            "Epoch:146\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.512\n",
            "Epoch:146\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.537\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:147\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.604\n",
            "Epoch:147\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.666\n",
            "Epoch:147\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.626\n",
            "Epoch:147\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.529\n",
            "Epoch:147\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.440\n",
            "Epoch:147\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.748\n",
            "Epoch:147\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.728\n",
            "Epoch:147\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.714\n",
            "Epoch:147\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.515\n",
            "Epoch:147\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.469\n",
            "Epoch:147\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.646\n",
            "Epoch:147\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.615\n",
            "Epoch:147\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.764\n",
            "Epoch:147\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.566\n",
            "Epoch:147\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.640\n",
            "Epoch:147\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.605\n",
            "Epoch:147\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.545\n",
            "Epoch:147\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.533\n",
            "Epoch:147\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.614\n",
            "Epoch:147\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.545\n",
            "Epoch:147\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.429\n",
            "Epoch:147\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.725\n",
            "Epoch:147\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.640\n",
            "Epoch:147\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.631\n",
            "Epoch:147\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.685\n",
            "Epoch:147\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.500\n",
            "Epoch:147\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.727\n",
            "Epoch:147\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.656\n",
            "Epoch:147\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.605\n",
            "Epoch:147\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.662\n",
            "Epoch:147\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.734\n",
            "Epoch:147\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.624\n",
            "Epoch:147\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.690\n",
            "Epoch:147\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.557\n",
            "Epoch:147\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.663\n",
            "Epoch:147\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.774\n",
            "Epoch:147\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.636\n",
            "Epoch:147\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.518\n",
            "Epoch:147\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.750\n",
            "Epoch:147\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.776\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:148\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.726\n",
            "Epoch:148\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.599\n",
            "Epoch:148\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.642\n",
            "Epoch:148\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.599\n",
            "Epoch:148\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.599\n",
            "Epoch:148\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.641\n",
            "Epoch:148\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.613\n",
            "Epoch:148\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.727\n",
            "Epoch:148\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.528\n",
            "Epoch:148\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.442\n",
            "Epoch:148\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.550\n",
            "Epoch:148\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.557\n",
            "Epoch:148\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.429\n",
            "Epoch:148\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.537\n",
            "Epoch:148\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.540\n",
            "Epoch:148\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.483\n",
            "Epoch:148\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.606\n",
            "Epoch:148\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.580\n",
            "Epoch:148\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.588\n",
            "Epoch:148\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.609\n",
            "Epoch:148\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.577\n",
            "Epoch:148\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.488\n",
            "Epoch:148\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.624\n",
            "Epoch:148\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.554\n",
            "Epoch:148\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.479\n",
            "Epoch:148\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.482\n",
            "Epoch:148\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.729\n",
            "Epoch:148\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.601\n",
            "Epoch:148\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.543\n",
            "Epoch:148\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.527\n",
            "Epoch:148\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.654\n",
            "Epoch:148\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.728\n",
            "Epoch:148\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.710\n",
            "Epoch:148\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.440\n",
            "Epoch:148\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.605\n",
            "Epoch:148\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.920\n",
            "Epoch:148\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.730\n",
            "Epoch:148\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.656\n",
            "Epoch:148\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.647\n",
            "Epoch:148\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.857\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:149\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.630\n",
            "Epoch:149\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.802\n",
            "Epoch:149\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.737\n",
            "Epoch:149\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.631\n",
            "Epoch:149\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.557\n",
            "Epoch:149\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.465\n",
            "Epoch:149\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.596\n",
            "Epoch:149\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.625\n",
            "Epoch:149\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.589\n",
            "Epoch:149\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.731\n",
            "Epoch:149\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.585\n",
            "Epoch:149\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.585\n",
            "Epoch:149\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.581\n",
            "Epoch:149\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.543\n",
            "Epoch:149\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.735\n",
            "Epoch:149\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.690\n",
            "Epoch:149\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.543\n",
            "Epoch:149\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.537\n",
            "Epoch:149\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.529\n",
            "Epoch:149\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.713\n",
            "Epoch:149\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.630\n",
            "Epoch:149\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.681\n",
            "Epoch:149\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.580\n",
            "Epoch:149\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.628\n",
            "Epoch:149\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.684\n",
            "Epoch:149\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.536\n",
            "Epoch:149\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.532\n",
            "Epoch:149\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.463\n",
            "Epoch:149\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.871\n",
            "Epoch:149\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.555\n",
            "Epoch:149\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.593\n",
            "Epoch:149\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.781\n",
            "Epoch:149\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.480\n",
            "Epoch:149\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.585\n",
            "Epoch:149\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.586\n",
            "Epoch:149\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.600\n",
            "Epoch:149\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.583\n",
            "Epoch:149\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.704\n",
            "Epoch:149\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.718\n",
            "Epoch:149\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.766\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:150\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.582\n",
            "Epoch:150\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.493\n",
            "Epoch:150\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.489\n",
            "Epoch:150\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.502\n",
            "Epoch:150\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.677\n",
            "Epoch:150\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.636\n",
            "Epoch:150\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.557\n",
            "Epoch:150\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.527\n",
            "Epoch:150\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.713\n",
            "Epoch:150\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.484\n",
            "Epoch:150\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.605\n",
            "Epoch:150\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.449\n",
            "Epoch:150\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.602\n",
            "Epoch:150\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.502\n",
            "Epoch:150\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.709\n",
            "Epoch:150\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.610\n",
            "Epoch:150\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.697\n",
            "Epoch:150\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.631\n",
            "Epoch:150\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.646\n",
            "Epoch:150\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.558\n",
            "Epoch:150\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.435\n",
            "Epoch:150\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.611\n",
            "Epoch:150\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.426\n",
            "Epoch:150\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.448\n",
            "Epoch:150\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.597\n",
            "Epoch:150\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.800\n",
            "Epoch:150\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.705\n",
            "Epoch:150\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.805\n",
            "Epoch:150\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.554\n",
            "Epoch:150\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.736\n",
            "Epoch:150\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.616\n",
            "Epoch:150\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.546\n",
            "Epoch:150\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.486\n",
            "Epoch:150\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.499\n",
            "Epoch:150\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.657\n",
            "Epoch:150\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.508\n",
            "Epoch:150\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.658\n",
            "Epoch:150\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.663\n",
            "Epoch:150\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.564\n",
            "Epoch:150\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.563\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:151\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.551\n",
            "Epoch:151\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.444\n",
            "Epoch:151\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.403\n",
            "Epoch:151\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.515\n",
            "Epoch:151\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.570\n",
            "Epoch:151\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.546\n",
            "Epoch:151\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.589\n",
            "Epoch:151\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.601\n",
            "Epoch:151\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.587\n",
            "Epoch:151\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.658\n",
            "Epoch:151\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.605\n",
            "Epoch:151\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.632\n",
            "Epoch:151\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.466\n",
            "Epoch:151\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.468\n",
            "Epoch:151\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.667\n",
            "Epoch:151\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.660\n",
            "Epoch:151\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.559\n",
            "Epoch:151\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.701\n",
            "Epoch:151\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.646\n",
            "Epoch:151\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.731\n",
            "Epoch:151\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.548\n",
            "Epoch:151\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.487\n",
            "Epoch:151\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.464\n",
            "Epoch:151\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.562\n",
            "Epoch:151\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.649\n",
            "Epoch:151\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.681\n",
            "Epoch:151\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.595\n",
            "Epoch:151\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.592\n",
            "Epoch:151\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.672\n",
            "Epoch:151\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.613\n",
            "Epoch:151\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.441\n",
            "Epoch:151\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.660\n",
            "Epoch:151\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.499\n",
            "Epoch:151\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.623\n",
            "Epoch:151\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.705\n",
            "Epoch:151\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.512\n",
            "Epoch:151\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.617\n",
            "Epoch:151\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.584\n",
            "Epoch:151\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.592\n",
            "Epoch:151\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.842\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:152\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.480\n",
            "Epoch:152\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.614\n",
            "Epoch:152\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.507\n",
            "Epoch:152\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.560\n",
            "Epoch:152\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.503\n",
            "Epoch:152\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.710\n",
            "Epoch:152\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.478\n",
            "Epoch:152\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.633\n",
            "Epoch:152\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.606\n",
            "Epoch:152\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.400\n",
            "Epoch:152\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.567\n",
            "Epoch:152\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.436\n",
            "Epoch:152\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.506\n",
            "Epoch:152\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.622\n",
            "Epoch:152\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.615\n",
            "Epoch:152\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.399\n",
            "Epoch:152\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.604\n",
            "Epoch:152\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.484\n",
            "Epoch:152\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.712\n",
            "Epoch:152\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.537\n",
            "Epoch:152\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.721\n",
            "Epoch:152\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.572\n",
            "Epoch:152\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.717\n",
            "Epoch:152\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.568\n",
            "Epoch:152\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.584\n",
            "Epoch:152\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.657\n",
            "Epoch:152\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.539\n",
            "Epoch:152\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.534\n",
            "Epoch:152\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.691\n",
            "Epoch:152\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.538\n",
            "Epoch:152\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.623\n",
            "Epoch:152\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.614\n",
            "Epoch:152\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.781\n",
            "Epoch:152\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.506\n",
            "Epoch:152\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.612\n",
            "Epoch:152\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.459\n",
            "Epoch:152\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.670\n",
            "Epoch:152\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.626\n",
            "Epoch:152\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.533\n",
            "Epoch:152\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.953\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:153\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.656\n",
            "Epoch:153\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.633\n",
            "Epoch:153\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.453\n",
            "Epoch:153\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.553\n",
            "Epoch:153\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.514\n",
            "Epoch:153\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.589\n",
            "Epoch:153\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.492\n",
            "Epoch:153\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.458\n",
            "Epoch:153\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.716\n",
            "Epoch:153\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.580\n",
            "Epoch:153\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.422\n",
            "Epoch:153\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.701\n",
            "Epoch:153\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.645\n",
            "Epoch:153\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.593\n",
            "Epoch:153\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.567\n",
            "Epoch:153\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.632\n",
            "Epoch:153\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.752\n",
            "Epoch:153\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.758\n",
            "Epoch:153\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.522\n",
            "Epoch:153\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.650\n",
            "Epoch:153\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.722\n",
            "Epoch:153\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.591\n",
            "Epoch:153\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.564\n",
            "Epoch:153\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.510\n",
            "Epoch:153\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.643\n",
            "Epoch:153\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.518\n",
            "Epoch:153\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.682\n",
            "Epoch:153\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.713\n",
            "Epoch:153\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.581\n",
            "Epoch:153\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.609\n",
            "Epoch:153\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.579\n",
            "Epoch:153\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.696\n",
            "Epoch:153\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.739\n",
            "Epoch:153\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.563\n",
            "Epoch:153\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.524\n",
            "Epoch:153\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.518\n",
            "Epoch:153\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.550\n",
            "Epoch:153\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.664\n",
            "Epoch:153\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.551\n",
            "Epoch:153\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.744\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:154\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.607\n",
            "Epoch:154\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.680\n",
            "Epoch:154\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.459\n",
            "Epoch:154\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.537\n",
            "Epoch:154\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.716\n",
            "Epoch:154\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.728\n",
            "Epoch:154\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.539\n",
            "Epoch:154\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.564\n",
            "Epoch:154\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.529\n",
            "Epoch:154\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.608\n",
            "Epoch:154\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.609\n",
            "Epoch:154\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.531\n",
            "Epoch:154\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.655\n",
            "Epoch:154\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.573\n",
            "Epoch:154\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.475\n",
            "Epoch:154\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.819\n",
            "Epoch:154\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.748\n",
            "Epoch:154\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.547\n",
            "Epoch:154\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.618\n",
            "Epoch:154\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.675\n",
            "Epoch:154\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.486\n",
            "Epoch:154\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.593\n",
            "Epoch:154\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.724\n",
            "Epoch:154\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.615\n",
            "Epoch:154\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.621\n",
            "Epoch:154\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.484\n",
            "Epoch:154\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.615\n",
            "Epoch:154\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.582\n",
            "Epoch:154\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.579\n",
            "Epoch:154\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.521\n",
            "Epoch:154\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.530\n",
            "Epoch:154\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.598\n",
            "Epoch:154\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.632\n",
            "Epoch:154\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.694\n",
            "Epoch:154\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.554\n",
            "Epoch:154\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.656\n",
            "Epoch:154\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.606\n",
            "Epoch:154\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.671\n",
            "Epoch:154\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.660\n",
            "Epoch:154\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.564\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:155\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.629\n",
            "Epoch:155\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.590\n",
            "Epoch:155\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.587\n",
            "Epoch:155\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.573\n",
            "Epoch:155\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.527\n",
            "Epoch:155\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.548\n",
            "Epoch:155\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.584\n",
            "Epoch:155\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.565\n",
            "Epoch:155\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.674\n",
            "Epoch:155\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.477\n",
            "Epoch:155\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.561\n",
            "Epoch:155\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.510\n",
            "Epoch:155\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.682\n",
            "Epoch:155\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.551\n",
            "Epoch:155\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.558\n",
            "Epoch:155\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.576\n",
            "Epoch:155\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.808\n",
            "Epoch:155\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.673\n",
            "Epoch:155\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.524\n",
            "Epoch:155\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.492\n",
            "Epoch:155\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.492\n",
            "Epoch:155\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.721\n",
            "Epoch:155\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.643\n",
            "Epoch:155\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.479\n",
            "Epoch:155\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.582\n",
            "Epoch:155\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.577\n",
            "Epoch:155\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.646\n",
            "Epoch:155\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.648\n",
            "Epoch:155\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.535\n",
            "Epoch:155\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.705\n",
            "Epoch:155\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.678\n",
            "Epoch:155\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.585\n",
            "Epoch:155\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.612\n",
            "Epoch:155\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.739\n",
            "Epoch:155\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.752\n",
            "Epoch:155\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.747\n",
            "Epoch:155\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.586\n",
            "Epoch:155\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.674\n",
            "Epoch:155\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.544\n",
            "Epoch:155\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.759\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:156\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.680\n",
            "Epoch:156\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.664\n",
            "Epoch:156\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.677\n",
            "Epoch:156\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.624\n",
            "Epoch:156\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.451\n",
            "Epoch:156\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.595\n",
            "Epoch:156\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.489\n",
            "Epoch:156\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.795\n",
            "Epoch:156\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.476\n",
            "Epoch:156\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.390\n",
            "Epoch:156\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.567\n",
            "Epoch:156\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.735\n",
            "Epoch:156\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.609\n",
            "Epoch:156\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.586\n",
            "Epoch:156\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.630\n",
            "Epoch:156\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.507\n",
            "Epoch:156\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.695\n",
            "Epoch:156\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.672\n",
            "Epoch:156\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.565\n",
            "Epoch:156\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.602\n",
            "Epoch:156\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.554\n",
            "Epoch:156\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.775\n",
            "Epoch:156\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.647\n",
            "Epoch:156\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.469\n",
            "Epoch:156\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.640\n",
            "Epoch:156\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.624\n",
            "Epoch:156\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.526\n",
            "Epoch:156\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.689\n",
            "Epoch:156\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.642\n",
            "Epoch:156\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.700\n",
            "Epoch:156\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.728\n",
            "Epoch:156\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.595\n",
            "Epoch:156\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.509\n",
            "Epoch:156\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.705\n",
            "Epoch:156\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.526\n",
            "Epoch:156\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.599\n",
            "Epoch:156\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.579\n",
            "Epoch:156\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.639\n",
            "Epoch:156\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.521\n",
            "Epoch:156\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.648\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:157\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.829\n",
            "Epoch:157\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.543\n",
            "Epoch:157\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.476\n",
            "Epoch:157\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.573\n",
            "Epoch:157\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.792\n",
            "Epoch:157\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.480\n",
            "Epoch:157\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.692\n",
            "Epoch:157\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.401\n",
            "Epoch:157\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.753\n",
            "Epoch:157\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.535\n",
            "Epoch:157\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.494\n",
            "Epoch:157\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.539\n",
            "Epoch:157\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.707\n",
            "Epoch:157\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.709\n",
            "Epoch:157\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.562\n",
            "Epoch:157\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.512\n",
            "Epoch:157\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.396\n",
            "Epoch:157\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.696\n",
            "Epoch:157\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.637\n",
            "Epoch:157\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.486\n",
            "Epoch:157\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.549\n",
            "Epoch:157\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.790\n",
            "Epoch:157\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.594\n",
            "Epoch:157\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.580\n",
            "Epoch:157\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.714\n",
            "Epoch:157\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.555\n",
            "Epoch:157\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.625\n",
            "Epoch:157\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.706\n",
            "Epoch:157\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.471\n",
            "Epoch:157\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.702\n",
            "Epoch:157\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.568\n",
            "Epoch:157\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.740\n",
            "Epoch:157\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.559\n",
            "Epoch:157\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.657\n",
            "Epoch:157\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.551\n",
            "Epoch:157\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.714\n",
            "Epoch:157\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.634\n",
            "Epoch:157\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.637\n",
            "Epoch:157\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.753\n",
            "Epoch:157\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.584\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:158\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.670\n",
            "Epoch:158\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.747\n",
            "Epoch:158\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.610\n",
            "Epoch:158\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.929\n",
            "Epoch:158\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.534\n",
            "Epoch:158\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.878\n",
            "Epoch:158\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.545\n",
            "Epoch:158\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.522\n",
            "Epoch:158\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.685\n",
            "Epoch:158\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.553\n",
            "Epoch:158\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.507\n",
            "Epoch:158\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.483\n",
            "Epoch:158\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.723\n",
            "Epoch:158\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.640\n",
            "Epoch:158\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.566\n",
            "Epoch:158\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.590\n",
            "Epoch:158\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.583\n",
            "Epoch:158\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.669\n",
            "Epoch:158\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.755\n",
            "Epoch:158\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.680\n",
            "Epoch:158\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.816\n",
            "Epoch:158\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.566\n",
            "Epoch:158\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.562\n",
            "Epoch:158\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.605\n",
            "Epoch:158\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.590\n",
            "Epoch:158\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.696\n",
            "Epoch:158\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.505\n",
            "Epoch:158\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.559\n",
            "Epoch:158\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.602\n",
            "Epoch:158\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.653\n",
            "Epoch:158\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.659\n",
            "Epoch:158\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.630\n",
            "Epoch:158\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.639\n",
            "Epoch:158\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.742\n",
            "Epoch:158\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.625\n",
            "Epoch:158\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.632\n",
            "Epoch:158\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.655\n",
            "Epoch:158\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.488\n",
            "Epoch:158\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.698\n",
            "Epoch:158\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.586\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:159\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.631\n",
            "Epoch:159\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.496\n",
            "Epoch:159\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.578\n",
            "Epoch:159\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.475\n",
            "Epoch:159\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.586\n",
            "Epoch:159\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.480\n",
            "Epoch:159\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.622\n",
            "Epoch:159\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.611\n",
            "Epoch:159\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.504\n",
            "Epoch:159\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.567\n",
            "Epoch:159\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.624\n",
            "Epoch:159\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.578\n",
            "Epoch:159\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.716\n",
            "Epoch:159\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.726\n",
            "Epoch:159\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.536\n",
            "Epoch:159\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.712\n",
            "Epoch:159\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.590\n",
            "Epoch:159\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.520\n",
            "Epoch:159\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.607\n",
            "Epoch:159\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.739\n",
            "Epoch:159\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.714\n",
            "Epoch:159\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.571\n",
            "Epoch:159\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.551\n",
            "Epoch:159\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.601\n",
            "Epoch:159\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.505\n",
            "Epoch:159\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.557\n",
            "Epoch:159\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.772\n",
            "Epoch:159\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.623\n",
            "Epoch:159\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.533\n",
            "Epoch:159\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.604\n",
            "Epoch:159\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.627\n",
            "Epoch:159\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.465\n",
            "Epoch:159\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.602\n",
            "Epoch:159\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.510\n",
            "Epoch:159\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.527\n",
            "Epoch:159\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.614\n",
            "Epoch:159\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.629\n",
            "Epoch:159\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.554\n",
            "Epoch:159\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.871\n",
            "Epoch:159\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.912\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:160\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.555\n",
            "Epoch:160\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.623\n",
            "Epoch:160\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.522\n",
            "Epoch:160\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.527\n",
            "Epoch:160\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.613\n",
            "Epoch:160\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.526\n",
            "Epoch:160\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.606\n",
            "Epoch:160\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.599\n",
            "Epoch:160\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.526\n",
            "Epoch:160\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.521\n",
            "Epoch:160\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.767\n",
            "Epoch:160\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.652\n",
            "Epoch:160\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.551\n",
            "Epoch:160\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.587\n",
            "Epoch:160\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.719\n",
            "Epoch:160\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.468\n",
            "Epoch:160\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.568\n",
            "Epoch:160\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.579\n",
            "Epoch:160\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.604\n",
            "Epoch:160\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.426\n",
            "Epoch:160\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.548\n",
            "Epoch:160\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.681\n",
            "Epoch:160\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.584\n",
            "Epoch:160\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.577\n",
            "Epoch:160\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.501\n",
            "Epoch:160\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.547\n",
            "Epoch:160\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.554\n",
            "Epoch:160\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.588\n",
            "Epoch:160\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.515\n",
            "Epoch:160\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.674\n",
            "Epoch:160\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.638\n",
            "Epoch:160\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.638\n",
            "Epoch:160\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.710\n",
            "Epoch:160\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.636\n",
            "Epoch:160\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.691\n",
            "Epoch:160\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.620\n",
            "Epoch:160\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.607\n",
            "Epoch:160\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.531\n",
            "Epoch:160\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.634\n",
            "Epoch:160\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.474\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:161\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.523\n",
            "Epoch:161\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.611\n",
            "Epoch:161\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.436\n",
            "Epoch:161\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.624\n",
            "Epoch:161\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.466\n",
            "Epoch:161\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.480\n",
            "Epoch:161\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.578\n",
            "Epoch:161\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.470\n",
            "Epoch:161\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.545\n",
            "Epoch:161\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.537\n",
            "Epoch:161\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.517\n",
            "Epoch:161\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.487\n",
            "Epoch:161\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.430\n",
            "Epoch:161\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.460\n",
            "Epoch:161\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.427\n",
            "Epoch:161\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.467\n",
            "Epoch:161\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.428\n",
            "Epoch:161\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.488\n",
            "Epoch:161\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.404\n",
            "Epoch:161\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.543\n",
            "Epoch:161\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.474\n",
            "Epoch:161\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.450\n",
            "Epoch:161\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.455\n",
            "Epoch:161\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.463\n",
            "Epoch:161\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.555\n",
            "Epoch:161\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.431\n",
            "Epoch:161\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.406\n",
            "Epoch:161\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.564\n",
            "Epoch:161\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.520\n",
            "Epoch:161\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.440\n",
            "Epoch:161\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.445\n",
            "Epoch:161\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.497\n",
            "Epoch:161\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.528\n",
            "Epoch:161\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.473\n",
            "Epoch:161\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.632\n",
            "Epoch:161\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.436\n",
            "Epoch:161\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.454\n",
            "Epoch:161\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.541\n",
            "Epoch:161\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.739\n",
            "Epoch:161\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.637\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:162\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.609\n",
            "Epoch:162\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.462\n",
            "Epoch:162\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.367\n",
            "Epoch:162\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.355\n",
            "Epoch:162\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.466\n",
            "Epoch:162\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.339\n",
            "Epoch:162\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.373\n",
            "Epoch:162\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.423\n",
            "Epoch:162\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.337\n",
            "Epoch:162\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.484\n",
            "Epoch:162\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.364\n",
            "Epoch:162\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.516\n",
            "Epoch:162\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.547\n",
            "Epoch:162\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.648\n",
            "Epoch:162\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.387\n",
            "Epoch:162\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.552\n",
            "Epoch:162\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.415\n",
            "Epoch:162\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.507\n",
            "Epoch:162\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.408\n",
            "Epoch:162\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.493\n",
            "Epoch:162\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.400\n",
            "Epoch:162\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.440\n",
            "Epoch:162\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.575\n",
            "Epoch:162\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.322\n",
            "Epoch:162\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.590\n",
            "Epoch:162\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.478\n",
            "Epoch:162\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.401\n",
            "Epoch:162\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.407\n",
            "Epoch:162\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.501\n",
            "Epoch:162\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.416\n",
            "Epoch:162\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.386\n",
            "Epoch:162\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.424\n",
            "Epoch:162\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.331\n",
            "Epoch:162\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.454\n",
            "Epoch:162\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.426\n",
            "Epoch:162\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.405\n",
            "Epoch:162\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.442\n",
            "Epoch:162\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.563\n",
            "Epoch:162\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.421\n",
            "Epoch:162\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.504\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:163\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.529\n",
            "Epoch:163\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.476\n",
            "Epoch:163\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.424\n",
            "Epoch:163\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.332\n",
            "Epoch:163\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.408\n",
            "Epoch:163\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.413\n",
            "Epoch:163\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.350\n",
            "Epoch:163\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.464\n",
            "Epoch:163\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.506\n",
            "Epoch:163\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.453\n",
            "Epoch:163\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.461\n",
            "Epoch:163\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.458\n",
            "Epoch:163\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.315\n",
            "Epoch:163\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.330\n",
            "Epoch:163\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.463\n",
            "Epoch:163\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.419\n",
            "Epoch:163\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.396\n",
            "Epoch:163\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.375\n",
            "Epoch:163\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.394\n",
            "Epoch:163\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.341\n",
            "Epoch:163\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.371\n",
            "Epoch:163\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.438\n",
            "Epoch:163\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.326\n",
            "Epoch:163\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.525\n",
            "Epoch:163\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.454\n",
            "Epoch:163\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.432\n",
            "Epoch:163\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.465\n",
            "Epoch:163\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.444\n",
            "Epoch:163\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.457\n",
            "Epoch:163\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.441\n",
            "Epoch:163\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.505\n",
            "Epoch:163\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.469\n",
            "Epoch:163\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.491\n",
            "Epoch:163\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.427\n",
            "Epoch:163\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.355\n",
            "Epoch:163\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.395\n",
            "Epoch:163\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.379\n",
            "Epoch:163\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.405\n",
            "Epoch:163\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.464\n",
            "Epoch:163\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.415\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:164\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.305\n",
            "Epoch:164\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.408\n",
            "Epoch:164\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.463\n",
            "Epoch:164\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.330\n",
            "Epoch:164\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.339\n",
            "Epoch:164\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.413\n",
            "Epoch:164\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.521\n",
            "Epoch:164\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.421\n",
            "Epoch:164\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.578\n",
            "Epoch:164\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.444\n",
            "Epoch:164\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.551\n",
            "Epoch:164\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.528\n",
            "Epoch:164\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.295\n",
            "Epoch:164\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.526\n",
            "Epoch:164\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.420\n",
            "Epoch:164\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.475\n",
            "Epoch:164\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.375\n",
            "Epoch:164\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.412\n",
            "Epoch:164\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.443\n",
            "Epoch:164\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.366\n",
            "Epoch:164\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.396\n",
            "Epoch:164\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.392\n",
            "Epoch:164\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.497\n",
            "Epoch:164\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.473\n",
            "Epoch:164\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.458\n",
            "Epoch:164\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.406\n",
            "Epoch:164\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.399\n",
            "Epoch:164\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.430\n",
            "Epoch:164\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.469\n",
            "Epoch:164\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.374\n",
            "Epoch:164\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.304\n",
            "Epoch:164\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.407\n",
            "Epoch:164\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.486\n",
            "Epoch:164\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.435\n",
            "Epoch:164\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.524\n",
            "Epoch:164\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.403\n",
            "Epoch:164\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.434\n",
            "Epoch:164\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.451\n",
            "Epoch:164\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.324\n",
            "Epoch:164\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.394\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:165\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.419\n",
            "Epoch:165\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.425\n",
            "Epoch:165\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.421\n",
            "Epoch:165\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.482\n",
            "Epoch:165\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.398\n",
            "Epoch:165\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.387\n",
            "Epoch:165\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.430\n",
            "Epoch:165\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.382\n",
            "Epoch:165\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.394\n",
            "Epoch:165\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.340\n",
            "Epoch:165\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.430\n",
            "Epoch:165\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.400\n",
            "Epoch:165\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.399\n",
            "Epoch:165\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.352\n",
            "Epoch:165\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.463\n",
            "Epoch:165\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.444\n",
            "Epoch:165\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.419\n",
            "Epoch:165\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.486\n",
            "Epoch:165\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.414\n",
            "Epoch:165\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.387\n",
            "Epoch:165\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.351\n",
            "Epoch:165\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.456\n",
            "Epoch:165\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.414\n",
            "Epoch:165\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.399\n",
            "Epoch:165\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.475\n",
            "Epoch:165\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.496\n",
            "Epoch:165\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.436\n",
            "Epoch:165\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.386\n",
            "Epoch:165\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.358\n",
            "Epoch:165\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.440\n",
            "Epoch:165\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.370\n",
            "Epoch:165\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.370\n",
            "Epoch:165\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.465\n",
            "Epoch:165\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.444\n",
            "Epoch:165\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.431\n",
            "Epoch:165\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.547\n",
            "Epoch:165\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.457\n",
            "Epoch:165\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.531\n",
            "Epoch:165\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.465\n",
            "Epoch:165\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.389\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:166\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.435\n",
            "Epoch:166\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.402\n",
            "Epoch:166\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.320\n",
            "Epoch:166\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.482\n",
            "Epoch:166\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.393\n",
            "Epoch:166\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.395\n",
            "Epoch:166\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.515\n",
            "Epoch:166\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.503\n",
            "Epoch:166\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.457\n",
            "Epoch:166\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.333\n",
            "Epoch:166\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.540\n",
            "Epoch:166\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.368\n",
            "Epoch:166\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.418\n",
            "Epoch:166\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.434\n",
            "Epoch:166\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.436\n",
            "Epoch:166\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.554\n",
            "Epoch:166\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.440\n",
            "Epoch:166\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.360\n",
            "Epoch:166\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.612\n",
            "Epoch:166\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.510\n",
            "Epoch:166\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.369\n",
            "Epoch:166\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.285\n",
            "Epoch:166\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.466\n",
            "Epoch:166\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.486\n",
            "Epoch:166\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.490\n",
            "Epoch:166\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.456\n",
            "Epoch:166\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.432\n",
            "Epoch:166\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.412\n",
            "Epoch:166\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.323\n",
            "Epoch:166\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.501\n",
            "Epoch:166\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.382\n",
            "Epoch:166\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.512\n",
            "Epoch:166\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.452\n",
            "Epoch:166\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.504\n",
            "Epoch:166\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.431\n",
            "Epoch:166\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.372\n",
            "Epoch:166\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.555\n",
            "Epoch:166\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.310\n",
            "Epoch:166\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.408\n",
            "Epoch:166\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.433\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:167\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.459\n",
            "Epoch:167\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.381\n",
            "Epoch:167\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.450\n",
            "Epoch:167\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.386\n",
            "Epoch:167\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.415\n",
            "Epoch:167\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.520\n",
            "Epoch:167\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.380\n",
            "Epoch:167\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.502\n",
            "Epoch:167\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.417\n",
            "Epoch:167\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.271\n",
            "Epoch:167\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.332\n",
            "Epoch:167\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.472\n",
            "Epoch:167\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.356\n",
            "Epoch:167\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.419\n",
            "Epoch:167\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.344\n",
            "Epoch:167\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.513\n",
            "Epoch:167\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.433\n",
            "Epoch:167\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.311\n",
            "Epoch:167\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.357\n",
            "Epoch:167\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.523\n",
            "Epoch:167\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.320\n",
            "Epoch:167\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.410\n",
            "Epoch:167\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.569\n",
            "Epoch:167\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.415\n",
            "Epoch:167\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.441\n",
            "Epoch:167\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.352\n",
            "Epoch:167\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.413\n",
            "Epoch:167\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.475\n",
            "Epoch:167\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.319\n",
            "Epoch:167\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.400\n",
            "Epoch:167\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.378\n",
            "Epoch:167\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.581\n",
            "Epoch:167\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.402\n",
            "Epoch:167\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.327\n",
            "Epoch:167\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.387\n",
            "Epoch:167\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.415\n",
            "Epoch:167\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.430\n",
            "Epoch:167\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.344\n",
            "Epoch:167\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.335\n",
            "Epoch:167\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.530\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:168\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.411\n",
            "Epoch:168\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.424\n",
            "Epoch:168\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.389\n",
            "Epoch:168\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.373\n",
            "Epoch:168\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.333\n",
            "Epoch:168\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.405\n",
            "Epoch:168\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.316\n",
            "Epoch:168\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.316\n",
            "Epoch:168\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.450\n",
            "Epoch:168\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.377\n",
            "Epoch:168\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.390\n",
            "Epoch:168\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.360\n",
            "Epoch:168\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.456\n",
            "Epoch:168\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.464\n",
            "Epoch:168\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.552\n",
            "Epoch:168\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.355\n",
            "Epoch:168\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.418\n",
            "Epoch:168\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.296\n",
            "Epoch:168\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.445\n",
            "Epoch:168\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.314\n",
            "Epoch:168\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.386\n",
            "Epoch:168\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.520\n",
            "Epoch:168\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.346\n",
            "Epoch:168\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.299\n",
            "Epoch:168\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.493\n",
            "Epoch:168\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.383\n",
            "Epoch:168\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.491\n",
            "Epoch:168\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.396\n",
            "Epoch:168\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.412\n",
            "Epoch:168\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.465\n",
            "Epoch:168\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.420\n",
            "Epoch:168\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.479\n",
            "Epoch:168\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.327\n",
            "Epoch:168\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.436\n",
            "Epoch:168\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.309\n",
            "Epoch:168\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.328\n",
            "Epoch:168\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.477\n",
            "Epoch:168\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.445\n",
            "Epoch:168\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.448\n",
            "Epoch:168\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.444\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:169\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.398\n",
            "Epoch:169\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.489\n",
            "Epoch:169\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.376\n",
            "Epoch:169\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.424\n",
            "Epoch:169\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.362\n",
            "Epoch:169\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.479\n",
            "Epoch:169\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.410\n",
            "Epoch:169\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.451\n",
            "Epoch:169\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.393\n",
            "Epoch:169\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.532\n",
            "Epoch:169\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.336\n",
            "Epoch:169\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.450\n",
            "Epoch:169\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.414\n",
            "Epoch:169\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.430\n",
            "Epoch:169\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.379\n",
            "Epoch:169\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.401\n",
            "Epoch:169\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.342\n",
            "Epoch:169\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.356\n",
            "Epoch:169\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.315\n",
            "Epoch:169\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.381\n",
            "Epoch:169\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.273\n",
            "Epoch:169\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.362\n",
            "Epoch:169\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.439\n",
            "Epoch:169\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.306\n",
            "Epoch:169\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.325\n",
            "Epoch:169\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.375\n",
            "Epoch:169\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.409\n",
            "Epoch:169\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.459\n",
            "Epoch:169\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.509\n",
            "Epoch:169\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.360\n",
            "Epoch:169\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.622\n",
            "Epoch:169\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.369\n",
            "Epoch:169\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.505\n",
            "Epoch:169\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.459\n",
            "Epoch:169\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.491\n",
            "Epoch:169\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.385\n",
            "Epoch:169\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.477\n",
            "Epoch:169\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.356\n",
            "Epoch:169\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.378\n",
            "Epoch:169\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.286\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:170\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.412\n",
            "Epoch:170\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.318\n",
            "Epoch:170\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.290\n",
            "Epoch:170\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.272\n",
            "Epoch:170\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.456\n",
            "Epoch:170\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.457\n",
            "Epoch:170\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.367\n",
            "Epoch:170\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.424\n",
            "Epoch:170\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.394\n",
            "Epoch:170\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.421\n",
            "Epoch:170\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.477\n",
            "Epoch:170\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.399\n",
            "Epoch:170\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.488\n",
            "Epoch:170\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.408\n",
            "Epoch:170\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.479\n",
            "Epoch:170\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.572\n",
            "Epoch:170\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.380\n",
            "Epoch:170\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.279\n",
            "Epoch:170\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.457\n",
            "Epoch:170\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.328\n",
            "Epoch:170\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.347\n",
            "Epoch:170\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.422\n",
            "Epoch:170\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.486\n",
            "Epoch:170\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.436\n",
            "Epoch:170\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.282\n",
            "Epoch:170\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.282\n",
            "Epoch:170\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.348\n",
            "Epoch:170\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.374\n",
            "Epoch:170\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.397\n",
            "Epoch:170\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.574\n",
            "Epoch:170\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.474\n",
            "Epoch:170\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.323\n",
            "Epoch:170\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.488\n",
            "Epoch:170\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.370\n",
            "Epoch:170\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.538\n",
            "Epoch:170\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.398\n",
            "Epoch:170\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.359\n",
            "Epoch:170\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.398\n",
            "Epoch:170\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.371\n",
            "Epoch:170\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.537\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:171\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.335\n",
            "Epoch:171\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.354\n",
            "Epoch:171\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.290\n",
            "Epoch:171\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.375\n",
            "Epoch:171\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.415\n",
            "Epoch:171\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.427\n",
            "Epoch:171\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.349\n",
            "Epoch:171\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.283\n",
            "Epoch:171\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.438\n",
            "Epoch:171\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.436\n",
            "Epoch:171\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.327\n",
            "Epoch:171\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.371\n",
            "Epoch:171\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.355\n",
            "Epoch:171\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.331\n",
            "Epoch:171\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.501\n",
            "Epoch:171\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.445\n",
            "Epoch:171\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.549\n",
            "Epoch:171\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.477\n",
            "Epoch:171\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.475\n",
            "Epoch:171\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.344\n",
            "Epoch:171\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.329\n",
            "Epoch:171\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.457\n",
            "Epoch:171\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.504\n",
            "Epoch:171\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.376\n",
            "Epoch:171\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.367\n",
            "Epoch:171\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.323\n",
            "Epoch:171\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.454\n",
            "Epoch:171\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.424\n",
            "Epoch:171\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.386\n",
            "Epoch:171\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.382\n",
            "Epoch:171\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.401\n",
            "Epoch:171\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.589\n",
            "Epoch:171\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.393\n",
            "Epoch:171\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.396\n",
            "Epoch:171\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.286\n",
            "Epoch:171\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.419\n",
            "Epoch:171\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.453\n",
            "Epoch:171\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.587\n",
            "Epoch:171\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.434\n",
            "Epoch:171\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.494\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:172\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.473\n",
            "Epoch:172\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.415\n",
            "Epoch:172\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.386\n",
            "Epoch:172\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.367\n",
            "Epoch:172\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.307\n",
            "Epoch:172\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.364\n",
            "Epoch:172\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.316\n",
            "Epoch:172\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.467\n",
            "Epoch:172\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.445\n",
            "Epoch:172\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.389\n",
            "Epoch:172\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.304\n",
            "Epoch:172\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.348\n",
            "Epoch:172\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.480\n",
            "Epoch:172\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.515\n",
            "Epoch:172\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.376\n",
            "Epoch:172\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.345\n",
            "Epoch:172\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.383\n",
            "Epoch:172\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.533\n",
            "Epoch:172\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.456\n",
            "Epoch:172\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.355\n",
            "Epoch:172\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.396\n",
            "Epoch:172\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.331\n",
            "Epoch:172\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.463\n",
            "Epoch:172\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.341\n",
            "Epoch:172\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.456\n",
            "Epoch:172\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.444\n",
            "Epoch:172\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.262\n",
            "Epoch:172\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.365\n",
            "Epoch:172\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.514\n",
            "Epoch:172\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.387\n",
            "Epoch:172\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.338\n",
            "Epoch:172\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.503\n",
            "Epoch:172\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.342\n",
            "Epoch:172\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.348\n",
            "Epoch:172\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.386\n",
            "Epoch:172\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.383\n",
            "Epoch:172\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.335\n",
            "Epoch:172\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.280\n",
            "Epoch:172\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.360\n",
            "Epoch:172\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.494\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:173\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.464\n",
            "Epoch:173\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.544\n",
            "Epoch:173\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.371\n",
            "Epoch:173\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.350\n",
            "Epoch:173\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.452\n",
            "Epoch:173\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.447\n",
            "Epoch:173\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.433\n",
            "Epoch:173\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.273\n",
            "Epoch:173\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.402\n",
            "Epoch:173\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.295\n",
            "Epoch:173\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.373\n",
            "Epoch:173\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.316\n",
            "Epoch:173\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.370\n",
            "Epoch:173\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.417\n",
            "Epoch:173\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.471\n",
            "Epoch:173\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.337\n",
            "Epoch:173\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.306\n",
            "Epoch:173\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.533\n",
            "Epoch:173\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.314\n",
            "Epoch:173\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.377\n",
            "Epoch:173\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.391\n",
            "Epoch:173\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.375\n",
            "Epoch:173\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.362\n",
            "Epoch:173\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.499\n",
            "Epoch:173\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.469\n",
            "Epoch:173\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.473\n",
            "Epoch:173\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.363\n",
            "Epoch:173\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.457\n",
            "Epoch:173\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.331\n",
            "Epoch:173\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.381\n",
            "Epoch:173\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.382\n",
            "Epoch:173\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.399\n",
            "Epoch:173\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.428\n",
            "Epoch:173\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.492\n",
            "Epoch:173\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.384\n",
            "Epoch:173\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.475\n",
            "Epoch:173\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.291\n",
            "Epoch:173\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.382\n",
            "Epoch:173\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.399\n",
            "Epoch:173\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.391\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:174\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.296\n",
            "Epoch:174\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.450\n",
            "Epoch:174\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.335\n",
            "Epoch:174\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.375\n",
            "Epoch:174\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.319\n",
            "Epoch:174\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.431\n",
            "Epoch:174\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.479\n",
            "Epoch:174\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.302\n",
            "Epoch:174\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.311\n",
            "Epoch:174\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.313\n",
            "Epoch:174\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.395\n",
            "Epoch:174\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.456\n",
            "Epoch:174\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.390\n",
            "Epoch:174\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.451\n",
            "Epoch:174\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.351\n",
            "Epoch:174\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.387\n",
            "Epoch:174\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.503\n",
            "Epoch:174\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.380\n",
            "Epoch:174\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.382\n",
            "Epoch:174\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.249\n",
            "Epoch:174\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.319\n",
            "Epoch:174\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.375\n",
            "Epoch:174\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.411\n",
            "Epoch:174\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.403\n",
            "Epoch:174\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.363\n",
            "Epoch:174\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.268\n",
            "Epoch:174\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.412\n",
            "Epoch:174\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.406\n",
            "Epoch:174\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.389\n",
            "Epoch:174\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.275\n",
            "Epoch:174\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.395\n",
            "Epoch:174\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.346\n",
            "Epoch:174\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.417\n",
            "Epoch:174\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.420\n",
            "Epoch:174\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.585\n",
            "Epoch:174\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.354\n",
            "Epoch:174\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.440\n",
            "Epoch:174\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.429\n",
            "Epoch:174\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.372\n",
            "Epoch:174\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.541\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:175\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.324\n",
            "Epoch:175\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.377\n",
            "Epoch:175\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.542\n",
            "Epoch:175\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.356\n",
            "Epoch:175\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.376\n",
            "Epoch:175\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.311\n",
            "Epoch:175\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.501\n",
            "Epoch:175\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.409\n",
            "Epoch:175\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.369\n",
            "Epoch:175\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.415\n",
            "Epoch:175\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.373\n",
            "Epoch:175\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.390\n",
            "Epoch:175\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.429\n",
            "Epoch:175\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.397\n",
            "Epoch:175\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.342\n",
            "Epoch:175\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.478\n",
            "Epoch:175\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.406\n",
            "Epoch:175\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.397\n",
            "Epoch:175\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.428\n",
            "Epoch:175\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.423\n",
            "Epoch:175\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.438\n",
            "Epoch:175\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.390\n",
            "Epoch:175\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.397\n",
            "Epoch:175\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.319\n",
            "Epoch:175\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.397\n",
            "Epoch:175\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.388\n",
            "Epoch:175\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.322\n",
            "Epoch:175\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.382\n",
            "Epoch:175\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.298\n",
            "Epoch:175\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.386\n",
            "Epoch:175\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.295\n",
            "Epoch:175\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.341\n",
            "Epoch:175\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.507\n",
            "Epoch:175\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.502\n",
            "Epoch:175\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.349\n",
            "Epoch:175\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.381\n",
            "Epoch:175\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.346\n",
            "Epoch:175\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.380\n",
            "Epoch:175\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.453\n",
            "Epoch:175\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.379\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:176\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.361\n",
            "Epoch:176\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.358\n",
            "Epoch:176\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.400\n",
            "Epoch:176\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.418\n",
            "Epoch:176\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.318\n",
            "Epoch:176\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.236\n",
            "Epoch:176\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.412\n",
            "Epoch:176\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.323\n",
            "Epoch:176\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.540\n",
            "Epoch:176\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.419\n",
            "Epoch:176\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.518\n",
            "Epoch:176\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.403\n",
            "Epoch:176\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.350\n",
            "Epoch:176\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.283\n",
            "Epoch:176\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.366\n",
            "Epoch:176\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.483\n",
            "Epoch:176\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.325\n",
            "Epoch:176\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.330\n",
            "Epoch:176\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.361\n",
            "Epoch:176\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.376\n",
            "Epoch:176\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.418\n",
            "Epoch:176\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.397\n",
            "Epoch:176\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.328\n",
            "Epoch:176\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.403\n",
            "Epoch:176\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.378\n",
            "Epoch:176\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.671\n",
            "Epoch:176\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.399\n",
            "Epoch:176\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.410\n",
            "Epoch:176\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.364\n",
            "Epoch:176\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.337\n",
            "Epoch:176\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.354\n",
            "Epoch:176\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.390\n",
            "Epoch:176\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.395\n",
            "Epoch:176\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.505\n",
            "Epoch:176\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.415\n",
            "Epoch:176\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.456\n",
            "Epoch:176\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.309\n",
            "Epoch:176\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.455\n",
            "Epoch:176\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.390\n",
            "Epoch:176\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.430\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:177\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.324\n",
            "Epoch:177\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.352\n",
            "Epoch:177\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.377\n",
            "Epoch:177\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.411\n",
            "Epoch:177\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.306\n",
            "Epoch:177\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.393\n",
            "Epoch:177\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.312\n",
            "Epoch:177\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.374\n",
            "Epoch:177\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.448\n",
            "Epoch:177\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.358\n",
            "Epoch:177\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.350\n",
            "Epoch:177\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.338\n",
            "Epoch:177\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.413\n",
            "Epoch:177\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.411\n",
            "Epoch:177\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.350\n",
            "Epoch:177\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.379\n",
            "Epoch:177\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.276\n",
            "Epoch:177\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.404\n",
            "Epoch:177\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.371\n",
            "Epoch:177\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.377\n",
            "Epoch:177\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.454\n",
            "Epoch:177\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.514\n",
            "Epoch:177\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.472\n",
            "Epoch:177\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.471\n",
            "Epoch:177\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.347\n",
            "Epoch:177\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.381\n",
            "Epoch:177\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.306\n",
            "Epoch:177\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.353\n",
            "Epoch:177\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.321\n",
            "Epoch:177\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.368\n",
            "Epoch:177\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.385\n",
            "Epoch:177\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.304\n",
            "Epoch:177\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.349\n",
            "Epoch:177\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.287\n",
            "Epoch:177\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.360\n",
            "Epoch:177\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.438\n",
            "Epoch:177\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.335\n",
            "Epoch:177\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.416\n",
            "Epoch:177\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.308\n",
            "Epoch:177\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.769\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:178\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.295\n",
            "Epoch:178\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.311\n",
            "Epoch:178\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.386\n",
            "Epoch:178\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.347\n",
            "Epoch:178\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.427\n",
            "Epoch:178\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.349\n",
            "Epoch:178\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.279\n",
            "Epoch:178\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.376\n",
            "Epoch:178\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.493\n",
            "Epoch:178\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.384\n",
            "Epoch:178\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.365\n",
            "Epoch:178\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.349\n",
            "Epoch:178\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.357\n",
            "Epoch:178\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.471\n",
            "Epoch:178\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.391\n",
            "Epoch:178\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.273\n",
            "Epoch:178\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.495\n",
            "Epoch:178\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.376\n",
            "Epoch:178\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.399\n",
            "Epoch:178\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.326\n",
            "Epoch:178\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.439\n",
            "Epoch:178\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.338\n",
            "Epoch:178\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.551\n",
            "Epoch:178\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.303\n",
            "Epoch:178\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.257\n",
            "Epoch:178\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.371\n",
            "Epoch:178\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.403\n",
            "Epoch:178\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.321\n",
            "Epoch:178\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.363\n",
            "Epoch:178\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.498\n",
            "Epoch:178\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.358\n",
            "Epoch:178\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.316\n",
            "Epoch:178\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.365\n",
            "Epoch:178\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.354\n",
            "Epoch:178\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.410\n",
            "Epoch:178\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.381\n",
            "Epoch:178\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.376\n",
            "Epoch:178\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.326\n",
            "Epoch:178\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.380\n",
            "Epoch:178\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.440\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:179\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.337\n",
            "Epoch:179\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.230\n",
            "Epoch:179\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.379\n",
            "Epoch:179\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.434\n",
            "Epoch:179\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.308\n",
            "Epoch:179\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.335\n",
            "Epoch:179\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.368\n",
            "Epoch:179\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.417\n",
            "Epoch:179\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.438\n",
            "Epoch:179\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.283\n",
            "Epoch:179\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.419\n",
            "Epoch:179\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.363\n",
            "Epoch:179\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.287\n",
            "Epoch:179\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.445\n",
            "Epoch:179\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.436\n",
            "Epoch:179\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.311\n",
            "Epoch:179\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.442\n",
            "Epoch:179\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.377\n",
            "Epoch:179\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.448\n",
            "Epoch:179\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.230\n",
            "Epoch:179\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.394\n",
            "Epoch:179\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.427\n",
            "Epoch:179\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.412\n",
            "Epoch:179\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.364\n",
            "Epoch:179\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.498\n",
            "Epoch:179\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.530\n",
            "Epoch:179\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.415\n",
            "Epoch:179\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.372\n",
            "Epoch:179\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.279\n",
            "Epoch:179\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.371\n",
            "Epoch:179\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.393\n",
            "Epoch:179\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.406\n",
            "Epoch:179\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.406\n",
            "Epoch:179\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.305\n",
            "Epoch:179\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.275\n",
            "Epoch:179\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.377\n",
            "Epoch:179\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.413\n",
            "Epoch:179\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.368\n",
            "Epoch:179\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.411\n",
            "Epoch:179\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.315\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:180\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.332\n",
            "Epoch:180\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.424\n",
            "Epoch:180\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.406\n",
            "Epoch:180\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.286\n",
            "Epoch:180\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.360\n",
            "Epoch:180\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.358\n",
            "Epoch:180\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.359\n",
            "Epoch:180\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.324\n",
            "Epoch:180\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.457\n",
            "Epoch:180\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.289\n",
            "Epoch:180\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.449\n",
            "Epoch:180\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.419\n",
            "Epoch:180\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.369\n",
            "Epoch:180\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.447\n",
            "Epoch:180\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.375\n",
            "Epoch:180\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.422\n",
            "Epoch:180\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.519\n",
            "Epoch:180\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.478\n",
            "Epoch:180\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.259\n",
            "Epoch:180\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.330\n",
            "Epoch:180\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.376\n",
            "Epoch:180\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.356\n",
            "Epoch:180\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.408\n",
            "Epoch:180\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.343\n",
            "Epoch:180\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.386\n",
            "Epoch:180\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.418\n",
            "Epoch:180\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.291\n",
            "Epoch:180\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.349\n",
            "Epoch:180\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.480\n",
            "Epoch:180\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.365\n",
            "Epoch:180\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.359\n",
            "Epoch:180\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.378\n",
            "Epoch:180\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.441\n",
            "Epoch:180\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.415\n",
            "Epoch:180\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.386\n",
            "Epoch:180\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.292\n",
            "Epoch:180\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.442\n",
            "Epoch:180\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.453\n",
            "Epoch:180\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.303\n",
            "Epoch:180\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.576\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:181\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.410\n",
            "Epoch:181\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.380\n",
            "Epoch:181\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.387\n",
            "Epoch:181\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.326\n",
            "Epoch:181\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.295\n",
            "Epoch:181\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.353\n",
            "Epoch:181\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.533\n",
            "Epoch:181\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.467\n",
            "Epoch:181\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.313\n",
            "Epoch:181\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.444\n",
            "Epoch:181\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.371\n",
            "Epoch:181\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.264\n",
            "Epoch:181\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.475\n",
            "Epoch:181\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.379\n",
            "Epoch:181\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.461\n",
            "Epoch:181\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.341\n",
            "Epoch:181\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.444\n",
            "Epoch:181\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.413\n",
            "Epoch:181\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.338\n",
            "Epoch:181\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.475\n",
            "Epoch:181\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.336\n",
            "Epoch:181\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.349\n",
            "Epoch:181\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.385\n",
            "Epoch:181\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.295\n",
            "Epoch:181\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.470\n",
            "Epoch:181\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.371\n",
            "Epoch:181\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.332\n",
            "Epoch:181\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.466\n",
            "Epoch:181\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.389\n",
            "Epoch:181\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.521\n",
            "Epoch:181\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.296\n",
            "Epoch:181\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.368\n",
            "Epoch:181\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.390\n",
            "Epoch:181\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.546\n",
            "Epoch:181\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.267\n",
            "Epoch:181\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.415\n",
            "Epoch:181\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.387\n",
            "Epoch:181\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.226\n",
            "Epoch:181\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.339\n",
            "Epoch:181\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.467\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:182\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.281\n",
            "Epoch:182\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.377\n",
            "Epoch:182\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.303\n",
            "Epoch:182\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.278\n",
            "Epoch:182\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.420\n",
            "Epoch:182\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.319\n",
            "Epoch:182\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.405\n",
            "Epoch:182\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.447\n",
            "Epoch:182\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.290\n",
            "Epoch:182\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.372\n",
            "Epoch:182\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.508\n",
            "Epoch:182\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.334\n",
            "Epoch:182\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.331\n",
            "Epoch:182\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.309\n",
            "Epoch:182\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.347\n",
            "Epoch:182\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.416\n",
            "Epoch:182\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.420\n",
            "Epoch:182\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.479\n",
            "Epoch:182\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.473\n",
            "Epoch:182\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.254\n",
            "Epoch:182\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.337\n",
            "Epoch:182\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.273\n",
            "Epoch:182\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.533\n",
            "Epoch:182\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.496\n",
            "Epoch:182\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.389\n",
            "Epoch:182\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.285\n",
            "Epoch:182\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.467\n",
            "Epoch:182\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.331\n",
            "Epoch:182\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.358\n",
            "Epoch:182\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.419\n",
            "Epoch:182\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.453\n",
            "Epoch:182\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.332\n",
            "Epoch:182\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.414\n",
            "Epoch:182\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.304\n",
            "Epoch:182\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.333\n",
            "Epoch:182\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.343\n",
            "Epoch:182\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.380\n",
            "Epoch:182\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.368\n",
            "Epoch:182\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.431\n",
            "Epoch:182\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.547\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:183\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.373\n",
            "Epoch:183\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.490\n",
            "Epoch:183\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.403\n",
            "Epoch:183\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.431\n",
            "Epoch:183\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.468\n",
            "Epoch:183\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.275\n",
            "Epoch:183\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.587\n",
            "Epoch:183\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.411\n",
            "Epoch:183\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.326\n",
            "Epoch:183\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.327\n",
            "Epoch:183\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.513\n",
            "Epoch:183\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.357\n",
            "Epoch:183\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.271\n",
            "Epoch:183\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.397\n",
            "Epoch:183\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.343\n",
            "Epoch:183\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.309\n",
            "Epoch:183\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.410\n",
            "Epoch:183\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.440\n",
            "Epoch:183\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.298\n",
            "Epoch:183\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.312\n",
            "Epoch:183\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.278\n",
            "Epoch:183\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.424\n",
            "Epoch:183\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.404\n",
            "Epoch:183\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.503\n",
            "Epoch:183\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.359\n",
            "Epoch:183\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.327\n",
            "Epoch:183\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.330\n",
            "Epoch:183\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.249\n",
            "Epoch:183\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.434\n",
            "Epoch:183\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.296\n",
            "Epoch:183\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.422\n",
            "Epoch:183\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.430\n",
            "Epoch:183\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.356\n",
            "Epoch:183\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.446\n",
            "Epoch:183\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.397\n",
            "Epoch:183\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.375\n",
            "Epoch:183\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.305\n",
            "Epoch:183\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.438\n",
            "Epoch:183\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.362\n",
            "Epoch:183\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.348\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:184\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.333\n",
            "Epoch:184\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.251\n",
            "Epoch:184\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.332\n",
            "Epoch:184\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.357\n",
            "Epoch:184\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.382\n",
            "Epoch:184\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.364\n",
            "Epoch:184\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.309\n",
            "Epoch:184\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.313\n",
            "Epoch:184\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.305\n",
            "Epoch:184\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.499\n",
            "Epoch:184\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.366\n",
            "Epoch:184\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.316\n",
            "Epoch:184\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.380\n",
            "Epoch:184\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.465\n",
            "Epoch:184\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.419\n",
            "Epoch:184\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.307\n",
            "Epoch:184\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.454\n",
            "Epoch:184\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.466\n",
            "Epoch:184\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.395\n",
            "Epoch:184\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.360\n",
            "Epoch:184\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.304\n",
            "Epoch:184\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.264\n",
            "Epoch:184\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.301\n",
            "Epoch:184\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.363\n",
            "Epoch:184\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.336\n",
            "Epoch:184\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.471\n",
            "Epoch:184\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.334\n",
            "Epoch:184\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.416\n",
            "Epoch:184\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.431\n",
            "Epoch:184\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.384\n",
            "Epoch:184\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.392\n",
            "Epoch:184\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.381\n",
            "Epoch:184\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.469\n",
            "Epoch:184\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.369\n",
            "Epoch:184\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.399\n",
            "Epoch:184\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.291\n",
            "Epoch:184\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.281\n",
            "Epoch:184\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.316\n",
            "Epoch:184\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.398\n",
            "Epoch:184\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.436\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:185\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.384\n",
            "Epoch:185\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.339\n",
            "Epoch:185\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.384\n",
            "Epoch:185\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.416\n",
            "Epoch:185\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.428\n",
            "Epoch:185\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.345\n",
            "Epoch:185\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.293\n",
            "Epoch:185\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.363\n",
            "Epoch:185\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.341\n",
            "Epoch:185\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.325\n",
            "Epoch:185\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.353\n",
            "Epoch:185\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.336\n",
            "Epoch:185\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.414\n",
            "Epoch:185\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.414\n",
            "Epoch:185\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.391\n",
            "Epoch:185\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.445\n",
            "Epoch:185\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.373\n",
            "Epoch:185\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.385\n",
            "Epoch:185\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.310\n",
            "Epoch:185\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.356\n",
            "Epoch:185\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.304\n",
            "Epoch:185\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.410\n",
            "Epoch:185\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.400\n",
            "Epoch:185\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.299\n",
            "Epoch:185\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.319\n",
            "Epoch:185\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.311\n",
            "Epoch:185\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.417\n",
            "Epoch:185\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.361\n",
            "Epoch:185\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.310\n",
            "Epoch:185\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.432\n",
            "Epoch:185\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.521\n",
            "Epoch:185\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.299\n",
            "Epoch:185\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.337\n",
            "Epoch:185\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.336\n",
            "Epoch:185\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.407\n",
            "Epoch:185\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.474\n",
            "Epoch:185\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.461\n",
            "Epoch:185\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.431\n",
            "Epoch:185\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.379\n",
            "Epoch:185\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.347\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:186\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.444\n",
            "Epoch:186\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.255\n",
            "Epoch:186\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.291\n",
            "Epoch:186\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.385\n",
            "Epoch:186\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.278\n",
            "Epoch:186\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.354\n",
            "Epoch:186\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.312\n",
            "Epoch:186\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.459\n",
            "Epoch:186\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.420\n",
            "Epoch:186\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.463\n",
            "Epoch:186\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.369\n",
            "Epoch:186\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.270\n",
            "Epoch:186\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.219\n",
            "Epoch:186\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.393\n",
            "Epoch:186\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.382\n",
            "Epoch:186\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.381\n",
            "Epoch:186\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.326\n",
            "Epoch:186\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.495\n",
            "Epoch:186\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.429\n",
            "Epoch:186\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.415\n",
            "Epoch:186\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.376\n",
            "Epoch:186\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.358\n",
            "Epoch:186\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.414\n",
            "Epoch:186\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.361\n",
            "Epoch:186\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.392\n",
            "Epoch:186\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.470\n",
            "Epoch:186\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.398\n",
            "Epoch:186\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.279\n",
            "Epoch:186\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.497\n",
            "Epoch:186\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.526\n",
            "Epoch:186\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.314\n",
            "Epoch:186\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.361\n",
            "Epoch:186\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.500\n",
            "Epoch:186\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.469\n",
            "Epoch:186\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.242\n",
            "Epoch:186\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.369\n",
            "Epoch:186\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.323\n",
            "Epoch:186\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.362\n",
            "Epoch:186\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.388\n",
            "Epoch:186\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.461\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:187\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.380\n",
            "Epoch:187\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.303\n",
            "Epoch:187\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.339\n",
            "Epoch:187\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.384\n",
            "Epoch:187\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.308\n",
            "Epoch:187\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.270\n",
            "Epoch:187\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.310\n",
            "Epoch:187\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.419\n",
            "Epoch:187\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.332\n",
            "Epoch:187\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.424\n",
            "Epoch:187\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.223\n",
            "Epoch:187\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.302\n",
            "Epoch:187\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.289\n",
            "Epoch:187\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.330\n",
            "Epoch:187\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.339\n",
            "Epoch:187\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.381\n",
            "Epoch:187\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.391\n",
            "Epoch:187\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.416\n",
            "Epoch:187\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.367\n",
            "Epoch:187\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.356\n",
            "Epoch:187\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.364\n",
            "Epoch:187\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.387\n",
            "Epoch:187\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.346\n",
            "Epoch:187\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.439\n",
            "Epoch:187\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.349\n",
            "Epoch:187\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.391\n",
            "Epoch:187\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.494\n",
            "Epoch:187\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.380\n",
            "Epoch:187\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.329\n",
            "Epoch:187\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.550\n",
            "Epoch:187\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.251\n",
            "Epoch:187\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.421\n",
            "Epoch:187\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.405\n",
            "Epoch:187\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.320\n",
            "Epoch:187\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.368\n",
            "Epoch:187\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.396\n",
            "Epoch:187\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.439\n",
            "Epoch:187\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.372\n",
            "Epoch:187\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.392\n",
            "Epoch:187\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.395\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:188\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.248\n",
            "Epoch:188\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.319\n",
            "Epoch:188\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.326\n",
            "Epoch:188\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.248\n",
            "Epoch:188\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.358\n",
            "Epoch:188\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.419\n",
            "Epoch:188\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.293\n",
            "Epoch:188\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.268\n",
            "Epoch:188\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.344\n",
            "Epoch:188\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.434\n",
            "Epoch:188\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.362\n",
            "Epoch:188\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.386\n",
            "Epoch:188\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.305\n",
            "Epoch:188\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.449\n",
            "Epoch:188\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.381\n",
            "Epoch:188\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.346\n",
            "Epoch:188\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.432\n",
            "Epoch:188\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.322\n",
            "Epoch:188\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.319\n",
            "Epoch:188\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.383\n",
            "Epoch:188\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.445\n",
            "Epoch:188\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.482\n",
            "Epoch:188\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.318\n",
            "Epoch:188\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.387\n",
            "Epoch:188\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.427\n",
            "Epoch:188\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.277\n",
            "Epoch:188\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.353\n",
            "Epoch:188\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.332\n",
            "Epoch:188\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.423\n",
            "Epoch:188\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.439\n",
            "Epoch:188\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.310\n",
            "Epoch:188\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.321\n",
            "Epoch:188\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.340\n",
            "Epoch:188\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.315\n",
            "Epoch:188\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.410\n",
            "Epoch:188\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.376\n",
            "Epoch:188\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.298\n",
            "Epoch:188\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.446\n",
            "Epoch:188\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.362\n",
            "Epoch:188\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.414\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:189\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.279\n",
            "Epoch:189\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.446\n",
            "Epoch:189\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.332\n",
            "Epoch:189\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.264\n",
            "Epoch:189\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.352\n",
            "Epoch:189\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.292\n",
            "Epoch:189\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.285\n",
            "Epoch:189\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.346\n",
            "Epoch:189\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.457\n",
            "Epoch:189\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.279\n",
            "Epoch:189\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.451\n",
            "Epoch:189\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.342\n",
            "Epoch:189\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.328\n",
            "Epoch:189\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.445\n",
            "Epoch:189\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.288\n",
            "Epoch:189\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.275\n",
            "Epoch:189\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.414\n",
            "Epoch:189\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.420\n",
            "Epoch:189\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.412\n",
            "Epoch:189\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.320\n",
            "Epoch:189\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.414\n",
            "Epoch:189\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.263\n",
            "Epoch:189\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.292\n",
            "Epoch:189\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.363\n",
            "Epoch:189\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.322\n",
            "Epoch:189\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.292\n",
            "Epoch:189\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.348\n",
            "Epoch:189\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.333\n",
            "Epoch:189\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.468\n",
            "Epoch:189\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.402\n",
            "Epoch:189\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.311\n",
            "Epoch:189\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.477\n",
            "Epoch:189\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.409\n",
            "Epoch:189\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.316\n",
            "Epoch:189\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.428\n",
            "Epoch:189\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.326\n",
            "Epoch:189\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.467\n",
            "Epoch:189\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.500\n",
            "Epoch:189\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.298\n",
            "Epoch:189\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.281\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:190\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.318\n",
            "Epoch:190\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.369\n",
            "Epoch:190\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.294\n",
            "Epoch:190\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.432\n",
            "Epoch:190\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.316\n",
            "Epoch:190\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.359\n",
            "Epoch:190\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.369\n",
            "Epoch:190\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.437\n",
            "Epoch:190\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.456\n",
            "Epoch:190\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.317\n",
            "Epoch:190\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.252\n",
            "Epoch:190\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.368\n",
            "Epoch:190\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.292\n",
            "Epoch:190\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.350\n",
            "Epoch:190\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.401\n",
            "Epoch:190\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.346\n",
            "Epoch:190\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.286\n",
            "Epoch:190\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.265\n",
            "Epoch:190\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.425\n",
            "Epoch:190\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.305\n",
            "Epoch:190\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.221\n",
            "Epoch:190\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.420\n",
            "Epoch:190\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.348\n",
            "Epoch:190\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.283\n",
            "Epoch:190\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.344\n",
            "Epoch:190\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.391\n",
            "Epoch:190\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.265\n",
            "Epoch:190\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.400\n",
            "Epoch:190\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.368\n",
            "Epoch:190\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.474\n",
            "Epoch:190\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.278\n",
            "Epoch:190\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.461\n",
            "Epoch:190\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.408\n",
            "Epoch:190\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.393\n",
            "Epoch:190\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.383\n",
            "Epoch:190\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.357\n",
            "Epoch:190\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.496\n",
            "Epoch:190\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.441\n",
            "Epoch:190\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.389\n",
            "Epoch:190\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.345\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:191\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.354\n",
            "Epoch:191\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.286\n",
            "Epoch:191\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.303\n",
            "Epoch:191\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.414\n",
            "Epoch:191\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.377\n",
            "Epoch:191\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.369\n",
            "Epoch:191\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.428\n",
            "Epoch:191\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.330\n",
            "Epoch:191\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.316\n",
            "Epoch:191\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.319\n",
            "Epoch:191\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.269\n",
            "Epoch:191\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.312\n",
            "Epoch:191\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.310\n",
            "Epoch:191\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.489\n",
            "Epoch:191\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.431\n",
            "Epoch:191\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.326\n",
            "Epoch:191\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.369\n",
            "Epoch:191\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.412\n",
            "Epoch:191\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.256\n",
            "Epoch:191\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.418\n",
            "Epoch:191\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.279\n",
            "Epoch:191\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.373\n",
            "Epoch:191\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.219\n",
            "Epoch:191\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.436\n",
            "Epoch:191\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.309\n",
            "Epoch:191\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.352\n",
            "Epoch:191\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.408\n",
            "Epoch:191\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.401\n",
            "Epoch:191\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.460\n",
            "Epoch:191\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.355\n",
            "Epoch:191\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.312\n",
            "Epoch:191\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.300\n",
            "Epoch:191\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.445\n",
            "Epoch:191\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.358\n",
            "Epoch:191\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.336\n",
            "Epoch:191\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.388\n",
            "Epoch:191\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.414\n",
            "Epoch:191\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.373\n",
            "Epoch:191\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.349\n",
            "Epoch:191\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.479\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:192\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.306\n",
            "Epoch:192\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.271\n",
            "Epoch:192\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.359\n",
            "Epoch:192\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.286\n",
            "Epoch:192\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.453\n",
            "Epoch:192\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.497\n",
            "Epoch:192\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.380\n",
            "Epoch:192\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.395\n",
            "Epoch:192\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.381\n",
            "Epoch:192\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.187\n",
            "Epoch:192\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.375\n",
            "Epoch:192\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.277\n",
            "Epoch:192\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.346\n",
            "Epoch:192\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.281\n",
            "Epoch:192\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.433\n",
            "Epoch:192\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.319\n",
            "Epoch:192\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.312\n",
            "Epoch:192\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.326\n",
            "Epoch:192\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.328\n",
            "Epoch:192\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.329\n",
            "Epoch:192\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.350\n",
            "Epoch:192\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.415\n",
            "Epoch:192\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.403\n",
            "Epoch:192\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.336\n",
            "Epoch:192\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.342\n",
            "Epoch:192\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.311\n",
            "Epoch:192\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.591\n",
            "Epoch:192\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.470\n",
            "Epoch:192\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.338\n",
            "Epoch:192\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.355\n",
            "Epoch:192\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.328\n",
            "Epoch:192\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.319\n",
            "Epoch:192\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.485\n",
            "Epoch:192\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.447\n",
            "Epoch:192\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.463\n",
            "Epoch:192\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.368\n",
            "Epoch:192\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.314\n",
            "Epoch:192\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.326\n",
            "Epoch:192\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.441\n",
            "Epoch:192\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.351\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:193\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.246\n",
            "Epoch:193\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.235\n",
            "Epoch:193\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.339\n",
            "Epoch:193\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.343\n",
            "Epoch:193\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.269\n",
            "Epoch:193\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.387\n",
            "Epoch:193\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.389\n",
            "Epoch:193\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.201\n",
            "Epoch:193\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.273\n",
            "Epoch:193\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.341\n",
            "Epoch:193\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.395\n",
            "Epoch:193\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.389\n",
            "Epoch:193\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.297\n",
            "Epoch:193\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.378\n",
            "Epoch:193\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.270\n",
            "Epoch:193\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.284\n",
            "Epoch:193\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.608\n",
            "Epoch:193\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.322\n",
            "Epoch:193\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.498\n",
            "Epoch:193\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.468\n",
            "Epoch:193\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.388\n",
            "Epoch:193\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.332\n",
            "Epoch:193\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.344\n",
            "Epoch:193\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.441\n",
            "Epoch:193\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.471\n",
            "Epoch:193\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.319\n",
            "Epoch:193\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.348\n",
            "Epoch:193\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.337\n",
            "Epoch:193\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.359\n",
            "Epoch:193\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.407\n",
            "Epoch:193\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.344\n",
            "Epoch:193\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.359\n",
            "Epoch:193\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.398\n",
            "Epoch:193\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.343\n",
            "Epoch:193\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.385\n",
            "Epoch:193\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.386\n",
            "Epoch:193\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.364\n",
            "Epoch:193\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.379\n",
            "Epoch:193\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.302\n",
            "Epoch:193\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.358\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:194\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.374\n",
            "Epoch:194\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.318\n",
            "Epoch:194\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.399\n",
            "Epoch:194\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.402\n",
            "Epoch:194\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.299\n",
            "Epoch:194\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.373\n",
            "Epoch:194\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.328\n",
            "Epoch:194\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.262\n",
            "Epoch:194\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.316\n",
            "Epoch:194\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.316\n",
            "Epoch:194\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.409\n",
            "Epoch:194\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.459\n",
            "Epoch:194\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.367\n",
            "Epoch:194\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.383\n",
            "Epoch:194\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.325\n",
            "Epoch:194\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.342\n",
            "Epoch:194\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.347\n",
            "Epoch:194\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.365\n",
            "Epoch:194\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.311\n",
            "Epoch:194\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.332\n",
            "Epoch:194\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.328\n",
            "Epoch:194\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.371\n",
            "Epoch:194\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.253\n",
            "Epoch:194\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.260\n",
            "Epoch:194\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.338\n",
            "Epoch:194\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.245\n",
            "Epoch:194\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.358\n",
            "Epoch:194\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.282\n",
            "Epoch:194\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.331\n",
            "Epoch:194\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.325\n",
            "Epoch:194\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.286\n",
            "Epoch:194\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.362\n",
            "Epoch:194\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.332\n",
            "Epoch:194\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.351\n",
            "Epoch:194\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.383\n",
            "Epoch:194\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.319\n",
            "Epoch:194\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.513\n",
            "Epoch:194\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.325\n",
            "Epoch:194\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.364\n",
            "Epoch:194\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.501\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:195\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.244\n",
            "Epoch:195\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.336\n",
            "Epoch:195\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.506\n",
            "Epoch:195\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.351\n",
            "Epoch:195\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.361\n",
            "Epoch:195\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.330\n",
            "Epoch:195\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.303\n",
            "Epoch:195\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.330\n",
            "Epoch:195\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.332\n",
            "Epoch:195\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.423\n",
            "Epoch:195\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.509\n",
            "Epoch:195\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.290\n",
            "Epoch:195\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.302\n",
            "Epoch:195\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.368\n",
            "Epoch:195\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.431\n",
            "Epoch:195\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.418\n",
            "Epoch:195\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.351\n",
            "Epoch:195\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.412\n",
            "Epoch:195\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.394\n",
            "Epoch:195\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.338\n",
            "Epoch:195\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.281\n",
            "Epoch:195\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.273\n",
            "Epoch:195\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.429\n",
            "Epoch:195\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.421\n",
            "Epoch:195\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.320\n",
            "Epoch:195\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.198\n",
            "Epoch:195\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.431\n",
            "Epoch:195\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.367\n",
            "Epoch:195\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.317\n",
            "Epoch:195\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.438\n",
            "Epoch:195\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.403\n",
            "Epoch:195\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.312\n",
            "Epoch:195\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.482\n",
            "Epoch:195\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.419\n",
            "Epoch:195\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.459\n",
            "Epoch:195\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.395\n",
            "Epoch:195\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.383\n",
            "Epoch:195\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.365\n",
            "Epoch:195\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.358\n",
            "Epoch:195\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.441\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:196\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.282\n",
            "Epoch:196\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.389\n",
            "Epoch:196\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.326\n",
            "Epoch:196\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.297\n",
            "Epoch:196\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.467\n",
            "Epoch:196\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.304\n",
            "Epoch:196\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.411\n",
            "Epoch:196\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.263\n",
            "Epoch:196\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.361\n",
            "Epoch:196\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.287\n",
            "Epoch:196\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.325\n",
            "Epoch:196\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.385\n",
            "Epoch:196\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.348\n",
            "Epoch:196\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.356\n",
            "Epoch:196\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.390\n",
            "Epoch:196\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.372\n",
            "Epoch:196\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.398\n",
            "Epoch:196\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.399\n",
            "Epoch:196\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.312\n",
            "Epoch:196\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.320\n",
            "Epoch:196\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.403\n",
            "Epoch:196\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.360\n",
            "Epoch:196\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.391\n",
            "Epoch:196\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.332\n",
            "Epoch:196\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.390\n",
            "Epoch:196\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.371\n",
            "Epoch:196\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.354\n",
            "Epoch:196\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.381\n",
            "Epoch:196\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.302\n",
            "Epoch:196\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.349\n",
            "Epoch:196\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.345\n",
            "Epoch:196\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.397\n",
            "Epoch:196\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.380\n",
            "Epoch:196\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.312\n",
            "Epoch:196\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.246\n",
            "Epoch:196\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.254\n",
            "Epoch:196\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.361\n",
            "Epoch:196\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.406\n",
            "Epoch:196\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.434\n",
            "Epoch:196\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.624\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:197\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.253\n",
            "Epoch:197\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.317\n",
            "Epoch:197\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.278\n",
            "Epoch:197\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.398\n",
            "Epoch:197\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.391\n",
            "Epoch:197\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.421\n",
            "Epoch:197\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.294\n",
            "Epoch:197\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.360\n",
            "Epoch:197\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.344\n",
            "Epoch:197\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.489\n",
            "Epoch:197\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.241\n",
            "Epoch:197\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.365\n",
            "Epoch:197\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.324\n",
            "Epoch:197\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.382\n",
            "Epoch:197\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.293\n",
            "Epoch:197\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.335\n",
            "Epoch:197\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.402\n",
            "Epoch:197\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.330\n",
            "Epoch:197\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.372\n",
            "Epoch:197\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.301\n",
            "Epoch:197\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.296\n",
            "Epoch:197\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.333\n",
            "Epoch:197\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.419\n",
            "Epoch:197\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.341\n",
            "Epoch:197\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.347\n",
            "Epoch:197\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.406\n",
            "Epoch:197\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.352\n",
            "Epoch:197\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.423\n",
            "Epoch:197\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.330\n",
            "Epoch:197\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.371\n",
            "Epoch:197\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.401\n",
            "Epoch:197\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.363\n",
            "Epoch:197\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.296\n",
            "Epoch:197\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.445\n",
            "Epoch:197\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.379\n",
            "Epoch:197\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.310\n",
            "Epoch:197\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.353\n",
            "Epoch:197\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.380\n",
            "Epoch:197\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.405\n",
            "Epoch:197\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.389\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:198\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.271\n",
            "Epoch:198\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.341\n",
            "Epoch:198\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.302\n",
            "Epoch:198\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.276\n",
            "Epoch:198\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.297\n",
            "Epoch:198\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.351\n",
            "Epoch:198\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.297\n",
            "Epoch:198\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.342\n",
            "Epoch:198\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.354\n",
            "Epoch:198\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.282\n",
            "Epoch:198\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.384\n",
            "Epoch:198\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.368\n",
            "Epoch:198\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.348\n",
            "Epoch:198\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.357\n",
            "Epoch:198\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.414\n",
            "Epoch:198\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.406\n",
            "Epoch:198\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.272\n",
            "Epoch:198\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.313\n",
            "Epoch:198\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.388\n",
            "Epoch:198\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.388\n",
            "Epoch:198\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.450\n",
            "Epoch:198\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.306\n",
            "Epoch:198\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.385\n",
            "Epoch:198\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.377\n",
            "Epoch:198\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.243\n",
            "Epoch:198\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.351\n",
            "Epoch:198\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.342\n",
            "Epoch:198\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.267\n",
            "Epoch:198\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.256\n",
            "Epoch:198\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.351\n",
            "Epoch:198\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.343\n",
            "Epoch:198\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.331\n",
            "Epoch:198\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.391\n",
            "Epoch:198\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.473\n",
            "Epoch:198\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.362\n",
            "Epoch:198\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.376\n",
            "Epoch:198\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.267\n",
            "Epoch:198\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.331\n",
            "Epoch:198\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.444\n",
            "Epoch:198\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.341\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:199\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.245\n",
            "Epoch:199\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.549\n",
            "Epoch:199\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.408\n",
            "Epoch:199\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.284\n",
            "Epoch:199\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.270\n",
            "Epoch:199\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.330\n",
            "Epoch:199\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.420\n",
            "Epoch:199\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.344\n",
            "Epoch:199\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.397\n",
            "Epoch:199\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.337\n",
            "Epoch:199\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.382\n",
            "Epoch:199\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.306\n",
            "Epoch:199\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.262\n",
            "Epoch:199\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.407\n",
            "Epoch:199\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.283\n",
            "Epoch:199\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.403\n",
            "Epoch:199\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.264\n",
            "Epoch:199\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.415\n",
            "Epoch:199\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.410\n",
            "Epoch:199\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.489\n",
            "Epoch:199\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.354\n",
            "Epoch:199\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.322\n",
            "Epoch:199\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.404\n",
            "Epoch:199\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.268\n",
            "Epoch:199\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.449\n",
            "Epoch:199\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.370\n",
            "Epoch:199\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.350\n",
            "Epoch:199\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.499\n",
            "Epoch:199\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.333\n",
            "Epoch:199\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.347\n",
            "Epoch:199\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.306\n",
            "Epoch:199\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.322\n",
            "Epoch:199\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.310\n",
            "Epoch:199\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.330\n",
            "Epoch:199\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.343\n",
            "Epoch:199\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.379\n",
            "Epoch:199\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.331\n",
            "Epoch:199\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.315\n",
            "Epoch:199\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.292\n",
            "Epoch:199\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.478\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Epoch:200\t Step:1\t TrainedSample:128\t TotalSample:50000\t Loss:0.309\n",
            "Epoch:200\t Step:11\t TrainedSample:1408\t TotalSample:50000\t Loss:0.335\n",
            "Epoch:200\t Step:21\t TrainedSample:2688\t TotalSample:50000\t Loss:0.373\n",
            "Epoch:200\t Step:31\t TrainedSample:3968\t TotalSample:50000\t Loss:0.370\n",
            "Epoch:200\t Step:41\t TrainedSample:5248\t TotalSample:50000\t Loss:0.368\n",
            "Epoch:200\t Step:51\t TrainedSample:6528\t TotalSample:50000\t Loss:0.343\n",
            "Epoch:200\t Step:61\t TrainedSample:7808\t TotalSample:50000\t Loss:0.257\n",
            "Epoch:200\t Step:71\t TrainedSample:9088\t TotalSample:50000\t Loss:0.407\n",
            "Epoch:200\t Step:81\t TrainedSample:10368\t TotalSample:50000\t Loss:0.361\n",
            "Epoch:200\t Step:91\t TrainedSample:11648\t TotalSample:50000\t Loss:0.326\n",
            "Epoch:200\t Step:101\t TrainedSample:12928\t TotalSample:50000\t Loss:0.363\n",
            "Epoch:200\t Step:111\t TrainedSample:14208\t TotalSample:50000\t Loss:0.345\n",
            "Epoch:200\t Step:121\t TrainedSample:15488\t TotalSample:50000\t Loss:0.356\n",
            "Epoch:200\t Step:131\t TrainedSample:16768\t TotalSample:50000\t Loss:0.280\n",
            "Epoch:200\t Step:141\t TrainedSample:18048\t TotalSample:50000\t Loss:0.414\n",
            "Epoch:200\t Step:151\t TrainedSample:19328\t TotalSample:50000\t Loss:0.444\n",
            "Epoch:200\t Step:161\t TrainedSample:20608\t TotalSample:50000\t Loss:0.399\n",
            "Epoch:200\t Step:171\t TrainedSample:21888\t TotalSample:50000\t Loss:0.326\n",
            "Epoch:200\t Step:181\t TrainedSample:23168\t TotalSample:50000\t Loss:0.366\n",
            "Epoch:200\t Step:191\t TrainedSample:24448\t TotalSample:50000\t Loss:0.338\n",
            "Epoch:200\t Step:201\t TrainedSample:25728\t TotalSample:50000\t Loss:0.393\n",
            "Epoch:200\t Step:211\t TrainedSample:27008\t TotalSample:50000\t Loss:0.372\n",
            "Epoch:200\t Step:221\t TrainedSample:28288\t TotalSample:50000\t Loss:0.308\n",
            "Epoch:200\t Step:231\t TrainedSample:29568\t TotalSample:50000\t Loss:0.237\n",
            "Epoch:200\t Step:241\t TrainedSample:30848\t TotalSample:50000\t Loss:0.303\n",
            "Epoch:200\t Step:251\t TrainedSample:32128\t TotalSample:50000\t Loss:0.326\n",
            "Epoch:200\t Step:261\t TrainedSample:33408\t TotalSample:50000\t Loss:0.303\n",
            "Epoch:200\t Step:271\t TrainedSample:34688\t TotalSample:50000\t Loss:0.376\n",
            "Epoch:200\t Step:281\t TrainedSample:35968\t TotalSample:50000\t Loss:0.330\n",
            "Epoch:200\t Step:291\t TrainedSample:37248\t TotalSample:50000\t Loss:0.372\n",
            "Epoch:200\t Step:301\t TrainedSample:38528\t TotalSample:50000\t Loss:0.274\n",
            "Epoch:200\t Step:311\t TrainedSample:39808\t TotalSample:50000\t Loss:0.295\n",
            "Epoch:200\t Step:321\t TrainedSample:41088\t TotalSample:50000\t Loss:0.295\n",
            "Epoch:200\t Step:331\t TrainedSample:42368\t TotalSample:50000\t Loss:0.333\n",
            "Epoch:200\t Step:341\t TrainedSample:43648\t TotalSample:50000\t Loss:0.341\n",
            "Epoch:200\t Step:351\t TrainedSample:44928\t TotalSample:50000\t Loss:0.425\n",
            "Epoch:200\t Step:361\t TrainedSample:46208\t TotalSample:50000\t Loss:0.447\n",
            "Epoch:200\t Step:371\t TrainedSample:47488\t TotalSample:50000\t Loss:0.322\n",
            "Epoch:200\t Step:381\t TrainedSample:48768\t TotalSample:50000\t Loss:0.333\n",
            "Epoch:200\t Step:391\t TrainedSample:50000\t TotalSample:50000\t Loss:0.389\n",
            "evaluating\n",
            "saving regular\n",
            "saving best\n",
            "Figure(1200x400)\n",
            "56.15578439350128\n",
            "0.28077892196750637\n"
          ]
        }
      ],
      "source": [
        "!python train.py -net efficientnetb0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bju_lVYdzqAj"
      },
      "source": [
        "# Save weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpSgCQ7PwsjP",
        "outputId": "cc67dac5-a8fb-4e85-ab2b-40014bf13103"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SkNDjIZzztbr",
        "outputId": "1b0906fd-5e1f-47e9-8d45-bdb396dddafa"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/gdrive/My Drive/bestParam.pth'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import shutil\n",
        "\n",
        "source_path = '/content/small-net-cifar100/checkpoint/efficientnetb0/2023-12-02/bestParam.pth'\n",
        "destination_path = '/content/gdrive/My Drive/bestParam.pth'\n",
        "\n",
        "shutil.copyfile(source_path, destination_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dd0gcyZ0I9Fb"
      },
      "source": [
        "# load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-OJjlaCzuvJ",
        "outputId": "e418c13b-2e71-434b-8329-9d21519bd997"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-2b031e1ed132>:106: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
            "  nn.init.uniform(m.weight, -bound, bound)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Define the EfficientNet model\n",
        "model = efficientnet(1, 1, 100, bn_momentum=0.9)\n",
        "\n",
        "# Load pre-trained weights\n",
        "pretrained_weights_path = '/content/small-net-cifar100/checkpoint/efficientnetb0/2023-12-02/bestParam.pth'\n",
        "state_dict = torch.load(pretrained_weights_path)\n",
        "\n",
        "# Load the state_dict into the model\n",
        "model.load_state_dict(state_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRaNG3eyF94P",
        "outputId": "90eb36eb-cd0a-4361-9db8-7a6fca027eb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Test set size: 10000\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "# Define data transformations\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# Load CIFAR-100 dataset\n",
        "cifar100_test = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "\n",
        "# Create PyTorch data loaders\n",
        "batch_size = 128\n",
        "val_loader = torch.utils.data.DataLoader(cifar100_test, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Check the number of samples in each set\n",
        "print(f\"Test set size: {len(cifar100_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIUZesr6GNmQ",
        "outputId": "6fd89d5b-cc4c-49da-dd64-599d5ffc87bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Test Accuracy: 69.78%\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "correct_test = 0\n",
        "total_test = 0\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move the model to the device\n",
        "model.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in val_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_test += labels.size(0)\n",
        "        correct_test += (predicted == labels).sum().item()\n",
        "\n",
        "test_accuracy = 100 * correct_test / total_test\n",
        "print(f\"Final Test Accuracy: {test_accuracy:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dmh21mtMGSs3"
      },
      "source": [
        "# Load Model and Get Feature Latent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUCICvMVGPAd",
        "outputId": "a5d2a858-f250-4ae1-fc5f-c0ecce7e36a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Length of class_sampler_indices_train: 8000\n",
            "Balanced Train set size: 8000\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "random_seed = 42\n",
        "torch.manual_seed(random_seed)\n",
        "np.random.seed(random_seed)\n",
        "random.seed(random_seed)\n",
        "\n",
        "# Load the CIFAR-100 dataset and create a balanced subset\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "# Load CIFAR-100 dataset\n",
        "cifar100_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_size = int(0.8 * len(cifar100_dataset))\n",
        "val_size = len(cifar100_dataset) - train_size\n",
        "cifar100_traindataset, cifar100_valdataset = torch.utils.data.random_split(cifar100_dataset, [train_size, val_size])\n",
        "\n",
        "# Define the subset size\n",
        "subset_fraction = 1\n",
        "subset_size_train = int(subset_fraction * len(cifar100_traindataset))\n",
        "\n",
        "class_indices = list(range(len(cifar100_traindataset.dataset.classes)))\n",
        "class_subset_size = int(subset_size_train / len(cifar100_traindataset.dataset.classes))\n",
        "\n",
        "class_sampler_indices_train = []\n",
        "\n",
        "for class_index in class_indices:\n",
        "    class_indices_list_train = [i for i, label in enumerate(cifar100_traindataset.dataset.targets) if label == class_index]\n",
        "    class_sampler_indices_train.extend(class_indices_list_train[:class_subset_size])\n",
        "\n",
        "\n",
        "print(f\"Length of class_sampler_indices_train: {len(class_sampler_indices_train)}\")\n",
        "\n",
        "train_sampler = SubsetRandomSampler(class_sampler_indices_train)\n",
        "\n",
        "batch_size = 256\n",
        "train_loader = torch.utils.data.DataLoader(cifar100_traindataset, batch_size=batch_size, sampler=train_sampler)\n",
        "\n",
        "# Check the number of samples in the balanced train set\n",
        "print(f\"Balanced Train set size: {len(train_loader.sampler)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EftHJSbtGbjf",
        "outputId": "868a0f6a-3534-4028-fb31-0534af875dc1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-2b031e1ed132>:106: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
            "  nn.init.uniform(m.weight, -bound, bound)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# Define the EfficientNet model\n",
        "model = efficientnet(1, 1, 100, bn_momentum=0.9)\n",
        "\n",
        "# Load pre-trained weights\n",
        "pretrained_weights_path = '/content/small-net-cifar100/checkpoint/efficientnetb0/2023-12-02/bestParam.pth'\n",
        "state_dict = torch.load(pretrained_weights_path)\n",
        "\n",
        "# Load the state_dict into the model\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "model = nn.Sequential(*list(model.children())[:-1]).to(device)\n",
        "\n",
        "# Initialize lists to store data and labels\n",
        "data_list = []\n",
        "labels_list = []\n",
        "\n",
        "# Iterate through the balanced training set to extract data and labels\n",
        "for inputs, labels in train_loader:\n",
        "    if torch.cuda.is_available():\n",
        "        inputs = inputs.to('cuda')\n",
        "    # Forward pass through the model to get feature representation\n",
        "    features = model(inputs)\n",
        "\n",
        "    # Append the features and labels to the lists\n",
        "    data_list.append(features)\n",
        "    labels_list.append(labels)\n",
        "\n",
        "    # Release GPU memory\n",
        "    del inputs\n",
        "    torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdCHyS9OJhKb"
      },
      "outputs": [],
      "source": [
        "# Stack and reshape the extracted features\n",
        "features = torch.cat(data_list)\n",
        "features = features.view(features.size(0), -1)\n",
        "labels = torch.cat(labels_list)\n",
        "labels = labels.unsqueeze(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvBCzqN1NqMT"
      },
      "source": [
        "# Metrics for train set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIdT5a2qM2zn",
        "outputId": "20b72b36-8d0f-4bf4-bb5a-574f10450c20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'data_complexity_measures'...\n",
            "remote: Enumerating objects: 140, done.\u001b[K\n",
            "remote: Counting objects: 100% (140/140), done.\u001b[K\n",
            "remote: Compressing objects: 100% (127/127), done.\u001b[K\n",
            "remote: Total 140 (delta 80), reused 33 (delta 9), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (140/140), 144.00 KiB | 1.40 MiB/s, done.\n",
            "Resolving deltas: 100% (80/80), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Arhosseini77/data_complexity_measures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BkToReZQNtIQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "import numpy as np\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "from data_complexity_measures.models.ARH_SeparationIndex import ARH_SeparationIndex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0MhZqcXN0c5",
        "outputId": "22bd9a16-2f24-47a1-9fe6-05dc0ce233a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data has been normalized\n"
          ]
        }
      ],
      "source": [
        "# Create Instance of class\n",
        "si_calculator = ARH_SeparationIndex(features, labels, normalize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHPddXSnN5or"
      },
      "source": [
        "# SI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIuu88pkN4Qd",
        "outputId": "d2400c6f-be1e-44de-ec22-1314add56d05"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating SI: 100%|██████████| 1/1 [00:00<00:00, 121.46it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.4624999761581421\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "si_data = si_calculator.si_batch(batch_size=1000)\n",
        "print(si_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8k99D6CQOD8E"
      },
      "source": [
        "# Calc High order SI (order = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CiW7HbaOBaZ",
        "outputId": "e0edcbc8-51c5-4390-83c4-57255121227c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing High Order SI: 100%|██████████| 1/1 [00:01<00:00,  1.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.2524999976158142\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "si_high_order_2_data = si_calculator.high_order_si_batch(order=2,batch_size=1000)\n",
        "print(si_high_order_2_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRXdp90IOMBz"
      },
      "source": [
        "# High order soft SI (order=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QhO8zpKOHeO",
        "outputId": "79c1c3d6-0d0b-4be6-e10a-d576a0a29098"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating Soft Order SI: 100%|██████████| 1/1 [00:00<00:00, 727.93it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Soft Order(2) SI : 0.4387499988079071\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "si_soft_order_2_data = si_calculator.soft_order_si_batch(order=2,batch_size=1000)\n",
        "print(\"Soft Order(2) SI :\", si_soft_order_2_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dcE0LwlOVOu"
      },
      "source": [
        "# Center Based SI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f16JkFMjOQyu",
        "outputId": "b457c518-1e66-480f-fe16-9d0506df65a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Center SI: 0.681\n"
          ]
        }
      ],
      "source": [
        "center_si_data = si_calculator.center_si_batch(batch_size=1000)\n",
        "print(\"Center SI:\", center_si_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LysLuDVPOa0K"
      },
      "source": [
        "# Anti SI (order = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgZjLi0hOZLK",
        "outputId": "d8224d26-0b39-4197-e528-d1941c6d7eda"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating Anti-SI: 100%|██████████| 400/400 [00:00<00:00, 10979.64it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "anti_si: 0.375\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "anti_si = si_calculator.anti_si(order=2)\n",
        "print(\"anti_si:\", anti_si)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNAP5nLsOyUa"
      },
      "source": [
        "# Metrics for test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsgLDHv6OeZ3",
        "outputId": "d569b7a6-7a3c-471c-db27-9b53e836e7bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Balanced Train set size: 10000\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "# Load the CIFAR-100 dataset and create a balanced subset\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "cifar100_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Define the subset size\n",
        "subset_fraction = 1\n",
        "subset_size_test = int(subset_fraction * len(cifar100_dataset))\n",
        "\n",
        "# Create a balanced subset for both train and test sets using SubsetRandomSampler\n",
        "class_indices = list(range(len(cifar100_dataset.classes)))\n",
        "class_subset_size = int(subset_size_test / len(cifar100_dataset.classes))\n",
        "\n",
        "class_sampler_indices_test = []\n",
        "\n",
        "for class_index in class_indices:\n",
        "    class_indices_list_train = [i for i, label in enumerate(cifar100_dataset.targets) if label == class_index]\n",
        "    class_sampler_indices_test.extend(class_indices_list_train[:class_subset_size])\n",
        "\n",
        "test_sampler = SubsetRandomSampler(class_sampler_indices_test)\n",
        "\n",
        "# Create PyTorch data loaders using the balanced subset for both train and test sets\n",
        "batch_size = 256\n",
        "test_loader = torch.utils.data.DataLoader(cifar100_dataset, batch_size=batch_size, sampler=test_sampler)\n",
        "\n",
        "# Check the number of samples in each set\n",
        "print(f\"Balanced Train set size: {len(test_loader.sampler)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfH3qNVeO0fB",
        "outputId": "a8d01670-7b16-42a5-c721-5a6281387cb2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-4-2b031e1ed132>:106: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
            "  nn.init.uniform(m.weight, -bound, bound)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# Define the EfficientNet model\n",
        "model = efficientnet(1, 1, 100, bn_momentum=0.9)\n",
        "\n",
        "# Load pre-trained weights\n",
        "pretrained_weights_path = '/content/small-net-cifar100/checkpoint/efficientnetb0/2023-12-02/bestParam.pth'\n",
        "state_dict = torch.load(pretrained_weights_path)\n",
        "\n",
        "# Load the state_dict into the model\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "model = nn.Sequential(*list(model.children())[:-1]).to(device)\n",
        "\n",
        "# Initialize lists to store data and labels\n",
        "data_list = []\n",
        "labels_list = []\n",
        "\n",
        "# Iterate through the balanced training set to extract data and labels\n",
        "for inputs, labels in test_loader:\n",
        "    if torch.cuda.is_available():\n",
        "        inputs = inputs.to('cuda')\n",
        "    # Forward pass through the model to get feature representation\n",
        "    features = model(inputs)\n",
        "\n",
        "    # Append the features and labels to the lists\n",
        "    data_list.append(features)\n",
        "    labels_list.append(labels)\n",
        "\n",
        "    # Release GPU memory\n",
        "    del inputs\n",
        "    torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAvnUE81O4dD"
      },
      "outputs": [],
      "source": [
        "# Stack and reshape the extracted features\n",
        "features = torch.cat(data_list)\n",
        "features = features.view(features.size(0), -1)\n",
        "labels = torch.cat(labels_list)\n",
        "labels = labels.unsqueeze(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "saQkWABlPtjx",
        "outputId": "7a941d54-a977-450c-efa6-b3c8f457cbae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data has been normalized\n"
          ]
        }
      ],
      "source": [
        "# Create Instance of class\n",
        "si_calculator = ARH_SeparationIndex(features, labels, normalize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3zqnKRJP2rj"
      },
      "source": [
        "# SI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llGo3rE5P1bc",
        "outputId": "a4fa4c16-3120-4fcb-fe46-d3f18443eb86"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating SI: 100%|██████████| 1/1 [00:00<00:00, 1040.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.36500000953674316\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "si_data = si_calculator.si_batch(batch_size=1000)\n",
        "print(si_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBZ2JPyaP9Xw"
      },
      "source": [
        "# Calc High order SI (order = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "500GVafLP8DS",
        "outputId": "e24ea1ad-ff51-4fde-82f4-596fda132b80"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing High Order SI: 100%|██████████| 1/1 [00:00<00:00, 210.73it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.18000000715255737\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "si_high_order_2_data = si_calculator.high_order_si_batch(order=2,batch_size=1000)\n",
        "print(si_high_order_2_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvLRbWUCQEBl"
      },
      "source": [
        "# High order soft SI (order=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-5yN0S4QDhy",
        "outputId": "b52ed944-2145-49f5-be74-f3ab4be3ad44"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating Soft Order SI: 100%|██████████| 1/1 [00:00<00:00, 582.22it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Soft Order(2) SI : 0.31950002908706665\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "si_soft_order_2_data = si_calculator.soft_order_si_batch(order=2,batch_size=1000)\n",
        "print(\"Soft Order(2) SI :\", si_soft_order_2_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjCq-ddKQLrh"
      },
      "source": [
        "# Center Based SI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFmQjf_dQI8p",
        "outputId": "d7046b10-72ed-4b21-bb78-194fa3c41276"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating CSI: 100%|██████████| 1/1 [00:00<00:00, 741.57it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Center SI: 0.9270000457763672\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "center_si_data = si_calculator.center_si_batch(batch_size=1000)\n",
        "print(\"Center SI:\", center_si_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jyp9cktQSg5"
      },
      "source": [
        "# Anti SI (order = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaQyWRd6QQyT",
        "outputId": "6c8f2af1-3a37-445d-93bd-9f3f43be724d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating Anti-SI: 100%|██████████| 1000/1000 [00:00<00:00, 14613.43it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "anti_si: 0.5410000085830688\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "anti_si = si_calculator.anti_si(order=2)\n",
        "print(\"anti_si:\", anti_si)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbGq1gakQeYd"
      },
      "source": [
        "# Pretrained Efficientnetb0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wB7VjvjeijVp"
      },
      "outputs": [],
      "source": [
        "!pip install -qq efficientnet_pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UZFkx-HijYI",
        "outputId": "6d15d486-3079-4984-fd09-114673da80e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded pretrained weights for efficientnet-b0\n"
          ]
        }
      ],
      "source": [
        "from efficientnet_pytorch import EfficientNet\n",
        "model = EfficientNet.from_pretrained('efficientnet-b0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kDoqboJitDs",
        "outputId": "2efd1412-707a-4502-d8ab-092999fa47bd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "EfficientNet(\n",
              "  (_conv_stem): Conv2dStaticSamePadding(\n",
              "    3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
              "    (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
              "  )\n",
              "  (_bn0): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "  (_blocks): ModuleList(\n",
              "    (0): MBConvBlock(\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n",
              "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(16, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (1): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n",
              "        (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (2): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
              "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (3): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n",
              "        (static_padding): ZeroPad2d((1, 2, 1, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (4): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n",
              "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (5): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n",
              "        (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (6-7): 2 x MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
              "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (8): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n",
              "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (9-10): 2 x MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
              "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (11): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n",
              "        (static_padding): ZeroPad2d((1, 2, 1, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (12-14): 3 x MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
              "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (15): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n",
              "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(320, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "  )\n",
              "  (_conv_head): Conv2dStaticSamePadding(\n",
              "    320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "    (static_padding): Identity()\n",
              "  )\n",
              "  (_bn1): BatchNorm2d(1280, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
              "  (_dropout): Dropout(p=0.2, inplace=False)\n",
              "  (_fc): Linear(in_features=1280, out_features=1000, bias=True)\n",
              "  (_swish): MemoryEfficientSwish()\n",
              ")"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgGmgo75Q6EC",
        "outputId": "5ec7948b-b46a-447e-9568-2963dca37b33"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 32/32 [00:01<00:00, 19.25it/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def extract_features(model, dataloader):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    # Extract features up to _avg_pooling\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    # Switch off gradient computation\n",
        "    with torch.no_grad():\n",
        "        for images, targets in tqdm(dataloader):\n",
        "            images, targets = images.to(device), targets.to(device)\n",
        "\n",
        "            # Forward pass up to _avg_pooling\n",
        "            x = model._conv_stem(images)\n",
        "            x = model._bn0(x)\n",
        "            for block in model._blocks:\n",
        "                x = block(x)\n",
        "                if isinstance(block, models.efficientnet.SqueezeExcitation):\n",
        "                    # SqueezeExcitation block, apply Swish activation\n",
        "                    x = block._swish(x)\n",
        "            x = model._conv_head(x)\n",
        "            x = model._bn1(x)\n",
        "            x = model._swish(x)\n",
        "            x = model._avg_pooling(x)\n",
        "\n",
        "            # Flatten the output before appending to features\n",
        "            features.append(x.view(x.size(0), -1))\n",
        "            labels.append(targets)\n",
        "\n",
        "    features = torch.cat(features)\n",
        "    labels = torch.cat(labels)\n",
        "\n",
        "    return features, labels\n",
        "\n",
        "# Move models to GPU\n",
        "model.to(device)\n",
        "\n",
        "efficientnet_features, efficientnet_labels = extract_features(model, train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0odolHBlBba",
        "outputId": "4d9f5fe6-7cac-4aa7-b362-e7e61f6b52fd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([32000, 1280])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "efficientnet_features.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGGsD8ShRD5Q",
        "outputId": "ee3ceb71-3382-4411-950d-452e4cd0a469"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data has been normalized\n"
          ]
        }
      ],
      "source": [
        "si_calculator = ARH_SeparationIndex(efficientnet_features, efficientnet_labels, normalize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0r36nXsLSAfD"
      },
      "source": [
        "**Train**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AATQbRyARe1g"
      },
      "source": [
        "# SI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtweddSDRbQw",
        "outputId": "53865ea9-65e5-4a22-db14-12028434747a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating SI: 100%|██████████| 32/32 [00:00<00:00, 5791.49it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.12784375250339508\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "si_data = si_calculator.si_batch(batch_size=1000)\n",
        "print(si_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_n2IwTCiRjJW"
      },
      "source": [
        "# Calc High order SI (order = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWdPiGPSRim7",
        "outputId": "db73e61c-ce3d-427d-9d23-3b7dfcfa2dae"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing High Order SI: 100%|██████████| 32/32 [00:00<00:00, 382.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.039750002324581146\n"
          ]
        }
      ],
      "source": [
        "si_high_order_2_data = si_calculator.high_order_si_batch(order=2,batch_size=1000)\n",
        "print(si_high_order_2_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLeMf-39Rn_G"
      },
      "source": [
        "# High order soft SI (order=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMfy3ageRnjJ",
        "outputId": "5c297718-6653-4d94-c5a7-9cb99708ab9c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating Soft Order SI: 100%|██████████| 32/32 [00:00<00:00, 1671.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Soft Order(2) SI : 0.11173438280820847\n"
          ]
        }
      ],
      "source": [
        "si_soft_order_2_data = si_calculator.soft_order_si_batch(order=2,batch_size=1000)\n",
        "print(\"Soft Order(2) SI :\", si_soft_order_2_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDaOvfBsRtNB"
      },
      "source": [
        "# Center Based SI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVf4U8wERvX7",
        "outputId": "7045a4b3-e9ac-4688-b309-23aa924e2f49"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating CSI: 100%|██████████| 32/32 [00:00<00:00, 3102.58it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Center SI: 0.10343750566244125\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "center_si_data = si_calculator.center_si_batch(batch_size=1000)\n",
        "print(\"Center SI:\", center_si_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itbNG959RyHs"
      },
      "source": [
        "# Anti SI (order = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I96-uaSHR0PU",
        "outputId": "941f873f-e692-4a69-b704-718a0312f93c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating Anti-SI: 100%|██████████| 8000/8000 [00:00<00:00, 10959.27it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "anti_si: 0.8500000238418579\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "anti_si = si_calculator.anti_si(order=2)\n",
        "print(\"anti_si:\", anti_si)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcufimjPSFh-"
      },
      "source": [
        "**Test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J90ydWTdR4wr",
        "outputId": "fe983617-84c3-4adc-81fe-30c214fdb7a0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [00:02<00:00, 17.13it/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def extract_features(model, dataloader):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    # Extract features up to _avg_pooling\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    # Switch off gradient computation\n",
        "    with torch.no_grad():\n",
        "        for images, targets in tqdm(dataloader):\n",
        "            images, targets = images.to(device), targets.to(device)\n",
        "\n",
        "            # Forward pass up to _avg_pooling\n",
        "            x = model._conv_stem(images)\n",
        "            x = model._bn0(x)\n",
        "            for block in model._blocks:\n",
        "                x = block(x)\n",
        "                if isinstance(block, models.efficientnet.SqueezeExcitation):\n",
        "                    # SqueezeExcitation block, apply Swish activation\n",
        "                    x = block._swish(x)\n",
        "            x = model._conv_head(x)\n",
        "            x = model._bn1(x)\n",
        "            x = model._swish(x)\n",
        "            x = model._avg_pooling(x)\n",
        "\n",
        "            # Flatten the output before appending to features\n",
        "            features.append(x.view(x.size(0), -1))\n",
        "            labels.append(targets)\n",
        "\n",
        "    features = torch.cat(features)\n",
        "    labels = torch.cat(labels)\n",
        "\n",
        "    return features, labels\n",
        "\n",
        "# Move models to GPU\n",
        "efficientnet_features.to(device)\n",
        "\n",
        "efficientnet_features, efficientnet_labels = extract_features(model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIIqz2jkSLXO",
        "outputId": "a58c2bae-9d8f-4eb1-ebc1-b4b85d009ff7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data has been normalized\n"
          ]
        }
      ],
      "source": [
        "si_calculator = ARH_SeparationIndex(efficientnet_features, efficientnet_labels, normalize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-s-fFLLQSqdS"
      },
      "source": [
        "# SI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqcPU7suSWHQ",
        "outputId": "2e7732eb-a1e0-47b4-e727-291241867b57"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating SI: 100%|██████████| 10/10 [00:00<00:00, 3928.72it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.1022999957203865\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "si_data = si_calculator.si_batch(batch_size=1000)\n",
        "print(si_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Sq4F2AASuND"
      },
      "source": [
        "# Calc High order SI (order = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juHKFs8bSYXy",
        "outputId": "c66a8901-f4e6-4926-c526-04caa77e5866"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing High Order SI: 100%|██████████| 10/10 [00:00<00:00, 1083.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.0284000001847744\n"
          ]
        }
      ],
      "source": [
        "si_high_order_2_data = si_calculator.high_order_si_batch(order=2,batch_size=1000)\n",
        "print(si_high_order_2_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PIEMSPFSyzx"
      },
      "source": [
        "# High order soft SI (order=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQxSTrFqSZrd",
        "outputId": "0843f3b7-8f4c-4e94-b034-62c6841081b0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating Soft Order SI: 100%|██████████| 10/10 [00:00<00:00, 946.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Soft Order(2) SI : 0.08899999409914017\n"
          ]
        }
      ],
      "source": [
        "si_soft_order_2_data = si_calculator.soft_order_si_batch(order=2,batch_size=1000)\n",
        "print(\"Soft Order(2) SI :\", si_soft_order_2_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgwLvbR3S06D"
      },
      "source": [
        "# Center Based SI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pp_7VMXVScV8",
        "outputId": "1ba8add9-1f58-4dfc-e404-87781a7ff32d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating CSI: 100%|██████████| 10/10 [00:00<00:00, 1102.37it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Center SI: 0.11729999631643295\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "center_si_data = si_calculator.center_si_batch(batch_size=1000)\n",
        "print(\"Center SI:\", center_si_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdNc3P7zS3A3"
      },
      "source": [
        "# Anti SI (order = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJZjnDEWSoPG",
        "outputId": "7bd14848-2a4f-4c6c-c1e1-08ffeaeff3e7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating Anti-SI: 100%|██████████| 10000/10000 [00:01<00:00, 9023.23it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "anti_si: 0.850600004196167\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "anti_si = si_calculator.anti_si(order=2)\n",
        "print(\"anti_si:\", anti_si)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scVUEvx0oMUj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
